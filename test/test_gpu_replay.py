#!/usr/bin/env python3
"""
–¢–µ—Å—Ç—ã –¥–ª—è GPU Replay Buffer
"""

import sys
import os
sys.path.append('/app')

import time
import torch
import numpy as np
from agents.vdqn.dqnsolver import PrioritizedReplayBuffer
from agents.vdqn.cfg.vconfig import vDqnConfig

def test_replay_buffer_performance():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ replay buffer"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GPU Replay Buffer...")
    
    try:
        cfg = vDqnConfig()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ—Å—Ç–∞
        capacity = 10000
        state_size = 144
        batch_size = 64
        
        print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ—Å—Ç–∞:")
        print(f"   - –ï–º–∫–æ—Å—Ç—å: {capacity}")
        print(f"   - –†–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è: {state_size}")
        print(f"   - –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        print(f"   - GPU storage: {cfg.use_gpu_storage}")
        
        # –°–æ–∑–¥–∞–µ–º replay buffer
        replay_buffer = PrioritizedReplayBuffer(
            capacity=capacity,
            state_size=state_size,
            alpha=cfg.alpha,
            beta=cfg.beta,
            beta_increment=cfg.beta_increment,
            use_gpu_storage=cfg.use_gpu_storage
        )
        
        print(f"‚úÖ Replay buffer —Å–æ–∑–¥–∞–Ω –Ω–∞ {replay_buffer.device}")
        
        # –¢–µ—Å—Ç 1: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –±—É—Ñ–µ—Ä–∞
        print("\nüîµ –¢–µ—Å—Ç 1: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –±—É—Ñ–µ—Ä–∞...")
        start_time = time.time()
        
        for i in range(capacity):
            state = np.random.randn(state_size).astype(np.float32)
            next_state = np.random.randn(state_size).astype(np.float32)
            action = np.random.randint(0, 3)
            reward = np.random.randn()
            done = np.random.choice([True, False])
            
            replay_buffer.push(state, action, reward, next_state, done)
            
            if i % 1000 == 0:
                print(f"   - –î–æ–±–∞–≤–ª–µ–Ω–æ {i}/{capacity} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        fill_time = time.time() - start_time
        fill_rate = capacity / fill_time
        print(f"‚úÖ –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {fill_time:.2f}—Å ({fill_rate:.1f} —ç–ª–µ–º–µ–Ω—Ç–æ–≤/—Å)")
        
        # –¢–µ—Å—Ç 2: –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        print("\nüü° –¢–µ—Å—Ç 2: –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        start_time = time.time()
        
        for i in range(100):
            batch = replay_buffer.sample(batch_size)
            if batch[0] is None:
                print("‚ùå –û—à–∏–±–∫–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è!")
                return False
        
        sample_time = time.time() - start_time
        sample_rate = 100 / sample_time
        print(f"‚úÖ –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {sample_time:.2f}—Å ({sample_rate:.1f} –±–∞—Ç—á–µ–π/—Å)")
        
        # –¢–µ—Å—Ç 3: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        print("\nüü¢ –¢–µ—Å—Ç 3: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤...")
        start_time = time.time()
        
        for i in range(100):
            indices = torch.randint(0, capacity, (batch_size,))
            priorities = torch.rand(batch_size)
            replay_buffer.update_priorities(indices, priorities)
        
        update_time = time.time() - start_time
        update_rate = 100 / update_time
        print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã –∑–∞ {update_time:.2f}—Å ({update_rate:.1f} –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π/—Å)")
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = fill_time + sample_time + update_time
        print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   ‚Ä¢ –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ: {fill_rate:.1f} —ç–ª–µ–º–µ–Ω—Ç–æ–≤/—Å")
        print(f"   ‚Ä¢ –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: {sample_rate:.1f} –±–∞—Ç—á–µ–π/—Å")
        print(f"   ‚Ä¢ –û–±–Ω–æ–≤–ª–µ–Ω–∏—è: {update_rate:.1f} –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π/—Å")
        print(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}—Å")
        
        # GPU –ø–∞–º—è—Ç—å
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024**2
            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**2
            print(f"   ‚Ä¢ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} MB")
            print(f"   ‚Ä¢ GPU –ø–∞–º—è—Ç—å –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∞: {gpu_memory_reserved:.1f} MB")
        
        print(f"   ‚Ä¢ –¢–∏–ø —Ö—Ä–∞–Ω–µ–Ω–∏—è: {'GPU storage' if cfg.use_gpu_storage else 'Pinned memory'}")
        print(f"   ‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {replay_buffer.device}")
        
        # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        print(f"\nüéØ –û–¶–ï–ù–ö–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
        if fill_rate > 500 and sample_rate > 20:
            print("‚úÖ –û–¢–õ–ò–ß–ù–û - –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        elif fill_rate > 200 and sample_rate > 10:
            print("‚úÖ –•–û–†–û–®–û - GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ")
        else:
            print("‚ö†Ô∏è –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û - –ï—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        if cfg.use_gpu_storage:
            print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU storage - –æ—Ç–ª–∏—á–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        else:
            print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è pinned memory - —Ö–æ—Ä–æ—à–æ –¥–ª—è CPU-GPU –ø–µ—Ä–µ–¥–∞—á–∏")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_replay_buffer_performance()
    if success:
        print("\nüéâ –¢–µ—Å—Ç GPU Replay Buffer –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("\n‚ùå –¢–µ—Å—Ç GPU Replay Buffer –Ω–µ –ø—Ä–æ–π–¥–µ–Ω!")
