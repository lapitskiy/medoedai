services:
  postgres:
    image: postgres:14
    container_name: postgres
    restart: always
    environment:
      POSTGRES_DB: medoed_db
      POSTGRES_USER: medoed_user
      POSTGRES_PASSWORD: medoed
      POSTGRES_MULTIPLE_DATABASES: medoed_db,mlflow_db
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:latest
    container_name: redis
    restart: always

  medoedai:
    build:
      context: .
      dockerfile: Dockerfile.cpu
    container_name: medoedai
    depends_on:
      - redis
    ports:
      - "5050:5050"
    environment:
      - TORCHSERVE_URL=http://serving:8080/predictions/medoedai_model
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0      
    volumes:
      - ./:/app
    working_dir: /app
    command: ["python", "main.py"]
    deploy:
      resources:
        reservations:
          devices: []

  xray:
    image: teddysun/xray:latest
    container_name: xray
    profiles: ["proxy"]
    restart: unless-stopped
    volumes:
      - ./xray/config.json:/etc/xray/config.json:ro
    command: ["xray","-c","/etc/xray/config.json"]
    # порт наружу НЕ обязателен (контейнеры общаются внутри сети)
    # ports:
    #   - "1080:1080"

  celery-worker:
    build: .
    container_name: celery-worker
    depends_on:
      - medoedai
      - redis 
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:    
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      #- HTTP_PROXY=socks5h://xray:1080
      #- HTTPS_PROXY=socks5h://xray:1080
      #- ALL_PROXY=socks5h://xray:1080
      #- NO_PROXY=localhost,127.0.0.1,redis,postgres,medoedai,serving,*.local,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - WANDB_HTTP_TIMEOUT=60
      - PYTHONDONTWRITEBYTECODE=1       
    env_file:
      - .env
    volumes:
      - ./:/app
    working_dir: /app
    command: ["celery", "-A", "tasks.celery_tasks", "worker",
              "-Q", "train", "-P", "solo", "-c", "1", "--loglevel=info"]
    deploy:
          resources:
            reservations:
              devices: [] 
              
  pytorch-gpu:
    image: pytorch/pytorch:latest
    container_name: pytorch-gpu
    profiles: ["gpu"]
    depends_on:
      - celery-worker    
    restart: unless-stopped
    ports:
      - "8090:8090"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ./workspace:/workspace
      - ./models:/models
    working_dir: /workspace
    command: ["sleep", "infinity"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  pgdata:
