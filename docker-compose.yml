services:
  postgres:
    image: postgres:14
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: medoed_db
      POSTGRES_USER: medoed_user
      POSTGRES_PASSWORD: medoed
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  redis:
    image: redis:latest
    container_name: redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 5s

  medoedai:
    build: .
    container_name: medoedai
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    ports:
      - "5050:5050"
    environment:
      TORCHSERVE_URL: http://serving:8000/predict_ensemble
      DISABLE_TORCH_COMPILE: "true"   # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å torch.compile –¥–ª—è Tesla P100
      FORCE_DISABLE_TORCH_COMPILE: "true"  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç torch.compile
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      PYTHONDONTWRITEBYTECODE: "1"
      FLASK_DEBUG: "False"
      # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏
      OMP_NUM_THREADS: "4"  # OpenMP –ø–æ—Ç–æ–∫–∏ –¥–ª—è –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞
      MKL_NUM_THREADS: "4"  # Intel MKL –ø–æ—Ç–æ–∫–∏
      TORCH_NUM_THREADS: "4"  # PyTorch –ø–æ—Ç–æ–∫–∏
      # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
      OMP_SCHEDULE: "dynamic"
      MKL_DYNAMIC: "true"
      MKL_INTERFACE_LAYER: "LP64"
    env_file:
      - .env
    volumes:
      - .:/workspace
    working_dir: /workspace
    command: >
      sh -c "
        echo 'üßπ –û—á–∏—â–∞—é Redis –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º...' &&
        python clear_redis.py &&
        echo '‚úÖ Redis –æ—á–∏—â–µ–Ω, –∑–∞–ø—É—Å–∫–∞—é Flask...' &&
        python main.py
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  celery-worker:
    build: .
    container_name: celery-worker
    depends_on:
      medoedai:
        condition: service_started
      redis:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      PYTHONDONTWRITEBYTECODE: "1"
      DISABLE_TORCH_COMPILE: "true"   # –û—Ç–∫–ª—é—á–∏—Ç—å torch.compile –¥–ª—è Tesla P100
      FORCE_DISABLE_TORCH_COMPILE: "true"  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞
      # –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º Celery (–æ–±—ã—á–Ω—ã–µ –æ—á–µ—Ä–µ–¥–∏)
      CELERY_POOL: prefork
      CELERY_CONCURRENCY: "6"
      # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤ –≤–Ω—É—Ç—Ä–∏ NumPy/MKL/Torch
      OMP_NUM_THREADS: "4"
      MKL_NUM_THREADS: "4"
      TORCH_NUM_THREADS: "4"
      # –Ø–≤–Ω–æ —Å–∫—Ä—ã—Ç—å GPU –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞
      CUDA_VISIBLE_DEVICES: ""
    env_file:
      - .env
    volumes:
      - .:/workspace
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /workspace
    # –ò—Å–∫–ª—é—á–∞–µ–º –æ—á–µ—Ä–µ–¥—å train –∏–∑ –æ–±—â–µ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞, —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω–∫—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞ CPU
    command: ["sh","-lc","celery -A tasks.celery_tasks worker -Q celery -P ${CELERY_POOL:-prefork} -c ${CELERY_CONCURRENCY:-6} --loglevel=info"]
    # CPU-only –≤–æ—Ä–∫–µ—Ä: GPU –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è

  # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–æ—Ä–∫–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ CPU —è–¥—Ä–∞
  celery-train:
    build: .
    container_name: celery-train
    depends_on:
      medoedai:
        condition: service_started
      redis:
        condition: service_healthy
    restart: unless-stopped
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      PYTHONDONTWRITEBYTECODE: "1"
      DISABLE_TORCH_COMPILE: "true"
      FORCE_DISABLE_TORCH_COMPILE: "true"
      CELERY_POOL: prefork
      CELERY_CONCURRENCY: "1"  # –æ–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å—Å, –Ω–æ –º–∞–∫—Å–∏–º—É–º –ø–æ—Ç–æ–∫–æ–≤ –≤–Ω—É—Ç—Ä–∏ BLAS/Torch
      CUDA_VISIBLE_DEVICES: ""
      TRAIN_CPU_FRACTION: "0.9"  # –¥–æ–ª—è CPU —è–¥–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –æ–±—É—á–µ–Ω–∏–µ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 80%)
      # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
      OMP_SCHEDULE: "dynamic"
      MKL_DYNAMIC: "true"
      MKL_INTERFACE_LAYER: "LP64"
    env_file:
      - .env
    volumes:
      - .:/workspace
    working_dir: /workspace
    command: ['sh','-lc','NPROC=$$(nproc); THREADS=$$(python -c "import os,math; n=os.cpu_count() or 1; f=float(os.environ.get(\"TRAIN_CPU_FRACTION\", \"0.8\")); print(max(1,int(n*f)))"); export OMP_NUM_THREADS=$${OMP_NUM_THREADS:-$$THREADS} MKL_NUM_THREADS=$${MKL_NUM_THREADS:-$$THREADS} TORCH_NUM_THREADS=$${TORCH_NUM_THREADS:-$$THREADS} OMP_SCHEDULE=$${OMP_SCHEDULE:-dynamic} MKL_DYNAMIC=$${MKL_DYNAMIC:-true} MKL_INTERFACE_LAYER=$${MKL_INTERFACE_LAYER:-LP64}; echo "üöÄ CPU Optimization: Using $$THREADS/$$NPROC threads (OMP=$$OMP_NUM_THREADS, MKL=$$MKL_NUM_THREADS, TORCH=$$TORCH_NUM_THREADS)"; exec celery -A tasks.celery_tasks worker -Q train -P $${CELERY_POOL:-prefork} -c $${CELERY_CONCURRENCY:-1} --loglevel=info']

  pytorch-gpu:
    image: pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime
    container_name: pytorch-gpu
    depends_on:
      celery-worker:
        condition: service_started
    ports:
      - "8090:8090"
    volumes:
      - .:/workspace
      - ./models:/models
    working_dir: /workspace
    command: ["bash","-lc","python train.py && sleep 2"]
    restart: "no"
    # CPU-only —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞: GPU –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è

  celery-trade:
    build: .
    container_name: celery-trade
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
      serving:
        condition: service_started
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      BYBIT_API_KEY: ${BYBIT_API_KEY:-}
      BYBIT_SECRET_KEY: ${BYBIT_SECRET_KEY:-}
      TRADING_MODE: ${TRADING_MODE:-sandbox}
      PYTHONDONTWRITEBYTECODE: "1"
      SERVING_URL: http://serving:8000/predict_ensemble
    env_file:
      - .env
    volumes:
      - .:/workspace
    working_dir: /workspace
    command: ["sh","-lc","celery -A tasks.celery_tasks worker -Q trade -P prefork -c 8 --loglevel=info"]

  celery-beat:
    build: .
    container_name: celery-beat
    command: ["celery", "-A", "tasks.celery_tasks", "beat", "--loglevel=info"]
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      ENABLE_TRADING_BEAT: "1"
    env_file:
      - .env      
    volumes:
      - .:/workspace
    working_dir: /workspace
    depends_on:
      - redis
      - celery-worker
    restart: unless-stopped

  serving:
    build:
      context: ./serving
    container_name: serving
    restart: unless-stopped
    # –ù–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–µ–±–∞, —Å—Ç–∞—Ä—Ç—É–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ
    environment:
      - PYTHONUNBUFFERED=1
      - MODEL_BASE_DIR=/workspace/models
    ports:
      - "8000:8000"
    volumes:
      - ./serving:/app
      - .:/workspace
      - ./models:/workspace/models
    working_dir: /app
    command: ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2", "--timeout-keep-alive", "30"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 10s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  pgdata:

