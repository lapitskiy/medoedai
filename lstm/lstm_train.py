from utils.env import lstmcfg
from utils.rolling import run_multiprocessing_rolling_window
from utils.path import generate_uuid, path_exist, clear_folder, read_temp_path, save_grid_checkpoint, \
    file_exist
from utils.models_list import create_model

import os
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
import shutil
import stat
import time
import gc
import pandas as pd
import numpy as np

from sklearn.metrics import precision_recall_curve
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
import concurrent.futures
import scikeras
from scikeras.wrappers import KerasRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from itertools import product
import logging
import psutil
import matplotlib.pyplot as plt
import matplotlib

import requests

import joblib
matplotlib.use('Agg')

TF_SERVING_URL = "http://tensorflow-server:8501/v1/models/medoedai_model:predict"

# –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ API
payload = {
    "signature_name": "serving_default",
    "instances": data.tolist()
}

# –û—Ç–ø—Ä–∞–≤–ª—è–µ–º POST-–∑–∞–ø—Ä–æ—Å –Ω–∞ TensorFlow Serving API
response = requests.post(TF_SERVING_URL, json=payload)

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
if response.status_code == 200:
    predictions = response.json()["predictions"]
    print("Predictions:", predictions)
else:
    print("Error:", response.text)


def goLSTM(task):
    task_dict = dict(task)
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω—É–∂–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    current_threshold = task_dict.get('threshold')
    current_window = task_dict.get('model__current_window')
    current_period = task_dict.get('period')
    current_batch_size = task_dict.get('batch_size')
    current_epochs = task_dict.get('epochs')
    current_dropout = task_dict.get('model__current_dropout')
    current_neiron = task_dict.get('model__current_neiron')
    num_features = task_dict.get('model__num_features')
    model_number = task_dict.get('model__model_number')
    num_samples = task_dict.get('num_samples')
    coin = task_dict.get('coin')
    date_df = task_dict.get('date_df').split(',')
    best_score = task_dict.get('best_score')

    list_ = read_temp_path(f'temp/lstm/roll_win/roll_path_ct{current_threshold}_cw{current_window}_cp{current_period}.txt', 4)
    x_path = list_[0]
    y_path = list_[1]
    num_samples = list_[2]
    hash_value = list_[3]
    if num_features != len(lstmcfg.numeric):
        print(f'num_features –Ω–µ —Å–æ–æ—Ç–≤–µ—Å—Ç–≤—É–µ—Ç config.numeric')
        exit()

    x = np.memmap(f'{x_path}', dtype=np.float32, mode='r', shape=(int(num_samples), current_window, num_features))
    y = np.memmap(f'{y_path}', dtype=np.int8, mode='r', shape=(int(num_samples),))

    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    unique, counts = np.unique(y, return_counts=True)
    class_distribution = dict(zip(unique, counts))
    print(class_distribution)

    # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â–∏–π+–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä—ã
    x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.2,
                                                                random_state=42)

    # –ó–∞—Ç–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ –æ–±—É—á–∞—é—â–∏–π+–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –Ω–∞ –æ–±—É—á–∞—é—â–∏–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä—ã
    x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25,
                                                      random_state=42)  # 0.25 x 0.8 = 0.2
     # –ü–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ train –∏ test
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ:")
    unique_train, counts_train = np.unique(y_train_val, return_counts=True)
    #print(dict(zip(unique_train, counts_train)))

    print(f"Shape of x_train: {x_train.shape}")
    print(f"Shape of y_train: {y_train.shape}")
    print(f"Shape of x_val: {x_val.shape}")
    print(f"Shape of y_val: {y_val.shape}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LSTM
    print(f'model_number {model_number}')


    model = create_model(current_dropout=current_dropout, current_neiron=current_neiron, current_window=current_window,
                         num_features=num_features, model_number=model_number, type='lstm')
    optimizer = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy',
                  metrics=[
                      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
                      tf.keras.metrics.Precision(name='precision'),
                      tf.keras.metrics.Recall(name='recall'),
                      tf.keras.metrics.AUC(name='auc'),
                      f1_score])
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ y_train_labels –∏ class_labels
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weights_dict = dict(enumerate(class_weights))

    data = {
        "batch_sizes": current_batch_size,
        "epoch": current_epochs,
        "current_period": f"{current_period}",
        "current_window": f"{current_window}",
        "current_threshold": f"{current_threshold}",
        "current_neiron": f"{current_neiron}",
        "current_dropout": f"{current_dropout}",
    }

    #lstmcfg.uuid = generate_uuid()
    #lstmcfg.directory_save = f'model/lstm/{coin}/{current_period}/'

    checkpoint = ModelCheckpointWithMetricThreshold(filedata=data, model=model)
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)

    history = model.fit(x_train, y_train, batch_size=current_batch_size, epochs=current_epochs, validation_data=(x_val, y_val), class_weight=class_weights_dict, callbacks=[checkpoint, early_stopping])

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    scores = model.evaluate(x_test, y_test, verbose=1)
    #
    # –ø–æ–¥–±–æ—Ä–∞ –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    #
    y_val_probs = model.predict(x_val)
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Precision –∏ Recall –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤
    precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_probs)
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ F1-score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä–æ–≥–∞
    f1_scores = 2 * (precisions * recalls) / (
            precisions + recalls + 1e-10)  # –¥–æ–±–∞–≤–ª—è–µ–º –º–∞–ª–µ–Ω—å–∫–æ–µ —á–∏—Å–ª–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
    # –ù–∞–π—Ç–∏ –ø–æ—Ä–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç F1-score
    opt_idx = np.argmax(f1_scores)
    opt_threshold = thresholds[opt_idx]
    opt_f1_score = f1_scores[opt_idx]

    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:", class_distribution)
    print("–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥:", opt_threshold)
    print("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π F1-Score:", opt_f1_score)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    y_test_probs = model.predict(x_test)
    y_test_pred = (y_test_probs >= opt_threshold).astype(int)

    # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    conf_matrx_test = confusion_matrix(y_test, y_test_pred)
    print("Confusion Matrix:\n", conf_matrx_test)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª
    data2 = {
        "hash": hash_value,
        "coin": lstmcfg.coin,
        "Test Accuracy": f"{scores[1] * 100:.2f}%",
        "Test Precision": f"{scores[2]:.2f}",
        "Test Recall": f"{scores[3]:.2f}",
        "Test AUC": f"{scores[4]:.2f}",
        "Test F1-Score": f"{scores[-1]:.2f}",
        "Optimal Threshold": opt_threshold,
        "Maximum F1-Score": opt_f1_score,
        "batch_sizes": current_batch_size,
        "epoch": current_epochs,
        "Confusion Matrix": conf_matrx_test,
        "current_period": f"{current_period}",
        "current_window": f"{current_window}",
        "current_threshold": f"{current_threshold}",
        "current_neiron": f"{current_neiron}",
        "current_dropout": f"{current_dropout}",
        "class_distribution": class_distribution,
        "date_df": ','.join(date_df)
    }

    data.update(data2)
    print(f"Test Accuracy: {scores[1] * 100:.2f}%")
    print(f"Test Precision: {scores[2]:.2f}")
    print(f"Test Recall: {scores[3]:.2f}")
    print(f"Test AUC: {scores[4]:.2f}")
    print(f"Test F1-Score: {scores[-1]:.2f}")
    print('–ó–ê–ü–ò–°–¨ –§–ê–ô–õ–ê?')
    if (scores[1] * 100 >= 60 and
            scores[2] >= 0.10 and
            scores[3] >= 0.40 and
            scores[4] >= 0.60 and
            scores[-1] >= 0.04):

        result_filename = f'{lstmcfg.directory_save}/metric/{lstmcfg.coin}/result.csv'
        save_path = f"/models/medoedai_model/{model_version}/"
        directory_save = f'{lstmcfg.directory_save}work_model/{coin}/{current_period}/'
        backup_path = f'/app/models_backup/{lstmcfg.directory_save}work_model/{coin}/{current_period}/'
        path_exist(f'{lstmcfg.directory_save}/metric/{lstmcfg.coin}/')
        path_exist(lstmcfg.directory_save)
        path_exist(directory_save)
        # –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª
        results_df = pd.DataFrame([data])
        print('–ó–ê–ü–ò–°–¨ –§–ê–ô–õ–ê')
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
            with open(result_filename, 'r') as f:
                existing_df = pd.read_csv(result_filename, delimiter=';')
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å—Ç–æ–ª–±–µ—Ü 'threshold' –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º —Ñ–∞–π–ª–µ
                if 'batch_sizes' not in existing_df.columns or 'epoch' not in existing_df.columns \
                        or 'hash' not in existing_df.columns or 'coin' not in existing_df.columns:
                    # –ï—Å–ª–∏ –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                    results_df.to_csv(result_filename, mode='a', header=True, index=False, sep=';')
                else:
                    # –ï—Å–ª–∏ –µ—Å—Ç—å, –¥–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    results_df.to_csv(result_filename, mode='a', header=False, index=False, sep=';')
        except FileNotFoundError:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ–≥–æ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            results_df.to_csv(result_filename, mode='w', header=True, index=False, sep=';')

        source_path = f'temp/{lstmcfg.ii_path}/scaler/scaler_ct{current_threshold}_cw{current_window}_cp{current_period}.gz'
        target_path = f'{directory_save}{hash_value}.gz'
        shutil.copy2(source_path, target_path)
        #joblib.dump(scaler, f"{directory_save}{hash_value}.gz")
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model.save(f"{directory_save}{hash_value}.h5")
        # –í—ã–≤–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
        model.summary()
    del x_train, x_val, y_train, y_val, x, y
    return 'End current epoch'

def f1_score(y_true, y_pred):
    # –ü–µ—Ä–≤–æ–µ, —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —ç—Ç–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–±–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–º–µ—é—Ç —Ç–∏–ø float32
    y_true = K.cast(y_true, 'float32')
    y_pred = K.round(y_pred)  # y_pred –æ–∫—Ä—É–≥–ª—è–µ—Ç—Å—è –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Ü–µ–ª–æ–≥–æ –∏ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è –∫ float32

    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–∏–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
    tp = K.sum(K.cast(y_true * y_pred, 'float32'), axis=0)
    # –õ–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float32'), axis=0)
    # –õ–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float32'), axis=0)

    # Precision
    precision = tp / (tp + fp + K.epsilon())
    # Recall
    recall = tp / (tp + fn + K.epsilon())

    # –í—ã—á–∏—Å–ª—è–µ–º F1 score
    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è NaN –≤ f1
    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)
    return K.mean(f1)

class ModelCheckpointWithMetricThreshold(Callback):
    def __init__(self, filedata, model, verbose=1):
        super(ModelCheckpointWithMetricThreshold, self).__init__()
        self.thresholds = lstmcfg.metric_thresholds  # –°–ª–æ–≤–∞—Ä—å —Å –ø–æ—Ä–æ–≥–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        self.best_metrics = {key: 0 for key in self.thresholds.keys()}  # –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        self.verbose = verbose
        self.filedata = filedata
        self.directory_save = lstmcfg.directory_save + 'metric/'
        self.current_threshold = filedata['current_threshold']
        self.current_window = filedata['current_window']
        self.mymodel = model

    def on_epoch_end(self, epoch, logs=None):
        current_metrics = {key: logs.get(key) for key in self.thresholds.keys()}
        if all(current_metrics[key] >= self.thresholds[key] for key in self.thresholds.keys()):
            improved = False
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É–ª—É—á—à–∏–ª–∞—Å—å –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞
            for key in self.thresholds.keys():
                if current_metrics[key] > self.best_metrics[key]:
                    self.best_metrics[key] = current_metrics[key]
                    improved = True
            if improved:
                if self.verbose > 0:
                    print("\nMetrics at the end of epoch {}:".format(epoch + 1))
                    print(f"Test Accuracy: {logs.get('val_accuracy') * 100:.2f}%")
                    print(f"Test Precision: {logs.get('val_precision'):.2f}")
                    print(f"Test Recall: {logs.get('val_recall'):.2f}")
                    print(f"Test AUC: {logs.get('val_auc'):.2f}")
                    print(f"Test F1-Score: {logs.get('val_f1_score'):.2f}")
                self.filedata.update(
                    {
                        "Test Accuracy": f"{logs.get('val_accuracy') * 100:.2f}%",
                        "Test Precision": f"{logs.get('val_precision'):.2f}",
                        "Test Recall": f"{logs.get('val_recall'):.2f}",
                        "Test AUC": f"{logs.get('val_auc'):.2f}",
                        "Test F1-Score": f"{logs.get('val_f1_score'):.2f}"
                    }
                )
                path_exist(self.directory_save)
                with open(f"{config.directory_save + config.uuid}/callback.txt", "w") as file:
                    for key, value in self.filedata.items():
                        file.write(f"{key}={value}\n")
                source_path = f'temp/scaler/scaler_ct-{self.current_threshold}_cw-{self.current_window}.gz'
                destination_path = f'{self.directory_save}/scaler.gz'
                try:
                    shutil.copy(source_path, destination_path)
                except Exception as e:
                    print(f'exc {e}')
                self.mymodel.save(f'{self.directory_save}/model.keras', overwrite=True)
                self.mymodel.summary()
                print(f'Metrics improved. Saving model')


def send_to_tensorflow_api(task):
    """ –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ TensorFlow Serving API """
    payload = {
        "signature_name": "serving_default",
        "instances": [dict(task)]  # üî• –ü–µ—Ä–µ–¥–∞—ë–º –¥–∞–Ω–Ω—ã–µ –≤ JSON-—Ñ–æ—Ä–º–∞—Ç–µ
    }

    try:
        response = requests.post(TF_SERVING_URL, json=payload)  # üî• –û—Ç–ø—Ä–∞–≤–ª—è–µ–º POST-–∑–∞–ø—Ä–æ—Å

        if response.status_code == 200:
            return response.json()["predictions"]
        else:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ TensorFlow: {response.text}")
            return None
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞: {str(e)}")
        return None


# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    tf.config.threading.set_intra_op_parallelism_threads(lstmcfg.CPU_COUNT)
    tf.config.threading.set_inter_op_parallelism_threads(lstmcfg.CPU_COUNT)

    tf.get_logger().setLevel('ERROR')
    log_file = os.path.expanduser('~/training.log')

    logging.basicConfig(
        filename=log_file,  # –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    # —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ
    path_exist('../temp/')
    path_exist('../temp/lstm/checkpoint/')
    path_exist('../temp/lstm/best_params/')
    path_exist('../temp/lstm/roll_win/')
    path_exist('../temp/lstm/scaler/')
    path_exist('../temp/lstm/mmap/')
    clear_folder('../temp/lstm/roll_win/')
    clear_folder('../temp/lstm/scaler/')
    #clear_folder('temp/lstm/mmap/')

    run_multiprocessing_rolling_window(config=lstmcfg)
    #goLSTM
    if lstmcfg.goLSTM:
        all_task = []
        directory_path = f'model/{lstmcfg.ii_path}/best_params/'
        for filename in os.listdir(f'{directory_path}'):
            if filename.endswith('.csv'):
                # –°–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É
                filepath = os.path.join(directory_path, filename)
                uuid_name, extension = os.path.splitext(filename)
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å —Ñ–∞–π–ª–æ–º
                data = pd.read_csv(f'{filepath}', delimiter=';')
                data_fields = data.columns.tolist()

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–æ–ª–µ–π –≤ DataFrame
                missing_fields = [field for field in lstmcfg.required_params if field not in data.columns]
                if missing_fields:
                    print(f"Error: Missing fields {', '.join(missing_fields)} in the file {filename}.")
                    exit()

                for index, row in data.iterrows():
                    if all(row[field] for field in lstmcfg.required_params):
                        #print(f'head {data.head()}')
                        #print(f'tail {data.tail()}')
                        #
                        date_df_check = row['date_df'].split(',')
                        #print(f'Row {index} dates:', date_df_check)
                        #print(f' date_df_check {date_df_check} != lstmcfg.date_df {lstmcfg.date_df}')
                        if date_df_check != lstmcfg.date_df:
                            print(
                                f'–û—à–∏–±–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º —Å–≤–µ—á–µ–π \ndate_df {lstmcfg.date_df}\n date ib csv {date_df_check}')
                            exit()
                    else:
                        print(f"Error: Missing data in required fields at row {index}.")
                        exit()
                    # –°–±–æ—Ä–∫–∞ –∑–∞–¥–∞—á–∏ —Å –∏–º–µ–Ω–∞–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    task = tuple((column_name, value) for column_name, value in row.items())
                    all_task.append(task)
                else:
                    print(f'Skipping row {index} in file {filename} due to missing values in required fields.')


        total_iterations = len(all_task)
        print(f'Total number of iterations: {total_iterations}')

        max_workers = min(lstmcfg.CPU_COUNT, len(all_task)) #max_workers=max_workers

        start_time = time.perf_counter()

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ProcessPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:
            # –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
            futures = [executor.submit(goLSTM, task) for task in all_task]
            for iteration, future in enumerate(concurrent.futures.as_completed(futures), start=1):
                iteration_start_time = time.perf_counter()
                result = future.result()
                iteration_end_time = time.perf_counter()
                iteration_time = iteration_end_time - iteration_start_time
                print(f'Iteration {iteration}: iteration_time {iteration_time}')

                if result is not None:
                    print(result)
                else:
                    print("A task failed. See logs for more details.")
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
                memory_info = psutil.virtual_memory()
                logging.info(f"Memory usage: {memory_info.percent}%")

        end_time = time.perf_counter()
        total_time = end_time - start_time
        print(f"Total time taken for all iterations: {total_time:.2f} seconds")
        logging.info(f"Total time taken for all iterations: {total_time:.2f} seconds")
