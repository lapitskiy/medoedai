"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è CNN –º–æ–¥–µ–ª–µ–π
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.sampler import WeightedRandomSampler
from typing import Dict, List, Tuple, Optional, Any
import os
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')


class CryptoDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è CNN –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
    
    def __init__(self, data: np.ndarray, labels: np.ndarray, sequence_length: int = 50):
        self.data = data
        self.labels = labels
        self.sequence_length = sequence_length
        
        assert len(data) == len(labels), "–î–∞–Ω–Ω—ã–µ –∏ –º–µ—Ç–∫–∏ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–ª–∏–Ω—É"
        assert len(data) >= sequence_length, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è sequence_length={sequence_length}"
    
    def __len__(self) -> int:
        return len(self.data) - self.sequence_length + 1
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        sequence = self.data[idx:idx + self.sequence_length]
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –º–µ—Ç–∫—É
        label = self.labels[idx + self.sequence_length - 1]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        sequence_tensor = torch.FloatTensor(sequence)
        label_tensor = torch.LongTensor([label])
        
        return sequence_tensor, label_tensor


class MultiTimeframeDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤–æ–π CNN"""
    
    def __init__(self, data_dict: Dict[str, np.ndarray], labels: np.ndarray, 
                 sequence_lengths: Dict[str, int]):
        self.data_dict = data_dict
        self.labels = labels
        self.sequence_lengths = sequence_lengths
        self.timeframes = list(data_dict.keys())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        min_length = min(len(data) for data in data_dict.values())
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∫–∏ –æ—Ç —Å–∞–º–æ–≥–æ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ñ—Ä–µ–π–º–∞, –Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ–º —Ç–æ—á–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        if len(labels) > min_length:
            labels = labels[:min_length]  # –û–±—Ä–µ–∑–∞–µ–º –º–µ—Ç–∫–∏ –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.max_sequence_length = max(sequence_lengths.values())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        for timeframe, seq_len in sequence_lengths.items():
            assert len(data_dict[timeframe]) >= seq_len, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {timeframe}"
    
    def __len__(self) -> int:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª–∏–Ω—É –º–µ—Ç–æ–∫ –∫–∞–∫ –æ—Å–Ω–æ–≤—É –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤
        return len(self.labels) - self.max_sequence_length + 1
    
    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–∞
        sequences = {}
        
        for timeframe in self.timeframes:
            seq_len = self.sequence_lengths[timeframe]
            data = self.data_dict[timeframe]
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —É –Ω–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            if idx + seq_len <= len(data):
                sequence = data[idx:idx + seq_len]
            else:
                # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ
                sequence = data[-seq_len:]
            
            sequences[timeframe] = torch.FloatTensor(sequence)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –º–µ—Ç–∫—É
        if idx + self.max_sequence_length - 1 < len(self.labels):
            label = self.labels[idx + self.max_sequence_length - 1]
        else:
            # –ï—Å–ª–∏ –º–µ—Ç–∫–∞ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã, –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é
            label = self.labels[-1]
        
        label_tensor = torch.LongTensor([label])
        
        return sequences, label_tensor


class CryptoDataLoader:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
    
    def __init__(self, config):
        self.config = config
        self.scalers = {}
        self.feature_means = {}
        self.feature_stds = {}
    
    def load_symbol_data(self, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–∞"""
        data_dir = self.config.data_dir
        
        # –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
        possible_paths = [
            os.path.join(data_dir, f"{symbol.lower()}_{timeframe}.csv"),
            os.path.join(data_dir, symbol, timeframe, "data.csv"),
            os.path.join(data_dir, "spot", symbol, f"{timeframe}.csv"),
            os.path.join(data_dir, symbol, f"{timeframe}.csv")
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                try:
                    df = pd.read_csv(path)
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ {symbol} {timeframe}: {len(df)} –∑–∞–ø–∏—Å–µ–π")
                    return df
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
                    continue

        # –ï—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ –≤ DQN —á–µ—Ä–µ–∑ –ë–î/–±–∏—Ä–∂—É
        try:
            from utils.db_utils import db_get_or_fetch_ohlcv
            candles_limit = self._estimate_required_candles(timeframe)
            print(f"üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –ë–î/–±–∏—Ä–∂–∏ {symbol} {timeframe} ~ {candles_limit} —Å–≤–µ—á–µ–π...")
            df = db_get_or_fetch_ohlcv(
                symbol_name=symbol,
                timeframe=timeframe,
                limit_candles=candles_limit,
                dry_run=False
            )
            if isinstance(df, pd.DataFrame) and not df.empty:
                print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –ë–î/–±–∏—Ä–∂–∏: {symbol} {timeframe}: {len(df)} –∑–∞–ø–∏—Å–µ–π")
                return df
            else:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î/–±–∏—Ä–∂–∏: –ø—É—Å—Ç–æ–π DataFrame –¥–ª—è {symbol} {timeframe}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑ –ë–î/–±–∏—Ä–∂–∏ –¥–ª—è {symbol} {timeframe}: {e}")

        print(f"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} {timeframe} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None

    def _estimate_required_candles(self, timeframe: str) -> int:
        """–û—Ü–µ–Ω–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–≤–µ—á–µ–π –ø–æ –¥–∞—Ç–∞–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            from datetime import datetime
            start = datetime.fromisoformat(getattr(self.config, 'train_start_date', '2023-01-01'))
            end = datetime.fromisoformat(getattr(self.config, 'test_end_date', '2024-12-01'))
            total_minutes = max(1, int((end - start).total_seconds() // 60))
            minutes_per_candle = {
                '1m': 1,
                '3m': 3,
                '5m': 5,
                '15m': 15,
                '30m': 30,
                '1h': 60,
                '4h': 240,
                '1d': 1440,
            }.get(timeframe, 5)
            candles = total_minutes // minutes_per_candle
            # –û–≥—Ä–∞–Ω–∏—á–∏–º —Ä–∞–∑—É–º–Ω—ã–º –≤–µ—Ä—Ö–Ω–∏–º –ø—Ä–µ–¥–µ–ª–æ–º
            return max(1000, min(candles, 200_000))
        except Exception:
            # –ë—ç–∫–∞–ø –∑–Ω–∞—á–µ–Ω–∏–µ
            return 50_000
    
    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
        df = df.dropna()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å timestamp –∫–æ–ª–æ–Ω–∫–∞, —É–±–∏—Ä–∞–µ–º –µ–µ
        if 'timestamp' in df.columns or 'open_time' in df.columns:
            timestamp_cols = [col for col in df.columns if 'time' in col.lower()]
            df = df.drop(columns=timestamp_cols)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        available_columns = df.columns.tolist()
        missing_columns = [col for col in required_columns if col not in available_columns]
        
        if missing_columns:
            print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            column_mapping = {}
            for col in required_columns:
                if col not in available_columns:
                    # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                    for available_col in available_columns:
                        if col in available_col.lower() or available_col.lower() in col:
                            column_mapping[available_col] = col
                            break
            
            if column_mapping:
                df = df.rename(columns=column_mapping)
                print(f"‚úÖ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {column_mapping}")
            else:
                raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        df = df[required_columns]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
        df = df.astype(float)
        
        return df
    
    def create_labels(self, df: pd.DataFrame, horizon: int = 1, threshold: float = 0.01) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã.
        –ü–æ–¥–¥–µ—Ä–∂–∫–∞ binary (0/1) –∏ ternary (0/1/2) —Å—Ö–µ–º —á–µ—Ä–µ–∑ config.label_scheme."""
        close_prices = df['close'].values
        
        # –í—ã—á–∏—Å–ª—è–µ–º –±—É–¥—É—â–∏–µ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ horizon —à–∞–≥–æ–≤
        future_prices = np.roll(close_prices, -horizon)
        future_prices[-horizon:] = close_prices[-horizon:]
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        price_changes = (future_prices - close_prices) / np.clip(close_prices, 1e-8, None)
        
        if getattr(self.config, 'label_scheme', 'binary') == 'binary':
            # –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏: 1 ‚Äî —Ä–æ—Å—Ç, 0 ‚Äî –ø–∞–¥–µ–Ω–∏–µ (–∫–∞–∫ –≤ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–µ)
            labels = (price_changes > 0).astype(int)
        else:
            # –¢–µ—Ä–Ω–∞—Ä–Ω—ã–µ: 0 ‚Äî –ø–∞–¥–µ–Ω–∏–µ, 1 ‚Äî –±–æ–∫–æ–≤–æ–µ, 2 ‚Äî —Ä–æ—Å—Ç
            labels = np.ones(len(price_changes), dtype=int)
            labels[price_changes < -threshold] = 0
            labels[price_changes > threshold] = 2
            
            # –ü–æ –∂–µ–ª–∞–Ω–∏—é: –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
            label_noise = getattr(self.config, 'label_noise', 0.0)
            if label_noise and label_noise > 0:
                noise_mask = np.random.random(len(labels)) < float(label_noise)
                labels[noise_mask] = np.random.randint(0, 3, size=int(np.sum(noise_mask)))
        
        return labels
    
    def normalize_data(self, data: np.ndarray, symbol: str, timeframe: str, 
                      fit_scaler: bool = True) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        scaler_key = f"{symbol}_{timeframe}"
        
        if fit_scaler:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º RobustScaler –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –≤—ã–±—Ä–æ—Å–∞–º
            scaler = RobustScaler()
            normalized_data = scaler.fit_transform(data)
            self.scalers[scaler_key] = scaler
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.feature_means[scaler_key] = np.mean(data, axis=0)
            self.feature_stds[scaler_key] = np.std(data, axis=0)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π scaler
            if scaler_key in self.scalers:
                scaler = self.scalers[scaler_key]
                normalized_data = scaler.transform(data)
            else:
                print(f"‚ö†Ô∏è Scaler –¥–ª—è {scaler_key} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º StandardScaler")
                scaler = StandardScaler()
                normalized_data = scaler.fit_transform(data)
                self.scalers[scaler_key] = scaler
        
        return normalized_data
    
    def prepare_training_data(self, symbols: List[str], timeframes: List[str]) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        all_data = {}
        
        for symbol in symbols:
            for timeframe in timeframes:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                df = self.load_symbol_data(symbol, timeframe)
                if df is None:
                    continue
                
                # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                df = self.preprocess_dataframe(df)
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏
                labels = self.create_labels(df, self.config.prediction_horizon, 
                                          self.config.prediction_threshold)
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
                data_array = df.values
                normalized_data = self.normalize_data(data_array, symbol, timeframe, fit_scaler=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                key = f"{symbol}_{timeframe}"
                all_data[key] = {
                    'data': normalized_data,
                    'labels': labels,
                    'symbol': symbol,
                    'timeframe': timeframe,
                    'scaler': self.scalers[key]
                }
                
                print(f"üìä {key}: {len(normalized_data)} –æ–±—Ä–∞–∑—Ü–æ–≤, "
                      f"–º–µ—Ç–∫–∏: {np.bincount(labels)}")
        
        return all_data
    
    def create_datasets(self, data_dict: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        datasets = {}
        
        for key, data_info in data_dict.items():
            symbol = data_info['symbol']
            timeframe = data_info['timeframe']
            data = data_info['data']
            labels = data_info['labels']
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–∞
            if timeframe == "5m":
                seq_len = self.config.sequence_length
            elif timeframe == "15m":
                seq_len = self.config.sequence_length // 3
            elif timeframe == "1h":
                seq_len = self.config.sequence_length // 12
            else:
                seq_len = self.config.sequence_length // 2
            
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = CryptoDataset(data, labels, seq_len)
            datasets[key] = dataset
            
            print(f"üì¶ –°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç {key}: {len(dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        return datasets
    
    def create_multiframe_dataset(self, data_dict: Dict[str, Any]) -> Tuple[Dataset, Dataset]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        symbols_data = {}
        
        for key, data_info in data_dict.items():
            symbol = data_info['symbol']
            timeframe = data_info['timeframe']
            data = data_info['data']
            labels = data_info['labels']
            
            if symbol not in symbols_data:
                symbols_data[symbol] = {
                    'data': {},
                    'labels_per_tf': {}
                }

            symbols_data[symbol]['data'][timeframe] = data
            symbols_data[symbol]['labels_per_tf'][timeframe] = labels
        
        # –°–æ–∑–¥–∞–µ–º –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        train_datasets = []
        val_datasets = []
        
        for symbol, symbol_data in symbols_data.items():
            data_dict_multi = symbol_data['data']
            labels_per_tf = symbol_data['labels_per_tf']
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–µ–π–º–∞
            sequence_lengths = {}
            for timeframe in data_dict_multi.keys():
                if timeframe == "5m":
                    seq_len = self.config.sequence_length
                elif timeframe == "15m":
                    seq_len = self.config.sequence_length // 3
                elif timeframe == "1h":
                    seq_len = self.config.sequence_length // 12
                else:
                    seq_len = self.config.sequence_length // 2
                
                sequence_lengths[timeframe] = seq_len
            
            # –í—ã–±–∏—Ä–∞–µ–º –æ–ø–æ—Ä–Ω—ã–µ –º–µ—Ç–∫–∏: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π —Ñ—Ä–µ–π–º (5m) –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤
            if '5m' in labels_per_tf:
                base_labels = labels_per_tf['5m']
                print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∫–∏ –æ—Ç 5m —Ñ—Ä–µ–π–º–∞: {len(base_labels)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            else:
                # –≤—ã–±–∏—Ä–∞–µ–º —Ñ—Ä–µ–π–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω–æ–π –¥–∞–Ω–Ω—ã—Ö
                tf_max = max(data_dict_multi.keys(), key=lambda tf: len(data_dict_multi[tf]))
                base_labels = labels_per_tf[tf_max]
                print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∫–∏ –æ—Ç {tf_max} —Ñ—Ä–µ–π–º–∞: {len(base_labels)} –æ–±—Ä–∞–∑—Ü–æ–≤")

            # –ù–ï –æ–±—Ä–µ–∑–∞–µ–º –¥–∞–Ω–Ω—ã–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–µ–π–º–∞
            # MultiTimeframeDataset —Å–∞–º –±—É–¥–µ—Ç –±—Ä–∞—Ç—å –Ω—É–∂–Ω—ã–µ —Å—Ä–µ–∑—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞
            
            # –°–æ–∑–¥–∞–µ–º –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            dataset = MultiTimeframeDataset(data_dict_multi, base_labels, sequence_lengths)
            
            print(f"üìä –î–∞—Ç–∞—Å–µ—Ç {symbol}: {len(dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            print(f"üìä –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ñ—Ä–µ–π–º–∞–º: {[(tf, len(data_dict_multi[tf])) for tf in data_dict_multi.keys()]}")
            print(f"üìä –†–∞–∑–º–µ—Ä –º–µ—Ç–æ–∫: {len(base_labels)}")
            print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫: {np.bincount(base_labels)}")
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
            train_size = int(len(dataset) * (1 - self.config.validation_split))
            val_size = len(dataset) - train_size
            
            train_dataset, val_dataset = torch.utils.data.random_split(
                dataset, [train_size, val_size]
            )
            
            print(f"üìä Train: {train_size} –æ–±—Ä–∞–∑—Ü–æ–≤, Val: {val_size} –æ–±—Ä–∞–∑—Ü–æ–≤")
            
            train_datasets.append(train_dataset)
            val_datasets.append(val_dataset)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        if len(train_datasets) > 1:
            train_dataset = torch.utils.data.ConcatDataset(train_datasets)
            val_dataset = torch.utils.data.ConcatDataset(val_datasets)
        else:
            train_dataset = train_datasets[0]
            val_dataset = val_datasets[0]
        
        return train_dataset, val_dataset
    
    def create_dataloaders(self, train_dataset: Dataset, val_dataset: Dataset) -> Tuple[DataLoader, DataLoader]:
        """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤"""
        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ WeightedRandomSampler –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        sampler = None
        if getattr(self.config, 'class_balance', 'auto') == 'auto':
            try:
                # –û—Ü–µ–Ω–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
                targets = []
                if hasattr(train_dataset, 'dataset') and hasattr(train_dataset.dataset, 'labels'):
                    # Subset –∏–∑ random_split
                    labels_full = train_dataset.dataset.labels
                    indices = train_dataset.indices if hasattr(train_dataset, 'indices') else range(len(train_dataset))
                    max_seq = getattr(train_dataset.dataset, 'sequence_length', 1)
                    for idx in indices:
                        label_idx = min(idx + max_seq - 1, len(labels_full) - 1)
                        targets.append(int(labels_full[label_idx]))
                elif hasattr(train_dataset, 'labels'):
                    labels_full = train_dataset.labels
                    max_seq = getattr(train_dataset, 'max_sequence_length', 1)
                    for idx in range(len(train_dataset)):
                        label_idx = min(idx + max_seq - 1, len(labels_full) - 1)
                        targets.append(int(labels_full[label_idx]))
                
                if targets:
                    counts = np.bincount(np.array(targets), minlength=self.config.num_classes)
                    class_weights = 1.0 / np.clip(counts, 1, None)
                    sample_weights = [class_weights[t] for t in targets]
                    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)
            except Exception:
                sampler = None
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=(sampler is None),
            sampler=sampler,
            num_workers=4,
            pin_memory=True,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True,
            drop_last=False
        )
        
        return train_loader, val_loader
