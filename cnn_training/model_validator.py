"""
–ú–æ–¥—É–ª—å –≤–∞–ª–∏–¥–∞—Ü–∏–∏ CNN –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–∞—Ö
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ —É—á–∞—Å—Ç–≤–æ–≤–∞–≤—à–∏—Ö –≤ –æ–±—É—á–µ–Ω–∏–∏
"""

import os
import json
import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union
import torch
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import ccxt

from .models import TradingCNN, MultiTimeframeCNN
from .data_loader import CryptoDataLoader
from .feature_extractor import CNNFeatureExtractor

logger = logging.getLogger(__name__)

class CNNModelValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä CNN –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –Ω–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–∞—Ö"""
    
    def __init__(self, config_path: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
        
        Args:
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        self.config_path = config_path
        
        # –°–∏–º–≤–æ–ª—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–Ω–µ —É—á–∞—Å—Ç–≤–æ–≤–∞–≤—à–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–∏)
        self.validation_symbols = ['SOLUSDT', 'XRPUSDT', 'TONUSDT']
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.timeframes = ['5m', '15m', '1h']
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è data loader
        from .config import CNNTrainingConfig
        data_config = CNNTrainingConfig(
            symbols=self.validation_symbols,
            timeframes=self.timeframes,
            device="cpu"
        )
        self.data_loader = CryptoDataLoader(data_config)
        
        # –°–æ–∑–¥–∞–µ–º feature extractor —Å —Ç–æ–π –∂–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        self.feature_extractor = CNNFeatureExtractor(data_config)
        
        # –ü–µ—Ä–∏–æ–¥—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.test_periods = {
            'last_year': 365,  # –ü–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥
            'last_6_months': 180,  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 6 –º–µ—Å—è—Ü–µ–≤
            'last_3_months': 90,   # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –º–µ—Å—è—Ü–∞
            'last_month': 30       # –ü–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü
        }
    
    def validate_model(self, model_path: str, test_symbols: List[str] = None, 
                      test_period: str = 'last_year') -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è CNN –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–∞—Ö
        
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
            test_symbols: –°–∏–º–≤–æ–ª—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            test_period: –ü–µ—Ä–∏–æ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        try:
            logger.info(f"üß™ –ù–∞—á–∏–Ω–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –º–æ–¥–µ–ª–∏: {model_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            model = self._load_model(model_path)
            if not model:
                return {'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å'}
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–º–≤–æ–ª—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            if not test_symbols:
                test_symbols = self.validation_symbols
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–∏–æ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            days_back = self.test_periods.get(test_period, 365)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days_back)
            
            logger.info(f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Å–∏–º–≤–æ–ª–∞—Ö: {test_symbols}")
            logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –Ω–∞ –∫–∞–∂–¥–æ–º —Å–∏–º–≤–æ–ª–µ
            symbol_results = []
            total_samples = 0
            total_correct = 0
            
            for symbol in test_symbols:
                logger.info(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∏–º–≤–æ–ª: {symbol}")
                
                symbol_result = self._validate_symbol(
                    model, symbol, start_date, end_date
                )
                
                if symbol_result['success']:
                    symbol_results.append(symbol_result)
                    total_samples += symbol_result['samples_tested']
                    total_correct += symbol_result['correct_predictions']
                else:
                    logger.warning(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è {symbol}: {symbol_result['error']}")
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            overall_accuracy = total_correct / total_samples if total_samples > 0 else 0
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendation = self._generate_recommendation(symbol_results, overall_accuracy)
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            try:
                from .validation_analyzer import analyze_validation_results
                detailed_analysis = analyze_validation_results({
                    'overall_accuracy': overall_accuracy,
                    'total_samples': total_samples,
                    'symbol_results': symbol_results,
                    'test_symbols': test_symbols,
                    'test_period': test_period
                })
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {str(e)}")
                detailed_analysis = {}
            
            result = {
                'success': True,
                'model_path': model_path,
                'test_symbols': test_symbols,
                'test_period': test_period,
                'total_samples': total_samples,
                'overall_accuracy': overall_accuracy,
                'symbol_results': symbol_results,
                'recommendation': recommendation,
                'detailed_analysis': detailed_analysis,
                'validation_date': datetime.now().isoformat()
            }
            
            logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {overall_accuracy:.2%}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _load_model(self, model_path: str) -> Optional[Union[TradingCNN, MultiTimeframeCNN]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ CNN –º–æ–¥–µ–ª–∏"""
        try:
            if not os.path.exists(model_path):
                logger.error(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
                return None
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
            manifest_path = os.path.join(os.path.dirname(model_path), 'manifest.json')
            if os.path.exists(manifest_path):
                with open(manifest_path, 'r') as f:
                    manifest = json.load(f)
            else:
                manifest = {}
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
            checkpoint = torch.load(model_path, map_location='cpu')
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ –∫–ª—é—á–∞–º –≤ state_dict
            model_type = manifest.get('model_type', 'unknown')
            is_multiframe = any(key.startswith('cnn_models.') for key in state_dict.keys())
            
            logger.info(f"üîç –¢–∏–ø –º–æ–¥–µ–ª–∏: {model_type}, –ú—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤–∞—è: {is_multiframe}")
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
            config = manifest.get('config', {})
            
            if is_multiframe or model_type == 'multiframe':
                # –ú—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤–∞—è –º–æ–¥–µ–ª—å
                timeframes = config.get('timeframes', ['5m', '15m', '1h'])
                sequence_lengths = {
                    '5m': 50,
                    '15m': 40, 
                    '1h': 30
                }
                
                model = MultiTimeframeCNN(
                    input_channels=config.get('input_channels', 4),
                    hidden_channels=config.get('hidden_channels', [32, 64, 128]),
                    output_features=config.get('output_features', 64),
                    sequence_lengths=sequence_lengths,
                    dropout_rate=config.get('dropout_rate', 0.2),
                    use_batch_norm=config.get('use_batch_norm', True)
                )
                logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤–∞—è –º–æ–¥–µ–ª—å")
            else:
                # –û–±—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å
                model = TradingCNN(
                    input_channels=config.get('input_channels', 4),
                    hidden_channels=config.get('hidden_channels', [32, 64, 128]),
                    output_features=config.get('output_features', 64),
                    dropout_rate=config.get('dropout_rate', 0.2),
                    use_batch_norm=config.get('use_batch_norm', True)
                )
                logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –æ–±—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            try:
                model.load_state_dict(state_dict)
                logger.info("‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤: {str(e)}")
                return None
            
            model.eval()
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
            return model
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
            return None
    
    def _validate_symbol(self, model: Union[TradingCNN, MultiTimeframeCNN], symbol: str, 
                        start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —Å–∏–º–≤–æ–ª–µ"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–∏–º–≤–æ–ª–∞
            data = self._load_symbol_data(symbol, start_date, end_date)
            if data is None or len(data) < 100:
                return {
                    'success': False,
                    'error': f'–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}'
                }
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            test_data = self._prepare_test_data(data, symbol)
            if not test_data:
                return {
                    'success': False,
                    'error': f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}'
                }
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∞–∫–∂–µ –º–µ—Ç–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏)
            predictions, confidences, patterns, labels_eval = self._test_model_on_data(
                model, test_data
            )
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            accuracy = self._calculate_accuracy(labels_eval, predictions)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ (features –º–æ–≥—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –¥–ª—è –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–∞)
            error_analysis = self._analyze_errors(
                test_data['labels'], predictions, test_data.get('features', np.array([]))
            )
            
            result = {
                'success': True,
                'symbol': symbol,
                'samples_tested': len(predictions),
                'correct_predictions': int(accuracy * len(predictions)),
                'accuracy': accuracy,
                'avg_confidence': np.mean(confidences),
                'test_period': f"{start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}",
                'patterns_detected': patterns,
                'error_analysis': error_analysis
            }
            
            logger.info(f"‚úÖ {symbol}: —Ç–æ—á–Ω–æ—Å—Ç—å {accuracy:.2%}, –æ–±—Ä–∞–∑—Ü–æ–≤ {len(predictions)}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {symbol}: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _load_symbol_data(self, symbol: str, start_date: datetime, 
                         end_date: datetime) -> Optional[pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ —Ç–µ–º–∏ –∂–µ —Ñ—É–Ω–∫—Ü–∏—è–º–∏, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏"""
        try:
            all_data = []

            for timeframe in self.timeframes:
                try:
                    # 1) –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
                    df_raw = self.data_loader.load_symbol_data(symbol, timeframe)
                    if df_raw is None or df_raw.empty:
                        continue

                    # 2) –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–µ—Ä–∏–æ–¥—É –î–û –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                    df = df_raw.copy()
                    ts_col = None
                    for cand_col in [
                        'timestamp', 'open_time', 'start_time', 'date', 'datetime'
                    ]:
                        if cand_col in df.columns:
                            ts_col = cand_col
                            break

                    if ts_col is not None:
                        try:
                            sample = df[ts_col].dropna().iloc[0]
                            unit = None
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º unit –ø–æ –º–∞—Å—à—Ç–∞–±—É (—Å–µ–∫/–º—Å)
                            if isinstance(sample, (int, float, np.integer, np.floating)):
                                val = float(sample)
                                if val > 1e11:
                                    unit = 'ms'   # –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                                elif val > 1e9:
                                    unit = 's'    # —Å–µ–∫—É–Ω–¥—ã (–∑–∞ –∑–∞–ø–∞—Å –≤–æ–∑—å–º–µ–º)
                                else:
                                    unit = 's'
                                df[ts_col] = pd.to_datetime(df[ts_col], unit=unit, errors='coerce')
                            else:
                                # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
                                df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce', utc=False)
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–µ—Ä–∏–æ–¥—É
                            df = df[(df[ts_col] >= start_date) & (df[ts_col] <= end_date)]
                        except Exception:
                            # –í —Å–ª—É—á–∞–µ –ª—é–±–æ–π –æ—à–∏–±–∫–∏ –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                            pass

                    if df is None or df.empty:
                        continue

                    # 3) –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏ (—É–¥–∞–ª–∏—Ç timestamp –∫–æ–ª–æ–Ω–∫–∏)
                    df = self.data_loader.preprocess_dataframe(df)

                    if df is not None and not df.empty:
                        df['timeframe'] = timeframe
                        all_data.append(df)

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {symbol} {timeframe}: {str(e)}")
                    continue

            if not all_data:
                return None

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–µ–∑ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —É–∂–µ —É–¥–∞–ª–µ–Ω—ã –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–æ–º)
            combined_data = pd.concat(all_data, ignore_index=True)
            return combined_data

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö {symbol}: {str(e)}")
            return None
    
    def _prepare_test_data(self, data: pd.DataFrame, symbol: str) -> Optional[Dict]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ OHLCV –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)"""
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame –Ω–∞ —Ñ—Ä–µ–π–º—ã
            data_by_tf: Dict[str, pd.DataFrame] = {}
            for tf in self.timeframes:
                df_tf = data[data.get('timeframe') == tf] if 'timeframe' in data.columns else data
                if df_tf is not None and not df_tf.empty:
                    # –î–µ—Ä–∂–∏–º —Ç–æ–ª—å–∫–æ OHLCV
                    df_tf = df_tf[['open', 'high', 'low', 'close', 'volume']]
                    data_by_tf[tf] = df_tf

            if not data_by_tf:
                return None

            # –î–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∫–∞–∫ –≤ MultiTimeframeCNN
            seq_len_map = {'5m': 50, '15m': 40, '1h': 30}

            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–µ–π–º–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [N, seq_len, 5]
            sequences_by_tf: Dict[str, List[np.ndarray]] = {}
            num_samples_per_tf: Dict[str, int] = {}

            for tf, df_tf in data_by_tf.items():
                seq_len = seq_len_map.get(tf, 50)
                arr = df_tf.values.astype(float)
                if len(arr) < seq_len + 1:
                    continue
                seq_list: List[np.ndarray] = []
                for i in range(len(arr) - seq_len):
                    window = arr[i:i + seq_len]            # [seq_len, 5]
                    # TradingCNN –æ–∂–∏–¥–∞–µ—Ç [batch, seq_len, channels]
                    seq_list.append(window)
                if seq_list:
                    sequences_by_tf[tf] = seq_list
                    num_samples_per_tf[tf] = len(seq_list)

            if not sequences_by_tf:
                return None

            # –ö–æ–ª-–≤–æ –æ–±—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ = –º–∏–Ω–∏–º—É–º –ø–æ —Ñ—Ä–µ–π–º–∞–º, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ
            common_samples = min(num_samples_per_tf.values()) if len(num_samples_per_tf) > 0 else 0
            if common_samples == 0:
                return None

            # –ú–µ—Ç–∫–∏ —Å—á–∏—Ç–∞–µ–º –ø–æ –±–∞–∑–æ–≤–æ–º—É —Ñ—Ä–µ–π–º—É 5m, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å, –∏–Ω–∞—á–µ –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π
            base_tf = '5m' if '5m' in data_by_tf else list(data_by_tf.keys())[0]
            base_seq_len = seq_len_map.get(base_tf, 50)
            base_close = data_by_tf[base_tf]['close'].values.astype(float)
            # –ú–µ—Ç–∫–∏ –∫–∞–∫ –≤ _create_labels (—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥)
            base_labels_full = (np.diff(base_close) > 0).astype(int)
            base_labels_full = np.append(base_labels_full, 0)
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ –æ–∫–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            labels_for_samples = base_labels_full[base_seq_len - 1 : base_seq_len - 1 + common_samples]
            if len(labels_for_samples) < common_samples:
                return None

            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã
            features_multiframe: Dict[str, np.ndarray] = {}
            for tf, seq_list in sequences_by_tf.items():
                features_multiframe[tf] = np.stack(seq_list[:common_samples], axis=0)  # [N, seq_len, 5]

            return {
                'features_multiframe': features_multiframe,
                'labels': labels_for_samples,
                'symbol': symbol
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
            return None
    
    def _extract_simple_features(self, data: pd.DataFrame) -> np.ndarray:
        """–ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            features = []
            
            # OHLCV –¥–∞–Ω–Ω—ã–µ
            ohlcv = data[['open', 'high', 'low', 'close', 'volume']].values
            
            # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            for i in range(len(ohlcv)):
                if i < 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å–≤–µ—á–µ–π
                    continue
                    
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–≤–µ—á–µ–π –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                window = ohlcv[i-10:i+1]
                
                # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                feature_vector = []
                
                # –¢–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
                current = window[-1]
                feature_vector.extend([
                    current[0],  # open
                    current[1],  # high
                    current[2],  # low
                    current[3],  # close
                    current[4]   # volume
                ])
                
                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                if len(window) > 1:
                    prev = window[-2]
                    feature_vector.extend([
                        (current[3] - prev[3]) / prev[3],  # price change
                        (current[1] - current[2]) / current[3],  # high-low ratio
                        current[4] / np.mean(window[:, 4]) if np.mean(window[:, 4]) > 0 else 0  # volume ratio
                    ])
                else:
                    feature_vector.extend([0, 0, 0])
                
                # –ü—Ä–æ—Å—Ç—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
                if len(window) >= 5:
                    sma5 = np.mean(window[-5:, 3])  # 5-period SMA
                    feature_vector.append((current[3] - sma5) / sma5 if sma5 > 0 else 0)
                else:
                    feature_vector.append(0)
                
                features.append(feature_vector)
            
            return np.array(features)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}")
            return np.array([])
    
    def _create_labels(self, data: pd.DataFrame) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: —Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ —Ü–µ–Ω—ã –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø–µ—Ä–∏–æ–¥
            prices = data['close'].values
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
            price_changes = np.diff(prices)
            
            # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏: 1 - —Ä–æ—Å—Ç, 0 - –ø–∞–¥–µ–Ω–∏–µ
            labels = (price_changes > 0).astype(int)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç (–Ω–µ –º–æ–∂–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å)
            labels = np.append(labels, 0)
            
            return labels
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫: {str(e)}")
            return np.zeros(len(data))
    
    def _test_model_on_data(self, model: Union[TradingCNN, MultiTimeframeCNN], test_data: Dict) -> Tuple:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: –∏–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –æ–±—É—á–∞–µ–º –ª—ë–≥–∫–∏–π –ª–∏–Ω–µ–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (linear probe)."""
        try:
            labels_all = test_data['labels']

            # –°–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ X
            X_features: List[np.ndarray] = []
            is_multiframe = isinstance(model, MultiTimeframeCNN)

            with torch.no_grad():
                if is_multiframe and 'features_multiframe' in test_data:
                    features_by_tf: Dict[str, np.ndarray] = test_data['features_multiframe']
                    num_samples = min(arr.shape[0] for arr in features_by_tf.values())
                    for i in range(num_samples):
                        try:
                            multi_data = {}
                            for tf, arr in features_by_tf.items():
                                multi_data[tf] = torch.tensor(arr[i], dtype=torch.float32).unsqueeze(0)
                            output = model(multi_data)
                            if isinstance(output, dict):
                                output = output.get('prediction', output.get('output', list(output.values())[0]))
                            X_features.append(output.squeeze(0).cpu().numpy())
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—Ä–∞–∑—Ü–∞ {i}: {str(e)}")
                else:
                    features = test_data.get('features')
                    if features is None:
                        return [], [], [], np.array([])
                    for i in range(len(features)):
                        try:
                            feature_tensor = torch.FloatTensor(features[i]).unsqueeze(0)
                            output = model(feature_tensor)
                            if isinstance(output, dict):
                                output = output.get('prediction', output.get('output', list(output.values())[0]))
                            X_features.append(output.squeeze(0).cpu().numpy())
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—Ä–∞–∑—Ü–∞ {i}: {str(e)}")

            if not X_features:
                return [], [], [], np.array([])

            X = np.vstack(X_features)
            y = np.asarray(labels_all[: len(X)])

            # –í—Ä–µ–º–µ–Ω–Ω–æ–π split (–±–µ–∑ —É—Ç–µ—á–∫–∏ –±—É–¥—É—â–µ–≥–æ)
            if len(X) < 50:
                return [], [], [], np.array([])
            split_idx = int(len(X) * 0.8)
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]

            # Linear probe
            from sklearn.linear_model import LogisticRegression
            clf = LogisticRegression(max_iter=1000, n_jobs=1)
            try:
                clf.fit(X_train, y_train)
                proba = clf.predict_proba(X_test)  # [:, 1] –µ—Å–ª–∏ –±–∏–Ω–∞—Ä–Ω–∞—è
                if proba.shape[1] == 2:
                    conf = proba[:, 1]
                    preds = (conf >= 0.5).astype(int)
                    confidences = conf.tolist()
                else:
                    preds = np.argmax(proba, axis=1)
                    confidences = np.max(proba, axis=1).tolist()
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è linear probe: {str(e)}")
                return [], [], [], np.array([])

            patterns = ["linear_probe"]
            return preds.tolist(), confidences, list(set(patterns)), y_test
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}")
            return [], [], [], np.array([])
    
    def _calculate_accuracy(self, true_labels: np.ndarray, 
                           predictions: List[int]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        try:
            if len(true_labels) != len(predictions):
                min_length = min(len(true_labels), len(predictions))
                true_labels = true_labels[:min_length]
                predictions = predictions[:min_length]
            
            return accuracy_score(true_labels, predictions)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏: {str(e)}")
            return 0.0
    
    def _analyze_errors(self, true_labels: np.ndarray, predictions: List[int], 
                       features: np.ndarray = None) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–∏"""
        try:
            if len(true_labels) != len(predictions):
                min_length = min(len(true_labels), len(predictions))
                true_labels = true_labels[:min_length]
                predictions = predictions[:min_length]
            
            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            cm = confusion_matrix(true_labels, predictions)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
            report = classification_report(
                true_labels, predictions, 
                output_dict=True, zero_division=0
            )
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
            error_analysis = {
                'confusion_matrix': cm.tolist(),
                'classification_report': report,
                'error_rate': 1 - accuracy_score(true_labels, predictions),
                'error_patterns': self._identify_error_patterns(true_labels, predictions),
                'class_balance': self._analyze_class_balance(true_labels),
                'prediction_confidence': self._analyze_prediction_confidence(predictions, true_labels)
            }
            
            return error_analysis
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫: {str(e)}")
            return {}
    
    def _identify_error_patterns(self, true_labels: np.ndarray, predictions: List[int]) -> Dict[str, Any]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫"""
        try:
            errors = []
            correct = []
            
            for i, (true, pred) in enumerate(zip(true_labels, predictions)):
                if true != pred:
                    errors.append({
                        'index': i,
                        'true_label': int(true),
                        'predicted_label': int(pred),
                        'error_type': f"True_{int(true)}_Pred_{int(pred)}"
                    })
                else:
                    correct.append({
                        'index': i,
                        'true_label': int(true),
                        'predicted_label': int(pred)
                    })
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫
            error_types = {}
            for error in errors:
                error_type = error['error_type']
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            return {
                'total_errors': len(errors),
                'total_correct': len(correct),
                'error_types': error_types,
                'most_common_error': max(error_types.items(), key=lambda x: x[1]) if error_types else None,
                'error_rate_by_type': {k: v/len(errors) for k, v in error_types.items()} if errors else {}
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫: {str(e)}")
            return {}
    
    def _analyze_class_balance(self, labels: np.ndarray) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""
        try:
            unique, counts = np.unique(labels, return_counts=True)
            total = len(labels)
            
            balance = {}
            for label, count in zip(unique, counts):
                balance[int(label)] = {
                    'count': int(count),
                    'percentage': float(count / total * 100)
                }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å
            max_count = max(counts)
            min_count = min(counts)
            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
            
            return {
                'class_distribution': balance,
                'imbalance_ratio': imbalance_ratio,
                'is_balanced': imbalance_ratio < 2.0,
                'dominant_class': int(unique[np.argmax(counts)]),
                'minority_class': int(unique[np.argmin(counts)])
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {str(e)}")
            return {}
    
    def _analyze_prediction_confidence(self, predictions: List[int], true_labels: np.ndarray) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ - —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ–¥—Ä—è–¥ –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ
            confidence_patterns = []
            current_pattern = 1
            current_pred = predictions[0]
            
            for i in range(1, len(predictions)):
                if predictions[i] == current_pred:
                    current_pattern += 1
                else:
                    confidence_patterns.append({
                        'prediction': int(current_pred),
                        'length': current_pattern,
                        'confidence_level': 'high' if current_pattern >= 3 else 'medium' if current_pattern >= 2 else 'low'
                    })
                    current_pred = predictions[i]
                    current_pattern = 1
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
            confidence_patterns.append({
                'prediction': int(current_pred),
                'length': current_pattern,
                'confidence_level': 'high' if current_pattern >= 3 else 'medium' if current_pattern >= 2 else 'low'
            })
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            high_conf = sum(1 for p in confidence_patterns if p['confidence_level'] == 'high')
            medium_conf = sum(1 for p in confidence_patterns if p['confidence_level'] == 'medium')
            low_conf = sum(1 for p in confidence_patterns if p['confidence_level'] == 'low')
            
            return {
                'confidence_patterns': confidence_patterns,
                'high_confidence_count': high_conf,
                'medium_confidence_count': medium_conf,
                'low_confidence_count': low_conf,
                'avg_pattern_length': np.mean([p['length'] for p in confidence_patterns]),
                'confidence_distribution': {
                    'high': high_conf / len(confidence_patterns) * 100,
                    'medium': medium_conf / len(confidence_patterns) * 100,
                    'low': low_conf / len(confidence_patterns) * 100
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {str(e)}")
            return {}
    
    def _generate_recommendation(self, symbol_results: List[Dict], 
                               overall_accuracy: float) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            if overall_accuracy >= 0.7:
                return "‚úÖ –û—Ç–ª–∏—á–Ω–∞—è –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ —Ç–æ—Ä–≥–æ–≤–ª–µ."
            elif overall_accuracy >= 0.6:
                return "‚ö†Ô∏è –•–æ—Ä–æ—à–∞—è –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ."
            elif overall_accuracy >= 0.5:
                return "‚ùå –°–ª–∞–±–∞—è –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
            else:
                return "‚ùå –ü–ª–æ—Ö–∞—è –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å. –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è."
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {str(e)}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."


def validate_cnn_model(model_path: str, test_symbols: List[str] = None, 
                      test_period: str = 'last_year') -> Dict[str, Any]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ CNN –º–æ–¥–µ–ª–∏
    
    Args:
        model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        test_symbols: –°–∏–º–≤–æ–ª—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_period: –ü–µ—Ä–∏–æ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    validator = CNNModelValidator()
    return validator.validate_model(model_path, test_symbols, test_period)
