"""
–¢—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è CNN –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import os
import time
from typing import Dict, List, Tuple, Optional, Any
import logging
from datetime import datetime

from .models import TradingCNN, MultiTimeframeCNN, PricePredictionCNN
from .data_loader import CryptoDataLoader, CryptoDataset, MultiTimeframeDataset
from .config import CNNTrainingConfig


class CNNTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è CNN –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, config: CNNTrainingConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self._setup_logging()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.data_loader = CryptoDataLoader(config)
        self.model = None
        self.classifier_head: Optional[nn.Module] = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        self.run_id: Optional[str] = None
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.best_val_accuracy = 0.0
        
        # Wandb (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if config.use_wandb:
            try:
                import wandb
                wandb.init(
                    project=config.wandb_project,
                    config=config.to_dict(),
                    name=f"cnn_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                )
                self.wandb = wandb
            except ImportError:
                print("‚ö†Ô∏è Wandb –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –æ—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ")
                self.wandb = None
        else:
            self.wandb = None
    
    def _setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Å–æ–ª—å (–±–µ–∑ —Ñ–∞–π–ª–æ–≤)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler()  # –¢–æ–ª—å–∫–æ –∫–æ–Ω—Å–æ–ª—å
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def create_model(self, model_type: str = "single") -> nn.Module:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞"""
        if model_type == "single":
            self.model = TradingCNN(
                input_channels=self.config.input_channels,
                hidden_channels=self.config.hidden_channels,
                output_features=self.config.output_features,
                sequence_length=self.config.sequence_length,
                dropout_rate=self.config.dropout_rate,
                use_batch_norm=self.config.use_batch_norm
            ).to(self.device)
        
        elif model_type == "multiframe":
            sequence_lengths = {
                "5m": self.config.sequence_length,
                "15m": self.config.sequence_length // 3,
                "1h": self.config.sequence_length // 12
            }
            
            self.model = MultiTimeframeCNN(
                input_channels=self.config.input_channels,
                hidden_channels=self.config.hidden_channels,
                output_features=self.config.output_features,
                sequence_lengths=sequence_lengths,
                dropout_rate=self.config.dropout_rate,
                use_batch_norm=self.config.use_batch_norm
            ).to(self.device)

            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ –º–µ—Ç–∫–∞–º (3 –∫–ª–∞—Å—Å–∞)
            self.classifier_head = nn.Linear(self.config.output_features, getattr(self.config, 'num_classes', 3)).to(self.device)
        
        elif model_type == "prediction":
            self.model = PricePredictionCNN(
                input_channels=self.config.input_channels,
                hidden_channels=self.config.hidden_channels,
                sequence_length=self.config.sequence_length,
                dropout_rate=self.config.dropout_rate,
                use_batch_norm=self.config.use_batch_norm,
                num_classes=self.config.num_classes  # 2 –∏–ª–∏ 3 –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ö–µ–º—ã
            ).to(self.device)
        
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
        
        self.logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å —Ç–∏–ø–∞ {model_type}: {self.model}")
        return self.model
    
    def setup_training(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        # –°–æ–±–µ—Ä–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: –º–æ–¥–µ–ª—å +, –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞
        params = list(self.model.parameters())
        if self.classifier_head is not None:
            params += list(self.classifier_head.parameters())
        if self.config.optimizer.lower() == "adam":
            self.optimizer = optim.Adam(
                params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer.lower() == "adamw":
            self.optimizer = optim.AdamW(
                params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer.lower() == "sgd":
            self.optimizer = optim.SGD(
                params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
                momentum=0.9
            )
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {self.config.optimizer}")
        
        # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        if self.config.scheduler.lower() == "cosine":
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=self.config.num_epochs
            )
        elif self.config.scheduler.lower() == "step":
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer, step_size=self.config.num_epochs // 3, gamma=0.1
            )
        elif self.config.scheduler.lower() == "plateau":
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='max', patience=5, factor=0.5
            )
        
        # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å —É—á–µ—Ç–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –∏ label smoothing
        label_smoothing = float(getattr(self.config, 'label_smoothing', 0.0))
        if getattr(self.config, 'class_balance', 'auto') == 'auto':
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–ª–∞–±–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤ –∫–ª–∞—Å—Å–∞ 1 –ø—Ä–∏ ternary –±—ã–ª–æ; —Ç–µ–ø–µ—Ä—å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ
            if self.config.num_classes == 2:
                class_weights = torch.tensor([1.0, 1.0], device=self.device)
            else:
                class_weights = torch.tensor([1.0, 1.0, 1.0], device=self.device)
            self.criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)
        else:
            self.criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
        
        self.logger.info(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {self.config.optimizer}")
        self.logger.info(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫: {self.config.scheduler}")
    
    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if isinstance(data, dict):
                # –ú—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                data = {k: v.to(self.device) for k, v in data.items()}
            else:
                data = data.to(self.device)
            target = target.to(self.device).squeeze()
            
            # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            self.optimizer.zero_grad()
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            if isinstance(self.model, MultiTimeframeCNN):
                features = self.model(data)
                output = self.classifier_head(features) if self.classifier_head is not None else features
            else:
                output = self.model(data)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
            loss = self.criterion(output, target)
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: weight decay —É–∂–µ –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–µ; –ø—Ä–∏–º–µ–Ω—è–µ–º step
            self.optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if batch_idx % self.config.log_interval == 0:
                self.logger.info(
                    f'Epoch: {len(self.train_losses)+1}, '
                    f'Batch: {batch_idx}/{len(train_loader)}, '
                    f'Loss: {loss.item():.4f}, '
                    f'Acc: {100.*correct/total:.2f}%'
                )
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                if isinstance(data, dict):
                    data = {k: v.to(self.device) for k, v in data.items()}
                else:
                    data = data.to(self.device)
                target = target.to(self.device).squeeze()
                
                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                if isinstance(self.model, MultiTimeframeCNN):
                    features = self.model(data)
                    output = self.classifier_head(features) if self.classifier_head is not None else features
                else:
                    output = self.model(data)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
                loss = self.criterion(output, target)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                total_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def save_model(self, symbol: str, timeframe: str, epoch: int, 
                   val_accuracy: float, is_best: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        save_dir = self.config.model_save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'classifier_state_dict': self.classifier_head.state_dict() if self.classifier_head is not None else None,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_accuracy': val_accuracy,
            'config': self.config.to_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'timestamp': datetime.now().isoformat()
        }
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π run_id –Ω–∞ –≤–µ—Å—å –∑–∞–ø—É—Å–∫, –µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω
        model_path = self.config.get_model_path(symbol, timeframe, run_id=self.run_id)
        torch.save(checkpoint, model_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ
        if is_best:
            best_model_path = model_path.replace('.pth', '_best.pth')
            torch.save(checkpoint, best_model_path)
            self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
        
        self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å —ç–ø–æ—Ö–∏ {epoch}: {model_path}")
    
    def train_single_model(self, symbol: str, timeframe: str, model_type: str = "prediction"):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–∞"""
        self.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ {model_type} –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol} {timeframe}")
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –µ–¥–∏–Ω—ã–π run_id –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞, –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω
        if not self.run_id:
            import time as _time
            self.run_id = hex(int(_time.time()))[-4:]
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.create_model(model_type)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        self.setup_training()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data_dict = self.data_loader.prepare_training_data([symbol], [timeframe])
        
        if not data_dict:
            self.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} {timeframe}")
            return None
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        datasets = self.data_loader.create_datasets(data_dict)
        key = f"{symbol}_{timeframe}"
        
        if key not in datasets:
            self.logger.error(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è {key} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        dataset = datasets[key]
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
        train_size = int(len(dataset) * (1 - self.config.validation_split))
        val_size = len(dataset) - train_size
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        # –°–æ–∑–¥–∞–µ–º DataLoader'—ã
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        self.logger.info(f"üìä –î–∞–Ω–Ω—ã–µ: train={len(train_dataset)}, val={len(val_dataset)}")
        
        # –û–±—É—á–µ–Ω–∏–µ
        best_val_accuracy = 0.0
        patience_counter = 0
        
        for epoch in range(self.config.num_epochs):
            start_time = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc = self.validate(val_loader)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_acc)
                else:
                    self.scheduler.step()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            is_best = val_acc > best_val_accuracy
            if is_best:
                best_val_accuracy = val_acc
                patience_counter = 0
            else:
                patience_counter += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            if epoch % 10 == 0 or is_best:
                self.save_model(symbol, timeframe, epoch, val_acc, is_best)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            epoch_time = time.time() - start_time
            self.logger.info(
                f'Epoch {epoch+1}/{self.config.num_epochs}: '
                f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
                f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, '
                f'Time: {epoch_time:.2f}s'
            )
            
            # Wandb –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if self.wandb:
                self.wandb.log({
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'train_accuracy': train_acc,
                    'val_accuracy': val_acc,
                    'learning_rate': self.optimizer.param_groups[0]['lr']
                })
            
            # Early stopping
            if patience_counter >= self.config.early_stopping_patience:
                self.logger.info(f"üõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                break
        
        self.logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_accuracy:.2f}%")
        
        return {
            'model': self.model,
            'best_val_accuracy': best_val_accuracy,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies
        }
    
    def train_multiframe_model(self, symbols: List[str]):
        """–û–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        self.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è {symbols}")
        # –ï–¥–∏–Ω—ã–π run_id –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
        if not self.run_id:
            import time as _time
            self.run_id = hex(int(_time.time()))[-4:]
        
        # –°–æ–∑–¥–∞–µ–º –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤—É—é –º–æ–¥–µ–ª—å
        self.create_model("multiframe")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        self.setup_training()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ñ—Ä–µ–π–º–æ–≤
        data_dict = self.data_loader.prepare_training_data(symbols, self.config.timeframes)
        
        if not data_dict:
            self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return None
        
        # –°–æ–∑–¥–∞–µ–º –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        train_dataset, val_dataset = self.data_loader.create_multiframe_dataset(data_dict)
        
        # –°–æ–∑–¥–∞–µ–º DataLoader'—ã
        train_loader, val_loader = self.data_loader.create_dataloaders(train_dataset, val_dataset)
        
        self.logger.info(f"üìä –î–∞–Ω–Ω—ã–µ: train={len(train_dataset)}, val={len(val_dataset)}")
        
        # –û–±—É—á–µ–Ω–∏–µ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ train_single_model)
        best_val_accuracy = 0.0
        patience_counter = 0
        
        for epoch in range(self.config.num_epochs):
            start_time = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
            train_loss, train_acc = self.train_epoch(train_loader)
            val_loss, val_acc = self.validate(val_loader)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_acc)
                else:
                    self.scheduler.step()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            is_best = val_acc > best_val_accuracy
            if is_best:
                best_val_accuracy = val_acc
                patience_counter = 0
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤—É—é –º–æ–¥–µ–ª—å
                self.save_model("multi", "multi", epoch, val_acc, True)
            else:
                patience_counter += 1
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            epoch_time = time.time() - start_time
            self.logger.info(
                f'Epoch {epoch+1}/{self.config.num_epochs}: '
                f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
                f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, '
                f'Time: {epoch_time:.2f}s'
            )
            
            # Early stopping
            if patience_counter >= self.config.early_stopping_patience:
                self.logger.info(f"üõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                break
        
        self.logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏—Ñ—Ä–µ–π–º–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_accuracy:.2f}%")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ manifest –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å—Ç–∏–ª–µ DQN
        try:
            # –í—ã—Ç—è–≥–∏–≤–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å GPU, –µ—Å–ª–∏ –µ—Å—Ç—å
            try:
                from agents.vdqn.cfg.gpu_configs import get_gpu_config
                gpu_cfg = get_gpu_config(None)
                gpu_profile = {
                    'gpu_name': gpu_cfg.name,
                    'vram_gb': gpu_cfg.vram_gb,
                    'batch_size': gpu_cfg.batch_size,
                    'memory_size': gpu_cfg.memory_size,
                    'hidden_sizes': list(gpu_cfg.hidden_sizes),
                    'train_repeats': gpu_cfg.train_repeats,
                    'use_amp': gpu_cfg.use_amp,
                    'use_gpu_storage': gpu_cfg.use_gpu_storage,
                    'learning_rate': gpu_cfg.learning_rate,
                }
            except Exception:
                gpu_profile = None

            manifest_path = self.config.save_manifest(
                symbol='multi',
                run_id=self.run_id,
                model_type='multiframe',
                timeframes=self.config.timeframes,
                config_dict={**self.config.to_dict(), **({'gpu_profile': gpu_profile} if gpu_profile else {})},
                symbols=self.config.symbols
            )
            results_payload = {
                'best_val_accuracy': float(best_val_accuracy),
                'epochs_trained': int(len(self.train_losses)),
                'train_loss_last': float(self.train_losses[-1]) if self.train_losses else None,
                'val_loss_last': float(self.val_losses[-1]) if self.val_losses else None,
                'gpu_profile': gpu_profile,
                'training_params': {
                    'batch_size': getattr(self.config, 'batch_size', None),
                    'learning_rate': getattr(self.config, 'learning_rate', None),
                    'hidden_sizes': getattr(self.config, 'hidden_channels', None),
                    'sequence_length': getattr(self.config, 'sequence_length', None),
                    'timeframes': self.config.timeframes,
                }
            }
            results_path = self.config.save_results('multi', self.run_id, results_payload, filename='result_multiframe.json')
            self.logger.info(f"üìù Manifest: {manifest_path}")
            self.logger.info(f"üìù Results:  {results_path}")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å manifest/results: {e}")
        
        return {
            'model': self.model,
            'best_val_accuracy': best_val_accuracy,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies
        }
