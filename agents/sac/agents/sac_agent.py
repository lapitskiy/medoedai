
"""ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ SAC Ð°Ð³ÐµÐ½Ñ‚."""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

import torch # type: ignore
import torch.nn.functional as F # type: ignore
from torch import nn, optim # type: ignore

from .config import SacConfig
from .networks import SacCategoricalActor, SacDoubleCritic
from .replay_buffer import SacReplayBuffer


@dataclass
class SacAgentState:
    actor: nn.Module
    critic: nn.Module
    target_critic: nn.Module
    log_alpha: torch.Tensor
    alpha_optimizer: optim.Optimizer


def _decay_optimizer_lr(self, optimizer: optim.Optimizer, which: str) -> None:
    decay = self.cfg.grad_fail_lr_decay
    if decay <= 0 or decay >= 1:
        return
    for param_group in optimizer.param_groups:
        old_lr = param_group.get("lr", None)
        if old_lr is None:
            continue
        new_lr = max(old_lr * decay, getattr(self.cfg, f"min_lr_{which}", old_lr))
        param_group["lr"] = new_lr
        logging.warning(
            "âš ï¸ [SacAgent] %s lr decayed from %.3e to %.3e",
            which,
            old_lr,
            new_lr,
        )

class SacAgent:
    def __init__(self, observation_dim: int, action_dim: int, cfg: Optional[SacConfig] = None) -> None:
        self.cfg = cfg or SacConfig()
        self.device = torch.device(self.cfg.device if torch.cuda.is_available() else "cpu")

        self.observation_dim = observation_dim
        self.action_dim = action_dim

        print(f"ðŸ” [SacAgent] Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ: observation_dim={observation_dim}, action_dim={action_dim}")

        # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÐºÑÑˆ CUDA Ð¿ÐµÑ€ÐµÐ´ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€Ð°
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ðŸ§¹ [SacAgent] ÐžÑ‡Ð¸Ñ‰ÐµÐ½ ÐºÑÑˆ CUDA Ð¿ÐµÑ€ÐµÐ´ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€Ð°")
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð°ÐºÑ‚Ð¾Ñ€Ð° Ð¸ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°
        self.actor = SacCategoricalActor(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        
        self.critic = SacDoubleCritic(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        
        self.target_critic = SacDoubleCritic(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        self.target_critic.load_state_dict(self.critic.state_dict())
        self.target_entropy = -torch.log(torch.tensor(1.0 / action_dim, device=self.device)) * self.cfg.target_entropy_scale

        self.actor_optimizer = optim.AdamW(self.actor.parameters(), lr=self.cfg.lr_actor)
        self.critic_optimizer = optim.AdamW(self.critic.parameters(), lr=self.cfg.lr_critic)

        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
        self.alpha_optimizer = optim.AdamW([self.log_alpha], lr=self.cfg.lr_alpha)

        # Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÑ‡Ñ‘Ñ‚Ñ‡Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸ lr Ð¸ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ð±ÑƒÑ„ÐµÑ€Ð° Ð¿Ñ€Ð¸ NaN Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ñ…
        self._critic_grad_fail_streak = 0
        self._critic_grad_fail_decays = 0
        self._actor_grad_fail_streak = 0
        self._actor_grad_fail_decays = 0
        self._initial_lr_actor = self.cfg.lr_actor
        self._initial_lr_critic = self.cfg.lr_critic
        self._initial_lr_alpha = self.cfg.lr_alpha

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ GradScaler Ð´Ð»Ñ AMP
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.cfg.use_amp)

        # Ð‘ÑƒÑ„ÐµÑ€ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð½Ð° CPU Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸ GPU Ð¿Ð°Ð¼ÑÑ‚Ð¸
        # ÐœÐ¾Ð¶Ð½Ð¾ Ð²ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð½Ð° GPU, ÐµÑÐ»Ð¸ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾
        buffer_device = torch.device("cpu")
        print(f"ðŸ”§ Ð‘ÑƒÑ„ÐµÑ€ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½ Ð½Ð° {buffer_device}")

        self.replay_buffer = SacReplayBuffer(
            capacity=self.cfg.memory_size,
            state_dim=observation_dim,
            device=buffer_device,
            use_gpu_storage=False,  # ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ GPU Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸ Ð¿Ð°Ð¼ÑÑ‚Ð¸
        )

        self.total_steps = 0

    # === Action selection ===
    def act(self, obs: torch.Tensor, deterministic: bool = False) -> torch.Tensor:
        obs = obs.to(self.device)
        with torch.no_grad():
            dist = self.actor.forward(obs)
            if deterministic:
                probs = dist.probs
                action = torch.argmax(probs, dim=-1, keepdim=True)
                return action.float()
            action = dist.sample().unsqueeze(-1).float()
        return action

    # === Training ===
    def update(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²Ñ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð° Ð½Ð° NaN Ð¸ inf
        for k, v in batch.items():
            if torch.isnan(v).any() or torch.isinf(v).any():
                logging.warning(f"ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ NaN Ð¸Ð»Ð¸ Inf Ð² {k} Ð±Ð°Ñ‚Ñ‡Ð°. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                logging.warning(
                    "ðŸ”Ž [SacAgent] batch stats %s: min=%s max=%s mean=%s std=%s",
                    k,
                    v.nanmin().item(),
                    v.nanmax().item(),
                    v.nanmean().item(),
                    v.nanstd().item(),
                )
                # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€ Ð¿Ñ€Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ð¸ NaN Ð² Ð±Ð°Ñ‚Ñ‡Ðµ
                if hasattr(self, 'clear_buffer_on_nan') and self.clear_buffer_on_nan:
                    self.replay_buffer.clear()
                    logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° NaN Ð² Ð±Ð°Ñ‚Ñ‡Ðµ")
                return {}

        # Ð›ÐµÐ½Ð¸Ð²Ð°Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÑ‡Ñ‘Ñ‚Ñ‡Ð¸ÐºÐ° ÑˆÐ°Ð³Ð¾Ð²
        if not hasattr(self, "update_steps"):
            self.update_steps = 0

        obs = batch["obs"].float().to(self.device)
        actions = batch["actions"].long().to(self.device).view(-1, 1)
        rewards = batch["rewards"].float().to(self.device).view(-1, 1)
        next_obs = batch["next_obs"].float().to(self.device)
        dones = batch["dones"].float().to(self.device).view(-1, 1)

        obs = torch.nan_to_num(
            obs,
            nan=0.0,
            posinf=float(self.cfg.obs_clip_value),
            neginf=-float(self.cfg.obs_clip_value),
        )
        next_obs = torch.nan_to_num(
            next_obs,
            nan=0.0,
            posinf=float(self.cfg.obs_clip_value),
            neginf=-float(self.cfg.obs_clip_value),
        )
        rewards = torch.nan_to_num(
            rewards,
            nan=0.0,
            posinf=float(self.cfg.reward_clip_value),
            neginf=-float(self.cfg.reward_clip_value),
        )

        obs_abs_max = torch.max(torch.abs(obs)).item()
        next_obs_abs_max = torch.max(torch.abs(next_obs)).item()
        reward_abs_max = torch.max(torch.abs(rewards)).item()

        clipped_any = False
        if obs_abs_max > self.cfg.obs_clip_value:
            obs.clamp_(min=-self.cfg.obs_clip_value, max=self.cfg.obs_clip_value)
            clipped_any = True
        if next_obs_abs_max > self.cfg.obs_clip_value:
            next_obs.clamp_(min=-self.cfg.obs_clip_value, max=self.cfg.obs_clip_value)
            clipped_any = True
        if reward_abs_max > self.cfg.reward_clip_value:
            rewards.clamp_(min=-self.cfg.reward_clip_value, max=self.cfg.reward_clip_value)
            clipped_any = True

        logging.debug(
            "ðŸ”Ž [SacAgent] batch stats obs: min=%.3f max=%.3f mean=%.3f std=%.3f",
            obs.min().item(),
            obs.max().item(),
            obs.mean().item(),
            obs.std().item(),
        )
        logging.debug(
            "ðŸ”Ž [SacAgent] batch stats next_obs: min=%.3f max=%.3f mean=%.3f std=%.3f",
            next_obs.min().item(),
            next_obs.max().item(),
            next_obs.mean().item(),
            next_obs.std().item(),
        )
        logging.debug(
            "ðŸ”Ž [SacAgent] batch stats rewards: min=%.3f max=%.3f mean=%.3f std=%.3f",
            rewards.min().item(),
            rewards.max().item(),
            rewards.mean().item(),
            rewards.std().item(),
        )

        step_idx = getattr(self, "update_steps", 0)
        should_log_extreme = self.cfg.warn_on_clipped_inputs and (
            step_idx % max(1, self.cfg.extreme_log_interval) == 0
        )

        if clipped_any and should_log_extreme:
            logging.warning(
                "âš ï¸ [SacAgent] Ð’Ñ…Ð¾Ð´Ñ‹ ÐºÐ»Ð¸Ð¿Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹: |obs|max=%.2f, |next_obs|max=%.2f, |reward|max=%.2f",
                obs_abs_max,
                next_obs_abs_max,
                reward_abs_max,
            )

        hard_limit_triggered = (
            obs_abs_max > self.cfg.obs_hard_limit
            or next_obs_abs_max > self.cfg.obs_hard_limit
            or reward_abs_max > self.cfg.reward_hard_limit
        )

        if hard_limit_triggered:
            if should_log_extreme:
                logging.warning(
                    "âš ï¸ [SacAgent] ÐŸÑ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ñ‹ Ð¶Ñ‘ÑÑ‚ÐºÐ¸Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹: |obs|max=%.2f, |next_obs|max=%.2f, |reward|max=%.2f",
                    obs_abs_max,
                    next_obs_abs_max,
                    reward_abs_max,
                )
            if self.cfg.drop_batch_on_extreme:
                if hasattr(self, "replay_buffer"):
                    self.replay_buffer.clear()
                    logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° ÑÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹")
                return {}

        with torch.cuda.amp.autocast(enabled=self.cfg.use_amp):
            alpha = self.log_alpha.exp()

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð½Ð° NaN Ð² alpha Ð¸ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ñ€Ð°ÑÑ‡ÐµÑ‚Ð°Ñ…
            if not torch.isfinite(alpha):
                logging.warning("âš ï¸ [SacAgent] alpha Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½ (NaN/Inf). ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                return {}

            with torch.no_grad():
                next_dist = self.actor.forward(next_obs)
                next_probs = next_dist.probs
                next_log_probs = next_dist.logits.log_softmax(dim=-1)

                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² next_probs Ð¸ next_log_probs
                if not torch.isfinite(next_probs).all() or not torch.isfinite(next_log_probs).all():
                    logging.warning("âš ï¸ [SacAgent] next_probs Ð¸Ð»Ð¸ next_log_probs Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                    return {}

                target_q1_all, target_q2_all = self.target_critic(next_obs)
                min_next_q = torch.min(target_q1_all, target_q2_all)

                # Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´ÐµÐ±Ð°Ð³ target_q
                if not torch.isfinite(target_q1_all).all() or not torch.isfinite(target_q2_all).all():
                    logging.warning(f"âš ï¸ [SacAgent] target_q1_all ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ NaN: {torch.isnan(target_q1_all).sum().item()}/{target_q1_all.numel()}")
                    logging.warning(f"âš ï¸ [SacAgent] target_q2_all ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ NaN: {torch.isnan(target_q2_all).sum().item()}/{target_q2_all.numel()}")
                    # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€ Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ… Ñ target_critic
                    self.replay_buffer.clear()
                    logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° NaN Ð² target_critic")
                    return {}

                target_q = (next_probs * (min_next_q - alpha * next_log_probs)).sum(dim=-1, keepdim=True)

                # Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´ÐµÐ±Ð°Ð³ target_q
                if not torch.isfinite(target_q).all():
                    nan_count = torch.isnan(target_q).sum().item()
                    logging.warning(f"âš ï¸ [SacAgent] target_q ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ {nan_count}/{target_q.numel()} NaN")
                    if nan_count > 0:
                        finite_mask = torch.isfinite(target_q).squeeze()
                        if finite_mask.any():
                            logging.warning(
                                "âš ï¸ [SacAgent] target_q stats (finite): min=%.3f max=%.3f mean=%.3f std=%.3f",
                                target_q[finite_mask].min().item(),
                                target_q[finite_mask].max().item(),
                                target_q[finite_mask].mean().item(),
                                target_q[finite_mask].std().item(),
                            )
                        logging.warning(
                            "âš ï¸ [SacAgent] next_probs stats: min=%.3f max=%.3f",
                            next_probs.min().item(),
                            next_probs.max().item(),
                        )
                        logging.warning(
                            "âš ï¸ [SacAgent] next_log_probs stats: min=%.3f max=%.3f",
                            next_log_probs.min().item(),
                            next_log_probs.max().item(),
                        )
                    # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€
                    self.replay_buffer.clear()
                    logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° NaN Ð² target_q")
                    return {}

                target_value = rewards + (1 - dones) * self.cfg.gamma * target_q

                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² target_value
                if not torch.isfinite(target_value).all():
                    logging.warning("âš ï¸ [SacAgent] target_value Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                    return {}

            current_q1_all, current_q2_all = self.critic(obs)
            actions_indices = actions.long().squeeze(-1)
            current_q1 = current_q1_all.gather(1, actions_indices.unsqueeze(-1))
            current_q2 = current_q2_all.gather(1, actions_indices.unsqueeze(-1))

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² current_q1 Ð¸ current_q2
            if not torch.isfinite(current_q1).all() or not torch.isfinite(current_q2).all():
                logging.warning("âš ï¸ [SacAgent] current_q Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                return {}

            critic_loss = F.mse_loss(current_q1, target_value) + F.mse_loss(current_q2, target_value)

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ° (Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð½Ð° NaN/Inf)
        self.critic_optimizer.zero_grad()
        if not torch.isfinite(critic_loss):
            logging.warning("âš ï¸ [SacAgent] critic_loss Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½ (NaN/Inf). ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑˆÐ°Ð³ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°.")
            return {}
        self.scaler.scale(critic_loss).backward()
        # Unscale Ð¿ÐµÑ€ÐµÐ´ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°Ð¼Ð¸ Ð¸ ÐºÐ»Ð¸Ð¿Ð¿Ð¸Ð½Ð³Ð¾Ð¼
        self.scaler.unscale_(self.critic_optimizer)
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð² ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ° Ð½Ð° NaN/Inf
        bad_grad_critic = any(
            (p.grad is not None) and (not torch.all(torch.isfinite(p.grad))) for p in self.critic.parameters()
        )
        if bad_grad_critic:
            finite_grad_squares = [
                torch.sum(p.grad.detach() ** 2).item()
                for p in self.critic.parameters()
                if p.grad is not None and torch.all(torch.isfinite(p.grad))
            ]
            grad_norm = (sum(finite_grad_squares) ** 0.5) if finite_grad_squares else float('nan')
            logging.warning("âš ï¸ [SacAgent] critic grad norm before zero: %.6f", grad_norm)
            logging.warning(
                "âš ï¸ [SacAgent] critic grad stats (finite parts): max=%.6f min=%.6f",
                max(
                    (
                        p.grad.max().item()
                        for p in self.critic.parameters()
                        if p.grad is not None and torch.all(torch.isfinite(p.grad))
                    ),
                    default=float('nan'),
                ),
                min(
                    (
                        p.grad.min().item()
                        for p in self.critic.parameters()
                        if p.grad is not None and torch.all(torch.isfinite(p.grad))
                    ),
                    default=float('nan'),
                ),
            )
            self.critic_optimizer.zero_grad(set_to_none=True)
            # Ð’Ð°Ð¶Ð½Ð¾: ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ GradScaler, Ð¸Ð½Ð°Ñ‡Ðµ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ unscale_ Ð²Ñ‹Ð·Ð¾Ð²ÐµÑ‚ Ð¾ÑˆÐ¸Ð±ÐºÑƒ
            self.scaler.update()
            self._critic_grad_fail_streak += 1
            if self.cfg.grad_fail_log_details:
                logging.warning(
                    "âš ï¸ [SacAgent] critic grad fail streak=%d decay=%d",
                    self._critic_grad_fail_streak,
                    getattr(self, "_critic_grad_fail_decays", 0),
                )
            if self.cfg.grad_fail_clear_buffer and hasattr(self, "replay_buffer"):
                self.replay_buffer.clear()
                logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° NaN Ð² Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ñ… ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°")
            if (
                self._critic_grad_fail_streak >= self.cfg.grad_fail_patience
                and self._critic_grad_fail_decays < self.cfg.grad_fail_max_decays
            ):
                self._decay_optimizer_lr(self.critic_optimizer, "critic")
                self._critic_grad_fail_streak = 0
                self._critic_grad_fail_decays += 1
            logging.warning("âš ï¸ [SacAgent] NaN/Inf Ð² Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ñ… ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°. Ð¨Ð°Ð³ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½.")
            return {}
        self._critic_grad_fail_streak = 0
        if self.cfg.max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.max_grad_norm)
        self.scaler.step(self.critic_optimizer)
        self.scaler.update()

        # ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð² ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ° (Ñ€ÐµÐ´ÐºÐ¾)
        try:
            max_critic_grad = max(torch.max(p.grad).item() for p in self.critic.parameters() if p.grad is not None)
        except ValueError:
            max_critic_grad = float("nan")
        if (self.update_steps % 200 == 0) or (not torch.isfinite(torch.tensor(max_critic_grad))):
            logging.warning(f"ðŸ“‰ [SacAgent] ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°: {max_critic_grad:.4f}")

        for param in self.critic.parameters():
            param.requires_grad = False

        # ÐŸÐ¾Ð»Ð¸Ñ‚Ð¸ÐºÐ°: Ð¾Ð¶Ð¸Ð´Ð°ÐµÐ¼ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ, Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸ÑŽ Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€Ð¸
        dist = self.actor.forward(obs)
        probs = dist.probs
        log_probs = dist.logits.log_softmax(dim=-1)

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² probs Ð¸ log_probs
        if not torch.isfinite(probs).all() or not torch.isfinite(log_probs).all():
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("âš ï¸ [SacAgent] probs Ð¸Ð»Ð¸ log_probs Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð°ÐºÑ‚Ð¾Ñ€Ð°.")
            return {}

        q1_all, q2_all = self.critic(obs)
        min_q_all = torch.min(q1_all, q2_all)

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² q1_all Ð¸ q2_all
        if not torch.isfinite(q1_all).all() or not torch.isfinite(q2_all).all():
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("âš ï¸ [SacAgent] q_values Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð°ÐºÑ‚Ð¾Ñ€Ð°.")
            return {}

        actor_loss = (alpha * (probs * log_probs).sum(dim=-1, keepdim=True) - (probs * min_q_all).sum(dim=-1, keepdim=True)).mean()

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² actor_loss
        if not torch.isfinite(actor_loss):
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("âš ï¸ [SacAgent] actor_loss Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½ (NaN/Inf). ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑˆÐ°Ð³ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð°ÐºÑ‚Ð¾Ñ€Ð°.")
            return {}

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð°ÐºÑ‚Ð¾Ñ€Ð° (Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð½Ð° NaN/Inf)
        self.actor_optimizer.zero_grad()
        self.scaler.scale(actor_loss).backward()
        self.scaler.unscale_(self.actor_optimizer)
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ð°ÐºÑ‚Ð¾Ñ€Ð° Ð½Ð° NaN/Inf
        bad_grad_actor = any(
            (p.grad is not None) and (not torch.all(torch.isfinite(p.grad))) for p in self.actor.parameters()
        )
        if bad_grad_actor:
            finite_actor_grad_squares = [
                torch.sum(p.grad.detach() ** 2).item()
                for p in self.actor.parameters()
                if p.grad is not None and torch.all(torch.isfinite(p.grad))
            ]
            actor_grad_norm = (sum(finite_actor_grad_squares) ** 0.5) if finite_actor_grad_squares else float('nan')
            logging.warning("âš ï¸ [SacAgent] actor grad norm before zero: %.6f", actor_grad_norm)
            logging.warning(
                "âš ï¸ [SacAgent] actor grad stats (finite parts): max=%.6f min=%.6f",
                max((p.grad.max().item() for p in self.actor.parameters() if p.grad is not None and torch.all(torch.isfinite(p.grad))), default=float('nan')),
                min((p.grad.min().item() for p in self.actor.parameters() if p.grad is not None and torch.all(torch.isfinite(p.grad))), default=float('nan')),
            )
            self.actor_optimizer.zero_grad(set_to_none=True)
            for param in self.critic.parameters():
                param.requires_grad = True
            # Ð’Ð°Ð¶Ð½Ð¾: ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ GradScaler, Ð¸Ð½Ð°Ñ‡Ðµ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ unscale_ Ð²Ñ‹Ð·Ð¾Ð²ÐµÑ‚ Ð¾ÑˆÐ¸Ð±ÐºÑƒ
            self.scaler.update()
            self._actor_grad_fail_streak += 1
            if self.cfg.grad_fail_log_details:
                logging.warning(
                    "âš ï¸ [SacAgent] actor grad fail streak=%d",
                    self._actor_grad_fail_streak,
                )
            if self.cfg.grad_fail_clear_buffer and hasattr(self, "replay_buffer"):
                self.replay_buffer.clear()
                logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° NaN Ð² Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ñ… Ð°ÐºÑ‚Ð¾Ñ€Ð°")
            if (
                self._actor_grad_fail_streak >= self.cfg.grad_fail_patience
                and self._actor_grad_fail_decays < self.cfg.grad_fail_max_decays
            ):
                self._decay_optimizer_lr(self.actor_optimizer, "actor")
                self._actor_grad_fail_streak = 0
                self._actor_grad_fail_decays += 1
            logging.warning("âš ï¸ [SacAgent] NaN/Inf Ð² Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ñ… Ð°ÐºÑ‚Ð¾Ñ€Ð°. Ð¨Ð°Ð³ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½.")
            return {}
        self._actor_grad_fail_streak = 0
        if self.cfg.max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.cfg.max_grad_norm)
        self.scaler.step(self.actor_optimizer)
        self.scaler.update()

        # ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ð°ÐºÑ‚Ð¾Ñ€Ð° (Ñ€ÐµÐ´ÐºÐ¾)
        try:
            max_actor_grad = max(torch.max(p.grad).item() for p in self.actor.parameters() if p.grad is not None)
        except ValueError:
            max_actor_grad = float("nan")
        if (self.update_steps % 200 == 0) or (not torch.isfinite(torch.tensor(max_actor_grad))):
            logging.warning(f"ðŸ“‰ [SacAgent] ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚ Ð°ÐºÑ‚Ð¾Ñ€Ð°: {max_actor_grad:.4f}")

        for param in self.critic.parameters():
            param.requires_grad = True

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹ (alpha)
        entropy = -(probs * log_probs).sum(dim=-1, keepdim=True)
        alpha_loss = -(self.log_alpha * (entropy - self.target_entropy).detach()).mean()
        if torch.isfinite(alpha_loss):
            self.alpha_optimizer.zero_grad()
            self.scaler.scale(alpha_loss).backward()
            self.scaler.step(self.alpha_optimizer)
            self.scaler.update()
        else:
            logging.warning("âš ï¸ [SacAgent] alpha_loss Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½ (NaN/Inf). ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑˆÐ°Ð³ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ alpha.")

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ†ÐµÐ»ÐµÐ²Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹
        self._soft_update(self.critic, self.target_critic)

        self.update_steps += 1

        avg_entropy = float(entropy.mean().item()) if torch.isfinite(entropy).all() else float('nan')

        return {
            "critic_loss": critic_loss.item() if critic_loss is not None else float('nan'),
            "actor_loss": actor_loss.item() if actor_loss is not None else float('nan'),
            "alpha_loss": alpha_loss.item() if torch.isfinite(alpha_loss) else float('nan'),
            "alpha": alpha.item() if alpha is not None else float('nan'),
            "entropy": avg_entropy,
            "max_actor_grad": max_actor_grad,
            "max_critic_grad": max_critic_grad,
        }

    def _soft_update(self, source: nn.Module, target: nn.Module) -> None:
        tau = self.cfg.tau
        for target_param, source_param in zip(target.parameters(), source.parameters(), strict=True):
            target_param.data.copy_(
                tau * source_param.data + (1.0 - tau) * target_param.data
            )

    # === Replay buffer integration ===
    def store_transition(
        self,
        state: torch.Tensor,
        action: torch.Tensor,
        reward: torch.Tensor,
        next_state: torch.Tensor,
        done: torch.Tensor,
    ) -> None:
        self.replay_buffer.add_transition(state, action, reward, next_state, done)
        self.total_steps += 1

    def ready_to_update(self) -> bool:
        return len(self.replay_buffer) >= self.cfg.start_learning_after

    def sample_batch(self) -> Optional[Dict[str, torch.Tensor]]:
        batch = self.replay_buffer.sample_batch(self.cfg.batch_size)
        if batch[0] is None:
            return None
        states, actions, rewards, next_states, dones, _, weights, _ = batch

        # ÐŸÐµÑ€ÐµÐ½Ð¾ÑÐ¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° GPU ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾, ÐµÑÐ»Ð¸ Ð±ÑƒÑ„ÐµÑ€ Ð½Ð° CPU
        if self.device.type == "cuda":
            states = states.to(self.device)
            actions = actions.to(self.device)
            rewards = rewards.to(self.device)
            next_states = next_states.to(self.device)
            dones = dones.to(self.device)
            weights = weights.to(self.device)

        return {
            "obs": states,
            "actions": actions,
            "rewards": rewards,
            "next_obs": next_states,
            "dones": dones.float(),
            "weights": weights,
        }

    def save(self, path: str) -> None:
        torch.save(
            {
                "actor": self.actor.state_dict(),
                "critic": self.critic.state_dict(),
                "target_critic": self.target_critic.state_dict(),
                "log_alpha": self.log_alpha.detach(),
            },
            path,
        )

    def load(self, path: str) -> None:
        state = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(state["actor"])
        self.critic.load_state_dict(state["critic"])
        self.target_critic.load_state_dict(state["target_critic"])
        self.log_alpha = state["log_alpha"].to(self.device).requires_grad_()


