"""–û—Å–Ω–æ–≤–Ω–æ–π SAC –∞–≥–µ–Ω—Ç."""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

import torch # type: ignore
import torch.nn.functional as F # type: ignore
from torch import nn, optim # type: ignore

from .config import SacConfig
from .networks import SacCategoricalActor, SacDoubleCritic
from .replay_buffer import SacReplayBuffer


@dataclass
class SacAgentState:
    actor: nn.Module
    critic: nn.Module
    target_critic: nn.Module
    log_alpha: torch.Tensor
    alpha_optimizer: optim.Optimizer


class SacAgent:
    def __init__(self, observation_dim: int, action_dim: int, cfg: Optional[SacConfig] = None) -> None:
        self.cfg = cfg or SacConfig()
        self.device = torch.device(self.cfg.device if torch.cuda.is_available() else "cpu")

        self.observation_dim = observation_dim
        self.action_dim = action_dim

        print(f"üîç [SacAgent] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: observation_dim={observation_dim}, action_dim={action_dim}")

        # –û—á–∏—â–∞–µ–º –∫—ç—à CUDA –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –±—É—Ñ–µ—Ä–∞
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("üßπ [SacAgent] –û—á–∏—â–µ–Ω –∫—ç—à CUDA –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –±—É—Ñ–µ—Ä–∞")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–æ—Ä–∞ –∏ –∫—Ä–∏—Ç–∏–∫–∞
        self.actor = SacCategoricalActor(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        
        self.critic = SacDoubleCritic(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        
        self.target_critic = SacDoubleCritic(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        self.target_critic.load_state_dict(self.critic.state_dict())
        self.target_entropy = -torch.log(torch.tensor(1.0 / action_dim, device=self.device)) * self.cfg.target_entropy_scale

        self.actor_optimizer = optim.AdamW(self.actor.parameters(), lr=self.cfg.lr_actor)
        self.critic_optimizer = optim.AdamW(self.critic.parameters(), lr=self.cfg.lr_critic)

        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
        self.alpha_optimizer = optim.AdamW([self.log_alpha], lr=self.cfg.lr_alpha)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GradScaler –¥–ª—è AMP
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.cfg.use_amp)

        # –ë—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Å–æ–∑–¥–∞—ë–º –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ GPU –ø–∞–º—è—Ç–∏
        # –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ GPU, –µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
        buffer_device = torch.device("cpu")
        print(f"üîß –ë—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω –Ω–∞ {buffer_device}")

        self.replay_buffer = SacReplayBuffer(
            capacity=self.cfg.memory_size,
            state_dim=observation_dim,
            device=buffer_device,
            use_gpu_storage=False,  # –û—Ç–∫–ª—é—á–∞–µ–º GPU —Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        )

        self.total_steps = 0

    # === Action selection ===
    def act(self, obs: torch.Tensor, deterministic: bool = False) -> torch.Tensor:
        obs = obs.to(self.device)
        if deterministic:
            with torch.no_grad():
                logits = self.actor.forward(obs)
                action = torch.argmax(logits, dim=-1, keepdim=True)
                return action.float()
        with torch.no_grad():
            action, _ = self.actor.sample(obs)
        return action

    # === Training ===
    def update(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ –Ω–∞ NaN –∏ inf
        for k, v in batch.items():
            if torch.isnan(v).any() or torch.isinf(v).any():
                logging.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∏–ª–∏ Inf –≤ {k} –±–∞—Ç—á–∞. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
                # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ NaN –≤ –±–∞—Ç—á–µ
                if hasattr(self, 'clear_buffer_on_nan') and self.clear_buffer_on_nan:
                    self.replay_buffer.clear()
                    logging.warning("üîÑ –ë—É—Ñ–µ—Ä –æ—á–∏—â–µ–Ω –∏–∑-–∑–∞ NaN –≤ –±–∞—Ç—á–µ")
                return {}

        # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á—ë—Ç—á–∏–∫–∞ —à–∞–≥–æ–≤
        if not hasattr(self, "update_steps"):
            self.update_steps = 0

        obs = batch["obs"].float().to(self.device)
        actions = batch["actions"].long().to(self.device).view(-1, 1)
        rewards = batch["rewards"].float().to(self.device).view(-1, 1)
        next_obs = batch["next_obs"].float().to(self.device)
        dones = batch["dones"].float().to(self.device).view(-1, 1)

                # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤–æ –≤—Ö–æ–¥–∞—Ö
        obs = torch.nan_to_num(obs, nan=0.0, posinf=1e5, neginf=-1e5)
        next_obs = torch.nan_to_num(next_obs, nan=0.0, posinf=1e5, neginf=-1e5)
        rewards = torch.nan_to_num(rewards, nan=0.0, posinf=1e3, neginf=-1e3)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if torch.abs(obs).max() > 1e6 or torch.abs(next_obs).max() > 1e6 or torch.abs(rewards).max() > 1e4:
            logging.warning("‚ö†Ô∏è [SacAgent] –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–æ –≤—Ö–æ–¥–∞—Ö. –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä.")
            self.replay_buffer.clear()
            return {}

        with torch.cuda.amp.autocast(enabled=self.cfg.use_amp):
            alpha = self.log_alpha.exp()

            # –ü—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ NaN –≤ alpha –∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—á–µ—Ç–∞—Ö
            if not torch.isfinite(alpha):
                logging.warning("‚ö†Ô∏è [SacAgent] alpha –Ω–µ –∫–æ–Ω–µ—á–µ–Ω (NaN/Inf). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
                return {}

            with torch.no_grad():
                next_dist = self.actor.forward(next_obs)
                next_probs = next_dist.probs
                next_log_probs = next_dist.logits.log_softmax(dim=-1)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –≤ next_probs –∏ next_log_probs
                if not torch.isfinite(next_probs).all() or not torch.isfinite(next_log_probs).all():
                    logging.warning("‚ö†Ô∏è [SacAgent] next_probs –∏–ª–∏ next_log_probs –Ω–µ –∫–æ–Ω–µ—á–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
                    return {}

                target_q1_all, target_q2_all = self.target_critic(next_obs)
                min_next_q = torch.min(target_q1_all, target_q2_all)

                # –î–µ—Ç–∞–ª—å–Ω—ã–π –¥–µ–±–∞–≥ target_q
                if not torch.isfinite(target_q1_all).all() or not torch.isfinite(target_q2_all).all():
                    logging.warning(f"‚ö†Ô∏è [SacAgent] target_q1_all —Å–æ–¥–µ—Ä–∂–∏—Ç NaN: {torch.isnan(target_q1_all).sum().item()}/{target_q1_all.numel()}")
                    logging.warning(f"‚ö†Ô∏è [SacAgent] target_q2_all —Å–æ–¥–µ—Ä–∂–∏—Ç NaN: {torch.isnan(target_q2_all).sum().item()}/{target_q2_all.numel()}")
                    # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å target_critic
                    self.replay_buffer.clear()
                    logging.warning("üîÑ –ë—É—Ñ–µ—Ä –æ—á–∏—â–µ–Ω –∏–∑-–∑–∞ NaN –≤ target_critic")
                    return {}

                target_q = (next_probs * (min_next_q - alpha * next_log_probs)).sum(dim=-1, keepdim=True)

                # –î–µ—Ç–∞–ª—å–Ω—ã–π –¥–µ–±–∞–≥ target_q
                if not torch.isfinite(target_q).all():
                    nan_count = torch.isnan(target_q).sum().item()
                    logging.warning(f"‚ö†Ô∏è [SacAgent] target_q —Å–æ–¥–µ—Ä–∂–∏—Ç {nan_count}/{target_q.numel()} NaN")
                    if nan_count > 0:
                        # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–Ω—è—Ç—å –æ—Ç–∫—É–¥–∞ NaN
                        finite_mask = torch.isfinite(target_q).squeeze()
                        if finite_mask.any():
                            logging.warning(f"‚ö†Ô∏è [SacAgent] –°—Ä–µ–¥–Ω–µ–µ –∫–æ–Ω–µ—á–Ω–æ–µ target_q: {target_q[finite_mask].mean().item():.6f}")
                        # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä
                        self.replay_buffer.clear()
                        logging.warning("üîÑ –ë—É—Ñ–µ—Ä –æ—á–∏—â–µ–Ω –∏–∑-–∑–∞ NaN –≤ target_q")
                    return {}

                target_value = rewards + (1 - dones) * self.cfg.gamma * target_q

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –≤ target_value
                if not torch.isfinite(target_value).all():
                    logging.warning("‚ö†Ô∏è [SacAgent] target_value –Ω–µ –∫–æ–Ω–µ—á–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
                    return {}

            current_q1_all, current_q2_all = self.critic(obs)
            actions_indices = actions.long().squeeze(-1)
            current_q1 = current_q1_all.gather(1, actions_indices.unsqueeze(-1))
            current_q2 = current_q2_all.gather(1, actions_indices.unsqueeze(-1))

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –≤ current_q1 –∏ current_q2
            if not torch.isfinite(current_q1).all() or not torch.isfinite(current_q2).all():
                logging.warning("‚ö†Ô∏è [SacAgent] current_q –Ω–µ –∫–æ–Ω–µ—á–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
                return {}

            critic_loss = F.mse_loss(current_q1, target_value) + F.mse_loss(current_q2, target_value)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—Ä–∏—Ç–∏–∫–∞ (–ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ NaN/Inf)
        self.critic_optimizer.zero_grad()
        if not torch.isfinite(critic_loss):
            logging.warning("‚ö†Ô∏è [SacAgent] critic_loss –Ω–µ –∫–æ–Ω–µ—á–µ–Ω (NaN/Inf). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏–∫–∞.")
            return {}
        self.scaler.scale(critic_loss).backward()
        # Unscale –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏ –∏ –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º
        self.scaler.unscale_(self.critic_optimizer)
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫—Ä–∏—Ç–∏–∫–∞ –Ω–∞ NaN/Inf
        bad_grad_critic = any(
            (p.grad is not None) and (not torch.all(torch.isfinite(p.grad))) for p in self.critic.parameters()
        )
        if bad_grad_critic:
            self.critic_optimizer.zero_grad(set_to_none=True)
            # –í–∞–∂–Ω–æ: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º GradScaler, –∏–Ω–∞—á–µ —Å–ª–µ–¥—É—é—â–∏–π unscale_ –≤—ã–∑–æ–≤–µ—Ç –æ—à–∏–±–∫—É
            self.scaler.update()
            logging.warning("‚ö†Ô∏è [SacAgent] NaN/Inf –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö –∫—Ä–∏—Ç–∏–∫–∞. –®–∞–≥ –ø—Ä–æ–ø—É—â–µ–Ω.")
            return {}
        if self.cfg.max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.max_grad_norm)
        self.scaler.step(self.critic_optimizer)
        self.scaler.update()

        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫—Ä–∏—Ç–∏–∫–∞ (—Ä–µ–¥–∫–æ)
        try:
            max_critic_grad = max(torch.max(p.grad).item() for p in self.critic.parameters() if p.grad is not None)
        except ValueError:
            max_critic_grad = float("nan")
        if (self.update_steps % 200 == 0) or (not torch.isfinite(torch.tensor(max_critic_grad))):
            logging.warning(f"üìâ [SacAgent] –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –∫—Ä–∏—Ç–∏–∫–∞: {max_critic_grad:.4f}")

        for param in self.critic.parameters():
            param.requires_grad = False

        # –ü–æ–ª–∏—Ç–∏–∫–∞: –æ–∂–∏–¥–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –≤—ã—á–∏—Å–ª—è–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é –∏ –ø–æ—Ç–µ—Ä–∏
        dist = self.actor.forward(obs)
        probs = dist.probs
        log_probs = dist.logits.log_softmax(dim=-1)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –≤ probs –∏ log_probs
        if not torch.isfinite(probs).all() or not torch.isfinite(log_probs).all():
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("‚ö†Ô∏è [SacAgent] probs –∏–ª–∏ log_probs –Ω–µ –∫–æ–Ω–µ—á–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–æ—Ä–∞.")
            return {}

        q1_all, q2_all = self.critic(obs)
        min_q_all = torch.min(q1_all, q2_all)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –≤ q1_all –∏ q2_all
        if not torch.isfinite(q1_all).all() or not torch.isfinite(q2_all).all():
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("‚ö†Ô∏è [SacAgent] q_values –Ω–µ –∫–æ–Ω–µ—á–Ω—ã. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–æ—Ä–∞.")
            return {}

        actor_loss = (alpha * (probs * log_probs).sum(dim=-1, keepdim=True) - (probs * min_q_all).sum(dim=-1, keepdim=True)).mean()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –≤ actor_loss
        if not torch.isfinite(actor_loss):
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("‚ö†Ô∏è [SacAgent] actor_loss –Ω–µ –∫–æ–Ω–µ—á–µ–Ω (NaN/Inf). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∞–∫—Ç–æ—Ä–∞.")
            return {}

        # –û–±–Ω–æ–≤–ª—è–µ–º –∞–∫—Ç–æ—Ä–∞ (–ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ NaN/Inf)
        self.actor_optimizer.zero_grad()
        self.scaler.scale(actor_loss).backward()
        self.scaler.unscale_(self.actor_optimizer)
        bad_grad_actor = any(
            (p.grad is not None) and (not torch.all(torch.isfinite(p.grad))) for p in self.actor.parameters()
        )
        if bad_grad_actor:
            self.actor_optimizer.zero_grad(set_to_none=True)
            for param in self.critic.parameters():
                param.requires_grad = True
            # –í–∞–∂–Ω–æ: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º GradScaler, –∏–Ω–∞—á–µ —Å–ª–µ–¥—É—é—â–∏–π unscale_ –≤—ã–∑–æ–≤–µ—Ç –æ—à–∏–±–∫—É
            self.scaler.update()
            logging.warning("‚ö†Ô∏è [SacAgent] NaN/Inf –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö –∞–∫—Ç–æ—Ä–∞. –®–∞–≥ –ø—Ä–æ–ø—É—â–µ–Ω.")
            return {}
        if self.cfg.max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.cfg.max_grad_norm)
        self.scaler.step(self.actor_optimizer)
        self.scaler.update()

        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∞–∫—Ç–æ—Ä–∞ (—Ä–µ–¥–∫–æ)
        try:
            max_actor_grad = max(torch.max(p.grad).item() for p in self.actor.parameters() if p.grad is not None)
        except ValueError:
            max_actor_grad = float("nan")
        if (self.update_steps % 200 == 0) or (not torch.isfinite(torch.tensor(max_actor_grad))):
            logging.warning(f"üìâ [SacAgent] –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –∞–∫—Ç–æ—Ä–∞: {max_actor_grad:.4f}")

        for param in self.critic.parameters():
            param.requires_grad = True

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã (alpha)
        entropy = -(probs * log_probs).sum(dim=-1, keepdim=True)
        alpha_loss = -(self.log_alpha * (entropy - self.target_entropy).detach()).mean()
        if torch.isfinite(alpha_loss):
            self.alpha_optimizer.zero_grad()
            self.scaler.scale(alpha_loss).backward()
            self.scaler.step(self.alpha_optimizer)
            self.scaler.update()
        else:
            logging.warning("‚ö†Ô∏è [SacAgent] alpha_loss –Ω–µ –∫–æ–Ω–µ—á–µ–Ω (NaN/Inf). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è alpha.")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö —Å–µ—Ç–µ–π
        self._soft_update(self.critic, self.target_critic)

        self.update_steps += 1

        return {
            "critic_loss": critic_loss.item() if critic_loss is not None else float('nan'),
            "actor_loss": actor_loss.item() if actor_loss is not None else float('nan'),
            "alpha_loss": alpha_loss.item() if torch.isfinite(alpha_loss) else float('nan'),
            "alpha": alpha.item() if alpha is not None else float('nan'),
            "max_actor_grad": max_actor_grad,
            "max_critic_grad": max_critic_grad,
        }

    def _soft_update(self, source: nn.Module, target: nn.Module) -> None:
        tau = self.cfg.tau
        for target_param, source_param in zip(target.parameters(), source.parameters(), strict=True):
            target_param.data.copy_(
                tau * source_param.data + (1.0 - tau) * target_param.data
            )

    # === Replay buffer integration ===
    def store_transition(
        self,
        state: torch.Tensor,
        action: torch.Tensor,
        reward: torch.Tensor,
        next_state: torch.Tensor,
        done: torch.Tensor,
    ) -> None:
        self.replay_buffer.add_transition(state, action, reward, next_state, done)
        self.total_steps += 1

    def ready_to_update(self) -> bool:
        return len(self.replay_buffer) >= self.cfg.start_learning_after

    def sample_batch(self) -> Optional[Dict[str, torch.Tensor]]:
        batch = self.replay_buffer.sample_batch(self.cfg.batch_size)
        if batch[0] is None:
            return None
        states, actions, rewards, next_states, dones, _, weights, _ = batch

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –µ—Å–ª–∏ –±—É—Ñ–µ—Ä –Ω–∞ CPU
        if self.device.type == "cuda":
            states = states.to(self.device)
            actions = actions.to(self.device)
            rewards = rewards.to(self.device)
            next_states = next_states.to(self.device)
            dones = dones.to(self.device)
            weights = weights.to(self.device)

        return {
            "obs": states,
            "actions": actions,
            "rewards": rewards,
            "next_obs": next_states,
            "dones": dones.float(),
            "weights": weights,
        }

    def save(self, path: str) -> None:
        torch.save(
            {
                "actor": self.actor.state_dict(),
                "critic": self.critic.state_dict(),
                "target_critic": self.target_critic.state_dict(),
                "log_alpha": self.log_alpha.detach(),
            },
            path,
        )

    def load(self, path: str) -> None:
        state = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(state["actor"])
        self.critic.load_state_dict(state["critic"])
        self.target_critic.load_state_dict(state["target_critic"])
        self.log_alpha = state["log_alpha"].to(self.device).requires_grad_()


