"""ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ SAC Ð°Ð³ÐµÐ½Ñ‚."""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

import torch # type: ignore
import torch.nn.functional as F # type: ignore
from torch import nn, optim # type: ignore

from .config import SacConfig
from .networks import SacCategoricalActor, SacDoubleCritic
from .replay_buffer import SacReplayBuffer


@dataclass
class SacAgentState:
    actor: nn.Module
    critic: nn.Module
    target_critic: nn.Module
    log_alpha: torch.Tensor
    alpha_optimizer: optim.Optimizer


class SacAgent:
    def __init__(self, observation_dim: int, action_dim: int, cfg: Optional[SacConfig] = None) -> None:
        self.cfg = cfg or SacConfig()
        self.device = torch.device(self.cfg.device if torch.cuda.is_available() else "cpu")

        self.observation_dim = observation_dim
        self.action_dim = action_dim

        print(f"ðŸ” [SacAgent] Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ: observation_dim={observation_dim}, action_dim={action_dim}")

        # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÐºÑÑˆ CUDA Ð¿ÐµÑ€ÐµÐ´ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€Ð°
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ðŸ§¹ [SacAgent] ÐžÑ‡Ð¸Ñ‰ÐµÐ½ ÐºÑÑˆ CUDA Ð¿ÐµÑ€ÐµÐ´ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€Ð°")
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð°ÐºÑ‚Ð¾Ñ€Ð° Ð¸ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°
        self.actor = SacCategoricalActor(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        
        self.critic = SacDoubleCritic(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        
        self.target_critic = SacDoubleCritic(
            obs_dim=observation_dim,
            action_dim=action_dim,
            cfg=self.cfg,
        ).to(self.device)
        self.target_critic.load_state_dict(self.critic.state_dict())
        self.target_entropy = -torch.log(torch.tensor(1.0 / action_dim, device=self.device)) * self.cfg.target_entropy_scale

        self.actor_optimizer = optim.AdamW(self.actor.parameters(), lr=self.cfg.lr_actor)
        self.critic_optimizer = optim.AdamW(self.critic.parameters(), lr=self.cfg.lr_critic)

        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
        self.alpha_optimizer = optim.AdamW([self.log_alpha], lr=self.cfg.lr_alpha)

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ GradScaler Ð´Ð»Ñ AMP
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.cfg.use_amp)

        # Ð‘ÑƒÑ„ÐµÑ€ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð½Ð° CPU Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸ GPU Ð¿Ð°Ð¼ÑÑ‚Ð¸
        # ÐœÐ¾Ð¶Ð½Ð¾ Ð²ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð½Ð° GPU, ÐµÑÐ»Ð¸ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾
        buffer_device = torch.device("cpu")
        print(f"ðŸ”§ Ð‘ÑƒÑ„ÐµÑ€ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½ Ð½Ð° {buffer_device}")

        self.replay_buffer = SacReplayBuffer(
            capacity=self.cfg.memory_size,
            state_dim=observation_dim,
            device=buffer_device,
            use_gpu_storage=False,  # ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ GPU Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸ Ð¿Ð°Ð¼ÑÑ‚Ð¸
        )

        self.total_steps = 0

    # === Action selection ===
    def act(self, obs: torch.Tensor, deterministic: bool = False) -> torch.Tensor:
        obs = obs.to(self.device)
        if deterministic:
            with torch.no_grad():
                logits = self.actor.forward(obs)
                action = torch.argmax(logits, dim=-1, keepdim=True)
                return action.float()
        with torch.no_grad():
            action, _ = self.actor.sample(obs)
        return action

    # === Training ===
    def update(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²Ñ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð±Ð°Ñ‚Ñ‡Ð° Ð½Ð° NaN Ð¸ inf
        for k, v in batch.items():
            if torch.isnan(v).any() or torch.isinf(v).any():
                logging.warning(f"ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ NaN Ð¸Ð»Ð¸ Inf Ð² {k} Ð±Ð°Ñ‚Ñ‡Ð°. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€ Ð¿Ñ€Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ð¸ NaN Ð² Ð±Ð°Ñ‚Ñ‡Ðµ
                if hasattr(self, 'clear_buffer_on_nan') and self.clear_buffer_on_nan:
                    self.replay_buffer.clear()
                    logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° NaN Ð² Ð±Ð°Ñ‚Ñ‡Ðµ")
                return {}

        # Ð›ÐµÐ½Ð¸Ð²Ð°Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÑ‡Ñ‘Ñ‚Ñ‡Ð¸ÐºÐ° ÑˆÐ°Ð³Ð¾Ð²
        if not hasattr(self, "update_steps"):
            self.update_steps = 0

        obs = batch["obs"].float().to(self.device)
        actions = batch["actions"].long().to(self.device).view(-1, 1)
        rewards = batch["rewards"].float().to(self.device).view(-1, 1)
        next_obs = batch["next_obs"].float().to(self.device)
        dones = batch["dones"].float().to(self.device).view(-1, 1)

                # ÐÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½Ð°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð½ÐµÑ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð²Ð¾ Ð²Ñ…Ð¾Ð´Ð°Ñ…
        obs = torch.nan_to_num(obs, nan=0.0, posinf=1e5, neginf=-1e5)
        next_obs = torch.nan_to_num(next_obs, nan=0.0, posinf=1e5, neginf=-1e5)
        rewards = torch.nan_to_num(rewards, nan=0.0, posinf=1e3, neginf=-1e3)

        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° ÑÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ
        if torch.abs(obs).max() > 1e6 or torch.abs(next_obs).max() > 1e6 or torch.abs(rewards).max() > 1e4:
            logging.warning("âš ï¸ [SacAgent] ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ ÑÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð²Ð¾ Ð²Ñ…Ð¾Ð´Ð°Ñ…. ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€.")
            self.replay_buffer.clear()
            return {}

        with torch.cuda.amp.autocast(enabled=self.cfg.use_amp):
            alpha = self.log_alpha.exp()

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð½Ð° NaN Ð² alpha Ð¸ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ñ€Ð°ÑÑ‡ÐµÑ‚Ð°Ñ…
            if not torch.isfinite(alpha):
                logging.warning("âš ï¸ [SacAgent] alpha Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½ (NaN/Inf). ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                return {}

            with torch.no_grad():
                next_dist = self.actor.forward(next_obs)
                next_probs = next_dist.probs
                next_log_probs = next_dist.logits.log_softmax(dim=-1)

                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² next_probs Ð¸ next_log_probs
                if not torch.isfinite(next_probs).all() or not torch.isfinite(next_log_probs).all():
                    logging.warning("âš ï¸ [SacAgent] next_probs Ð¸Ð»Ð¸ next_log_probs Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                    return {}

                target_q1_all, target_q2_all = self.target_critic(next_obs)
                min_next_q = torch.min(target_q1_all, target_q2_all)

                # Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´ÐµÐ±Ð°Ð³ target_q
                if not torch.isfinite(target_q1_all).all() or not torch.isfinite(target_q2_all).all():
                    logging.warning(f"âš ï¸ [SacAgent] target_q1_all ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ NaN: {torch.isnan(target_q1_all).sum().item()}/{target_q1_all.numel()}")
                    logging.warning(f"âš ï¸ [SacAgent] target_q2_all ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ NaN: {torch.isnan(target_q2_all).sum().item()}/{target_q2_all.numel()}")
                    # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€ Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ… Ñ target_critic
                    self.replay_buffer.clear()
                    logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° NaN Ð² target_critic")
                    return {}

                target_q = (next_probs * (min_next_q - alpha * next_log_probs)).sum(dim=-1, keepdim=True)

                # Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´ÐµÐ±Ð°Ð³ target_q
                if not torch.isfinite(target_q).all():
                    nan_count = torch.isnan(target_q).sum().item()
                    logging.warning(f"âš ï¸ [SacAgent] target_q ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ {nan_count}/{target_q.numel()} NaN")
                    if nan_count > 0:
                        # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ð¾Ð½ÑÑ‚ÑŒ Ð¾Ñ‚ÐºÑƒÐ´Ð° NaN
                        finite_mask = torch.isfinite(target_q).squeeze()
                        if finite_mask.any():
                            logging.warning(f"âš ï¸ [SacAgent] Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾Ðµ target_q: {target_q[finite_mask].mean().item():.6f}")
                        # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±ÑƒÑ„ÐµÑ€
                        self.replay_buffer.clear()
                        logging.warning("ðŸ”„ Ð‘ÑƒÑ„ÐµÑ€ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½ Ð¸Ð·-Ð·Ð° NaN Ð² target_q")
                    return {}

                target_value = rewards + (1 - dones) * self.cfg.gamma * target_q

                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² target_value
                if not torch.isfinite(target_value).all():
                    logging.warning("âš ï¸ [SacAgent] target_value Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                    return {}

            current_q1_all, current_q2_all = self.critic(obs)
            actions_indices = actions.long().squeeze(-1)
            current_q1 = current_q1_all.gather(1, actions_indices.unsqueeze(-1))
            current_q2 = current_q2_all.gather(1, actions_indices.unsqueeze(-1))

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² current_q1 Ð¸ current_q2
            if not torch.isfinite(current_q1).all() or not torch.isfinite(current_q2).all():
                logging.warning("âš ï¸ [SacAgent] current_q Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ.")
                return {}

            critic_loss = F.mse_loss(current_q1, target_value) + F.mse_loss(current_q2, target_value)

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ° (Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð½Ð° NaN/Inf)
        self.critic_optimizer.zero_grad()
        if not torch.isfinite(critic_loss):
            logging.warning("âš ï¸ [SacAgent] critic_loss Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½ (NaN/Inf). ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑˆÐ°Ð³ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°.")
            return {}
        self.scaler.scale(critic_loss).backward()
        # Unscale Ð¿ÐµÑ€ÐµÐ´ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°Ð¼Ð¸ Ð¸ ÐºÐ»Ð¸Ð¿Ð¿Ð¸Ð½Ð³Ð¾Ð¼
        self.scaler.unscale_(self.critic_optimizer)
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð² ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ° Ð½Ð° NaN/Inf
        bad_grad_critic = any(
            (p.grad is not None) and (not torch.all(torch.isfinite(p.grad))) for p in self.critic.parameters()
        )
        if bad_grad_critic:
            self.critic_optimizer.zero_grad(set_to_none=True)
            # Ð’Ð°Ð¶Ð½Ð¾: ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ GradScaler, Ð¸Ð½Ð°Ñ‡Ðµ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ unscale_ Ð²Ñ‹Ð·Ð¾Ð²ÐµÑ‚ Ð¾ÑˆÐ¸Ð±ÐºÑƒ
            self.scaler.update()
            logging.warning("âš ï¸ [SacAgent] NaN/Inf Ð² Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ñ… ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°. Ð¨Ð°Ð³ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½.")
            return {}
        if self.cfg.max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.max_grad_norm)
        self.scaler.step(self.critic_optimizer)
        self.scaler.update()

        # ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð² ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ° (Ñ€ÐµÐ´ÐºÐ¾)
        try:
            max_critic_grad = max(torch.max(p.grad).item() for p in self.critic.parameters() if p.grad is not None)
        except ValueError:
            max_critic_grad = float("nan")
        if (self.update_steps % 200 == 0) or (not torch.isfinite(torch.tensor(max_critic_grad))):
            logging.warning(f"ðŸ“‰ [SacAgent] ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚ ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ°: {max_critic_grad:.4f}")

        for param in self.critic.parameters():
            param.requires_grad = False

        # ÐŸÐ¾Ð»Ð¸Ñ‚Ð¸ÐºÐ°: Ð¾Ð¶Ð¸Ð´Ð°ÐµÐ¼ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ, Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸ÑŽ Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€Ð¸
        dist = self.actor.forward(obs)
        probs = dist.probs
        log_probs = dist.logits.log_softmax(dim=-1)

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² probs Ð¸ log_probs
        if not torch.isfinite(probs).all() or not torch.isfinite(log_probs).all():
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("âš ï¸ [SacAgent] probs Ð¸Ð»Ð¸ log_probs Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð°ÐºÑ‚Ð¾Ñ€Ð°.")
            return {}

        q1_all, q2_all = self.critic(obs)
        min_q_all = torch.min(q1_all, q2_all)

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² q1_all Ð¸ q2_all
        if not torch.isfinite(q1_all).all() or not torch.isfinite(q2_all).all():
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("âš ï¸ [SacAgent] q_values Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹. ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð°ÐºÑ‚Ð¾Ñ€Ð°.")
            return {}

        actor_loss = (alpha * (probs * log_probs).sum(dim=-1, keepdim=True) - (probs * min_q_all).sum(dim=-1, keepdim=True)).mean()

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° NaN Ð² actor_loss
        if not torch.isfinite(actor_loss):
            for param in self.critic.parameters():
                param.requires_grad = True
            logging.warning("âš ï¸ [SacAgent] actor_loss Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½ (NaN/Inf). ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑˆÐ°Ð³ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð°ÐºÑ‚Ð¾Ñ€Ð°.")
            return {}

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð°ÐºÑ‚Ð¾Ñ€Ð° (Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð½Ð° NaN/Inf)
        self.actor_optimizer.zero_grad()
        self.scaler.scale(actor_loss).backward()
        self.scaler.unscale_(self.actor_optimizer)
        bad_grad_actor = any(
            (p.grad is not None) and (not torch.all(torch.isfinite(p.grad))) for p in self.actor.parameters()
        )
        if bad_grad_actor:
            self.actor_optimizer.zero_grad(set_to_none=True)
            for param in self.critic.parameters():
                param.requires_grad = True
            # Ð’Ð°Ð¶Ð½Ð¾: ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ GradScaler, Ð¸Ð½Ð°Ñ‡Ðµ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ unscale_ Ð²Ñ‹Ð·Ð¾Ð²ÐµÑ‚ Ð¾ÑˆÐ¸Ð±ÐºÑƒ
            self.scaler.update()
            logging.warning("âš ï¸ [SacAgent] NaN/Inf Ð² Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ñ… Ð°ÐºÑ‚Ð¾Ñ€Ð°. Ð¨Ð°Ð³ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½.")
            return {}
        if self.cfg.max_grad_norm is not None:
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.cfg.max_grad_norm)
        self.scaler.step(self.actor_optimizer)
        self.scaler.update()

        # ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ð°ÐºÑ‚Ð¾Ñ€Ð° (Ñ€ÐµÐ´ÐºÐ¾)
        try:
            max_actor_grad = max(torch.max(p.grad).item() for p in self.actor.parameters() if p.grad is not None)
        except ValueError:
            max_actor_grad = float("nan")
        if (self.update_steps % 200 == 0) or (not torch.isfinite(torch.tensor(max_actor_grad))):
            logging.warning(f"ðŸ“‰ [SacAgent] ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚ Ð°ÐºÑ‚Ð¾Ñ€Ð°: {max_actor_grad:.4f}")

        for param in self.critic.parameters():
            param.requires_grad = True

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹ (alpha)
        entropy = -(probs * log_probs).sum(dim=-1, keepdim=True)
        alpha_loss = -(self.log_alpha * (entropy - self.target_entropy).detach()).mean()
        if torch.isfinite(alpha_loss):
            self.alpha_optimizer.zero_grad()
            self.scaler.scale(alpha_loss).backward()
            self.scaler.step(self.alpha_optimizer)
            self.scaler.update()
        else:
            logging.warning("âš ï¸ [SacAgent] alpha_loss Ð½Ðµ ÐºÐ¾Ð½ÐµÑ‡ÐµÐ½ (NaN/Inf). ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑˆÐ°Ð³ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ alpha.")

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ†ÐµÐ»ÐµÐ²Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹
        self._soft_update(self.critic, self.target_critic)

        self.update_steps += 1

        return {
            "critic_loss": critic_loss.item() if critic_loss is not None else float('nan'),
            "actor_loss": actor_loss.item() if actor_loss is not None else float('nan'),
            "alpha_loss": alpha_loss.item() if torch.isfinite(alpha_loss) else float('nan'),
            "alpha": alpha.item() if alpha is not None else float('nan'),
            "max_actor_grad": max_actor_grad,
            "max_critic_grad": max_critic_grad,
        }

    def _soft_update(self, source: nn.Module, target: nn.Module) -> None:
        tau = self.cfg.tau
        for target_param, source_param in zip(target.parameters(), source.parameters(), strict=True):
            target_param.data.copy_(
                tau * source_param.data + (1.0 - tau) * target_param.data
            )

    # === Replay buffer integration ===
    def store_transition(
        self,
        state: torch.Tensor,
        action: torch.Tensor,
        reward: torch.Tensor,
        next_state: torch.Tensor,
        done: torch.Tensor,
    ) -> None:
        self.replay_buffer.add_transition(state, action, reward, next_state, done)
        self.total_steps += 1

    def ready_to_update(self) -> bool:
        return len(self.replay_buffer) >= self.cfg.start_learning_after

    def sample_batch(self) -> Optional[Dict[str, torch.Tensor]]:
        batch = self.replay_buffer.sample_batch(self.cfg.batch_size)
        if batch[0] is None:
            return None
        states, actions, rewards, next_states, dones, _, weights, _ = batch

        # ÐŸÐµÑ€ÐµÐ½Ð¾ÑÐ¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° GPU ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾, ÐµÑÐ»Ð¸ Ð±ÑƒÑ„ÐµÑ€ Ð½Ð° CPU
        if self.device.type == "cuda":
            states = states.to(self.device)
            actions = actions.to(self.device)
            rewards = rewards.to(self.device)
            next_states = next_states.to(self.device)
            dones = dones.to(self.device)
            weights = weights.to(self.device)

        return {
            "obs": states,
            "actions": actions,
            "rewards": rewards,
            "next_obs": next_states,
            "dones": dones.float(),
            "weights": weights,
        }

    def save(self, path: str) -> None:
        torch.save(
            {
                "actor": self.actor.state_dict(),
                "critic": self.critic.state_dict(),
                "target_critic": self.target_critic.state_dict(),
                "log_alpha": self.log_alpha.detach(),
            },
            path,
        )

    def load(self, path: str) -> None:
        state = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(state["actor"])
        self.critic.load_state_dict(state["critic"])
        self.target_critic.load_state_dict(state["target_critic"])
        self.log_alpha = state["log_alpha"].to(self.device).requires_grad_()


