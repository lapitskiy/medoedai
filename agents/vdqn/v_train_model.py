import torch
import numpy as np
from agents.vdqn.dqnsolver import DQNSolver
from envs.dqn_model.gym.crypto_trading_env import CryptoTradingEnv
import wandb
from agents.vdqn.cfg.vconfig import vDqnConfig
from envs.dqn_model.gym.gutils import get_nan_stats, log_csv, setup_logger, setup_wandb

cfg = vDqnConfig()

def train_model(dfs: dict, load_previous: bool = False, episodes: int = 200):
    """
    –û–±—É—á–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å DQN –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π —Å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏.

    Args:
        dfs (dict): –°–ª–æ–≤–∞—Ä—å —Å Pandas DataFrames –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (df_5min, df_15min, df_1h).
        load_previous (bool): –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª–∏ —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.
        episodes (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
    Returns:
        str: –°–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è.
    """

    import time    

    all_trades = []
    best_winrate = 0.0
    patience_counter = 0
    patience_limit = 500  # Early stopping –ø–æ—Å–ª–µ 500 —ç–ø–∏–∑–æ–¥–æ–≤ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)

    wandb_run = None

    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')            
        
        if device.type == 'cuda':
            # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            torch.cuda.set_device(0)
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # –û—á–∏—â–∞–µ–º GPU –∫—ç—à
            torch.cuda.empty_cache()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            print(f"Available Memory: {torch.cuda.memory_allocated(0) / 1e9:.1f} GB")
            
        cfg.device = device 
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
        env = CryptoTradingEnv(dfs=dfs) 

        observation_space_dim = env.observation_space.shape[0]
        action_space = env.action_space.n

        logger = setup_logger("rl")
        if getattr(cfg, "use_wandb", False):
            wandb_run, _ = setup_wandb(cfg)
                        
        global_step = 0
        last_time = time.perf_counter()
        _next_tick = {}

        def tick(label: str):
            nonlocal last_time, global_step, _next_tick
            now = time.perf_counter()
            dt_ms = (now - last_time) * 1e3
            last_time = now

            if (dt_ms >= cfg.tick_slow_ms) or (global_step >= _next_tick.get(label, -1)):
                logger.info("[T] %s: %.1f ms", label, dt_ms)
                _next_tick[label] = global_step + cfg.tick_every

        dqn_solver = DQNSolver(observation_space_dim, action_space, load=load_previous)
                
        logger.info("Training started: torch=%s cuda=%s device=%s",
            torch.__version__, torch.version.cuda, device)

        successful_episodes = 0        
        episode_rewards = []
        episode_profits = []
        episode_winrates = []

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ GPU –ø–∞–º—è—Ç—å
        if device.type == 'cuda':
            # –°–æ–∑–¥–∞–µ–º dummy tensor –¥–ª—è —Ä–∞–∑–æ–≥—Ä–µ–≤–∞ GPU
            dummy_tensor = torch.randn(1000, observation_space_dim).to(device)
            _ = dqn_solver.model(dummy_tensor)
            del dummy_tensor
            torch.cuda.empty_cache()

        for episode in range(episodes):
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ
            state = env.reset()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–∞ NaN
            if np.isnan(state).any():
                state = np.nan_to_num(state, nan=0.0)
                logger.warning("NaN detected in initial state, replaced with zeros")
            
            grad_steps = 0
            episode_reward = 0
            tick(f"{episode} episode [{cfg.device}]")
            
            while True:                                                
                env.epsilon = dqn_solver.epsilon
                                  
                action = dqn_solver.act(state)
                state_next, reward, terminal, info = env.step(action)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º next_state –Ω–∞ NaN
                if np.isnan(state_next).any():
                    state_next = np.nan_to_num(state_next, nan=0.0)
                    logger.warning("NaN detected in next_state, replaced with zeros")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –≤ replay buffer
                dqn_solver.store_transition(state, action, reward, state_next, terminal)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                state = state_next
                episode_reward += reward
                global_step += 1
                
                # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —á–∞—â–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                if global_step % cfg.soft_update_every == 0 and len(dqn_solver.memory) >= cfg.batch_size:
                    success, loss, abs_q, q_gap = dqn_solver.experience_replay(need_metrics=True)
                    if success:
                        grad_steps += 1
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º target network —á–∞—â–µ
                        if global_step % cfg.target_update_freq == 0:
                            dqn_solver.update_target_model()
                
                if terminal:
                    break
            
            # –û–±–Ω–æ–≤–ª—è–µ–º epsilon
            dqn_solver.epsilon = max(cfg.eps_final, dqn_solver.epsilon * dqn_solver._eps_decay_rate)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —ç–ø–∏–∑–æ–¥–∞
            if hasattr(env, 'trades') and env.trades:
                all_trades.extend(env.trades)
                
                # –í—ã—á–∏—Å–ª—è–µ–º winrate –¥–ª—è —ç–ø–∏–∑–æ–¥–∞
                profitable_trades = [t for t in env.trades if t.get('profit', 0) > 0]
                episode_winrate = len(profitable_trades) / len(env.trades) if env.trades else 0
                episode_winrates.append(episode_winrate)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ
                if episode_winrate > best_winrate:
                    best_winrate = episode_winrate
                    patience_counter = 0
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    dqn_solver.save_model()
                    logger.info("[INFO] New best winrate: %.3f, saving model", best_winrate)
                else:
                    patience_counter += 1
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Å–¥–µ–ª–æ–∫, –¥–æ–±–∞–≤–ª—è–µ–º 0 winrate
                episode_winrates.append(0.0)
                patience_counter += 1
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if episode % 10 == 0:
                avg_winrate = np.mean(episode_winrates[-10:]) if episode_winrates else 0
                logger.info(f"[INFO] Episode {episode}/{episodes}, Avg Winrate: {avg_winrate:.3f}, Epsilon: {dqn_solver.epsilon:.4f}")
                
                # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
            
            # Early stopping
            if patience_counter >= patience_limit:
                logger.info(f"[INFO] Early stopping triggered after {episode} episodes")
                break

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats_all = dqn_solver.print_trade_stats(all_trades)
        log_csv(cfg.csv_metrics_path, {"scope":"cumulative", "episode": episodes, **stats_all})
        
        if cfg.use_wandb:
            wandb.log({**stats_all, "scope": "cumulative", "episode": episodes})
        
        dqn_solver.save()
        print(stats_all)
        print("–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        if len(episode_winrates) > 10:
            recent_winrate = np.mean(episode_winrates[-10:])
            overall_winrate = np.mean(episode_winrates)
            print(f"üìà Winrate —Ç—Ä–µ–Ω–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤: {recent_winrate:.3f}, –æ–±—â–∏–π: {overall_winrate:.3f}")
            
            if recent_winrate > overall_winrate:
                print("‚úÖ –ú–æ–¥–µ–ª—å —É–ª—É—á—à–∞–µ—Ç—Å—è!")
            else:
                print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è")
        
        return "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ"    
    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º wandb
        if wandb_run is not None:
            wandb_run.finish()