import os
import sys
import logging
import numpy as np
import torch
import wandb
import time
from typing import Dict, List, Optional
import pickle
from pickle import HIGHEST_PROTOCOL

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from agents.vdqn.dqnsolver import DQNSolver
from agents.vdqn.cfg.vconfig import vDqnConfig
from envs.dqn_model.gym.crypto_trading_env_optimized import CryptoTradingEnvOptimized
from envs.dqn_model.gym.gutils import log_csv

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def prepare_data_for_training(dfs: Dict) -> Dict:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—è DataFrame –≤ numpy –º–∞—Å—Å–∏–≤—ã
    
    Args:
        dfs: —Å–ª–æ–≤–∞—Ä—å —Å DataFrame –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        
    Returns:
        Dict: —Å–ª–æ–≤–∞—Ä—å —Å numpy –º–∞—Å—Å–∏–≤–∞–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
    """
    print(f"üìä –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    required_keys = ['df_5min', 'df_15min', 'df_1h']
    for key in required_keys:
        if key not in dfs:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç {key} –≤ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        if dfs[key] is None or dfs[key].empty:
            raise ValueError(f"{key} –ø—É—Å—Ç–æ–π –∏–ª–∏ None")
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã: 5min={len(dfs['df_5min'])}, 15min={len(dfs['df_15min'])}, 1h={len(dfs['df_1h'])}")
    
    return dfs

def train_model_optimized(
    dfs: Dict,
    cfg: Optional[vDqnConfig] = None,
    episodes: int = 10,
    patience_limit: int = 50,
    use_wandb: bool = False
) -> str:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ –±–µ–∑ pandas –≤ hot-path
    
    Args:
        dfs: —Å–ª–æ–≤–∞—Ä—å —Å DataFrame –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (df_5min, df_15min, df_1h)
        cfg: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        episodes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        patience_limit: –ª–∏–º–∏—Ç —Ç–µ—Ä–ø–µ–Ω–∏—è –¥–ª—è early stopping
        use_wandb: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Weights & Biases
        
    Returns:
        str: —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    """
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è wandb
    wandb_run = None
    if use_wandb:
        try:
            run_name = getattr(cfg, 'run_name', 'default') if cfg else 'default'
            config_dict = cfg.__dict__ if cfg else {}
            
            wandb_run = wandb.init(
                project="medoedai-optimized",
                name=f"vDQN-optimized-{run_name}",
                config=config_dict
            )
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å wandb: {e}")
            use_wandb = False
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if cfg is None:
            cfg = vDqnConfig()
            print("‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        dfs = prepare_data_for_training(dfs)
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
        env = CryptoTradingEnvOptimized(
            dfs=dfs,
            cfg=cfg,
            lookback_window=20,
            indicators_config=None  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        )
        
        # –ù–∞—á–∏–Ω–∞–µ–º –æ—Ç—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        training_start_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
        if not hasattr(env, 'observation_space_shape'):
            raise ValueError("–û–∫—Ä—É–∂–µ–Ω–∏–µ –Ω–µ –∏–º–µ–µ—Ç observation_space_shape")
        
        print(f"‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ, —Ä–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è: {env.observation_space_shape}")
        
        # –°–æ–∑–¥–∞–µ–º DQN solver
        print(f"üöÄ –°–æ–∑–¥–∞—é DQN solver")
        
        dqn_solver = DQNSolver(
            observation_space=env.observation_space_shape,
            action_space=env.action_space.n
        )
        
        # üöÄ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è PyTorch 2.x
        if torch.cuda.is_available():
            # –í–∫–ª—é—á–∞–µ–º cudnn benchmark –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # –í–∫–ª—é—á–∞–µ–º TF32 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ Ampere+ GPU
            if hasattr(torch.backends.cuda, 'matmul.allow_tf32'):
                torch.backends.cuda.matmul.allow_tf32 = True
            if hasattr(torch.backends.cudnn, 'allow_tf32'):
                torch.backends.cudnn.allow_tf32 = True
                
            print("üöÄ CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã: cudnn.benchmark, TF32")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
        dqn_solver.load_model()
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        all_trades = []
        episode_winrates = []
        best_winrate = 0.0
        patience_counter = 0
        global_step = 0
        grad_steps = 0
        
        print(f"üéØ –ù–∞—á–∏–Ω–∞—é —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –Ω–∞ {episodes} —ç–ø–∏–∑–æ–¥–æ–≤")
        print(f"üìà –†–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è: {env.observation_space_shape}")
        print(f"üéÆ –†–∞–∑–º–µ—Ä –¥–µ–π—Å—Ç–≤–∏–π: {env.action_space.n}")
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        for episode in range(episodes):         
            state = env.reset()            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ state —è–≤–ª—è–µ—Ç—Å—è numpy –º–∞—Å—Å–∏–≤–æ–º
            if isinstance(state, (list, tuple)):
                state = np.array(state, dtype=np.float32)
            elif not isinstance(state, np.ndarray):
                state = np.array(state, dtype=np.float32)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è
            if episode == 0:
                print(f"üîç –ü–µ—Ä–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: —Ç–∏–ø={type(state)}, —Ä–∞–∑–º–µ—Ä={state.shape if hasattr(state, 'shape') else len(state)}")
            
            episode_reward = 0
            print(f"  üéØ –≠–ø–∏–∑–æ–¥ {episode} –Ω–∞—á–∞—Ç, reward={episode_reward}")
            
            # –≠–ø–∏–∑–æ–¥
            step_count = 0
            while True:
                step_count += 1
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                if step_count % 10000 == 0:
                    print(f"    üîÑ Step {step_count} –≤ —ç–ø–∏–∑–æ–¥–µ {episode}")
                
                env.epsilon = dqn_solver.epsilon
                
                action = dqn_solver.act(state)
                state_next, reward, terminal, info = env.step(action)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º next_state –Ω–∞ NaN
                if isinstance(state_next, (list, tuple)):
                    state_next = np.array(state_next, dtype=np.float32)
                elif not isinstance(state_next, np.ndarray):
                    state_next = np.array(state_next, dtype=np.float32)
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
                try:
                    if np.isnan(state_next).any():
                        state_next = np.nan_to_num(state_next, nan=0.0)
                except (TypeError, ValueError):
                    # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ NaN, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –∏ –ø–æ–ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞ (–±–µ–∑ —Å–ø–∞–º–∞ –≤ –ª–æ–≥)
                    state_next = np.array(state_next, dtype=np.float32)
                    if np.isnan(state_next).any():
                        state_next = np.nan_to_num(state_next, nan=0.0)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –≤ replay buffer
                dqn_solver.store_transition(state, action, reward, state_next, terminal)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                state = state_next
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π state —è–≤–ª—è–µ—Ç—Å—è numpy –º–∞—Å—Å–∏–≤–æ–º
                if isinstance(state, (list, tuple)):
                    state = np.array(state, dtype=np.float32)
                elif not isinstance(state, np.ndarray):
                    state = np.array(state, dtype=np.float32)
                
                episode_reward += reward
                global_step += 1
                
                # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Ä–µ–∂–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–∫–∞–∫ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ)
                soft_update_every = getattr(cfg, 'soft_update_every', 100)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
                batch_size = getattr(cfg, 'batch_size', 64)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ 64
                target_update_freq = getattr(cfg, 'target_update_freq', 1000)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
                
                if global_step % soft_update_every == 0 and len(dqn_solver.memory) >= batch_size:                    
                    success, loss, abs_q, q_gap = dqn_solver.experience_replay(need_metrics=True)
                    if success:
                        grad_steps += 1                        
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º target network —á–∞—â–µ
                        if global_step % target_update_freq == 0:
                            dqn_solver.update_target_model()
                    else:
                        print(f"      ‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å")

                if terminal:
                    break
            
            print(f"  üèÅ –≠–ø–∏–∑–æ–¥ {episode} –∑–∞–≤–µ—Ä—à–µ–Ω, reward={episode_reward:.4f}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
            if hasattr(env, 'action_counts'):
                print(f"  üéÆ –î–µ–π—Å—Ç–≤–∏—è: HOLD={env.action_counts.get(0, 0)}, BUY={env.action_counts.get(1, 0)}, SELL={env.action_counts.get(2, 0)}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º epsilon
            eps_final = getattr(cfg, 'eps_final', 0.01)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π epsilon 0.01
            dqn_solver.epsilon = max(eps_final, dqn_solver.epsilon * dqn_solver._eps_decay_rate)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —ç–ø–∏–∑–æ–¥–∞
            if hasattr(env, 'trades') and env.trades:
                all_trades.extend(env.trades)
                
                # –í—ã—á–∏—Å–ª—è–µ–º winrate –¥–ª—è —ç–ø–∏–∑–æ–¥–∞
                profitable_trades = [t for t in env.trades if t.get('roi', 0) > 0]
                episode_winrate = len(profitable_trades) / len(env.trades) if env.trades else 0
                episode_winrates.append(episode_winrate)
                
                # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
                episode_stats = dqn_solver.print_trade_stats(env.trades)
                print(f"  üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞: —Å–¥–µ–ª–æ–∫={len(env.trades)}, winrate={episode_winrate:.3f}")
                print(f"  üí∞ –ü—Ä–∏–±—ã–ª—å: {episode_stats['avg_profit']:.4f}, –£–±—ã—Ç–æ–∫: {episode_stats['avg_loss']:.4f}")
                print(f"  üìä P/L ratio: {episode_stats['pl_ratio']:.2f}, Bad trades: {episode_stats['bad_trades_count']}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ
                if episode_winrate > best_winrate:
                    best_winrate = episode_winrate
                    patience_counter = 0
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    dqn_solver.save_model()
                    logger.info("[INFO] New best winrate: %.3f, saving model", best_winrate)
                    print(f"  üéâ –ù–æ–≤—ã–π –ª—É—á—à–∏–π winrate: {best_winrate:.3f}!")
                else:
                    patience_counter += 1
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Å–¥–µ–ª–æ–∫, –¥–æ–±–∞–≤–ª—è–µ–º 0 winrate
                episode_winrates.append(0.0)
                patience_counter += 1
                print(f"  ‚ö†Ô∏è –≠–ø–∏–∑–æ–¥ {episode}: –Ω–µ—Ç —Å–¥–µ–ª–æ–∫")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–æ–≤
                if hasattr(env, 'buy_attempts') and env.buy_attempts > 0:
                    vol_rejected = getattr(env, 'buy_rejected_vol', 0)
                    roi_rejected = getattr(env, 'buy_rejected_roi', 0)
                    print(f"  üîç –ü–æ–ø—ã—Ç–∫–∏ –ø–æ–∫—É–ø–∫–∏: {env.buy_attempts}, –æ—Ç–∫–ª–æ–Ω–µ–Ω–æ –ø–æ –æ–±—ä–µ–º—É: {vol_rejected}, –æ—Ç–∫–ª–æ–Ω–µ–Ω–æ –ø–æ ROI: {roi_rejected}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if episode % 10 == 0:
                avg_winrate = np.mean(episode_winrates[-10:]) if episode_winrates else 0
                logger.info(f"[INFO] Episode {episode}/{episodes}, Avg Winrate: {avg_winrate:.3f}, Epsilon: {dqn_solver.epsilon:.4f}")
                
                # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # Early stopping
            if patience_counter >= patience_limit:
                logger.info(f"[INFO] Early stopping triggered after {episode} episodes")
                break

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        training_end_time = time.time()
        total_training_time = training_end_time - training_start_time
        
        print("\n" + "="*60)
        print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–£–ß–ï–ù–ò–Ø")
        print("="*60)
        
        print(f"‚è±Ô∏è –í–†–ï–ú–Ø –û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"  ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_training_time:.2f} —Å–µ–∫—É–Ω–¥ ({total_training_time/60:.1f} –º–∏–Ω—É—Ç)")
        print(f"  ‚Ä¢ –í—Ä–µ–º—è –Ω–∞ —ç–ø–∏–∑–æ–¥: {total_training_time/episodes:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"  ‚Ä¢ –≠–ø–∏–∑–æ–¥–æ–≤ –≤ –º–∏–Ω—É—Ç—É: {episodes/(total_training_time/60):.1f}")
        
        stats_all = dqn_solver.print_trade_stats(all_trades)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if all_trades:
            total_profit = sum([t.get('roi', 0) for t in all_trades if t.get('roi', 0) > 0])
            total_loss = abs(sum([t.get('roi', 0) for t in all_trades if t.get('roi', 0) < 0]))
            avg_duration = np.mean([t.get('duration', 0) for t in all_trades])
            
            print(f"\nüí∞ –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"  ‚Ä¢ –û–±—â–∞—è –ø—Ä–∏–±—ã–ª—å: {total_profit:.4f}")
            print(f"  ‚Ä¢ –û–±—â–∏–π —É–±—ã—Ç–æ–∫: {total_loss:.4f}")
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–¥–µ–ª–∫–∏: {avg_duration:.1f} –º–∏–Ω—É—Ç")
            print(f"  ‚Ä¢ –í—Å–µ–≥–æ —ç–ø–∏–∑–æ–¥–æ–≤: {episodes}")
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π winrate: {np.mean(episode_winrates):.3f}")
        else:
            print(f"\n‚ö†Ô∏è –ù–µ—Ç —Å–¥–µ–ª–æ–∫ –∑–∞ –≤—Å–µ {episodes} —ç–ø–∏–∑–æ–¥–æ–≤!")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É cfg –µ—Å—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        if hasattr(cfg, 'csv_metrics_path'):
            log_csv(cfg.csv_metrics_path, {"scope":"cumulative", "episode": episodes, **stats_all})
        else:
            print("‚ö†Ô∏è csv_metrics_path –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –ø—Ä–æ–ø—É—Å–∫–∞—é –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ CSV")
        
        if hasattr(cfg, 'use_wandb') and cfg.use_wandb:
            wandb.log({**stats_all, "scope": "cumulative", "episode": episodes})
        
        dqn_solver.save()
        print("\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        training_results = {
            'episodes': episodes,
            'total_training_time': total_training_time,
            'episode_winrates': episode_winrates,
            'all_trades': all_trades,
            'best_winrate': best_winrate,
            'final_stats': stats_all,
            'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),
            'model_path': 'dqn_model.pth'
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª
        results_file = f'training_results_{int(time.time())}.pkl'
        with open(results_file, 'wb') as f:
            pickle.dump(training_results, f, protocol=HIGHEST_PROTOCOL)
        
        print(f"üìä –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_file}")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        if len(episode_winrates) > 10:
            recent_winrate = np.mean(episode_winrates[-10:])
            overall_winrate = np.mean(episode_winrates)
            print(f"üìà Winrate —Ç—Ä–µ–Ω–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤: {recent_winrate:.3f}, –æ–±—â–∏–π: {overall_winrate:.3f}")
            
            if recent_winrate > overall_winrate:
                print("‚úÖ –ú–æ–¥–µ–ª—å —É–ª—É—á—à–∞–µ—Ç—Å—è!")
            else:
                print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è")
        
        return "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ"    
    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º wandb
        if wandb_run is not None:
            wandb_run.finish()
