"""
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö GPU –∫–∞—Ä—Ç
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä batch_size –∏ –¥—Ä—É–≥–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç GPU
"""

from dataclasses import dataclass
from typing import Dict, Any
import os
import torch

@dataclass
class GPUConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π GPU"""
    name: str
    vram_gb: float
    batch_size: int
    memory_size: int
    hidden_sizes: tuple
    train_repeats: int
    use_amp: bool
    use_gpu_storage: bool
    learning_rate: float
    description: str
    use_torch_compile: bool = False
    eps_decay_steps: int = 100000
    dropout_rate: float = 0.1

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö GPU
GPU_CONFIGS: Dict[str, GPUConfig] = {
    # Tesla P100 - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å —ç–ø–∏–∑–æ–¥–æ–≤
    "tesla_p100": GPUConfig(
        name="Tesla P100",
        vram_gb=16.0,
        batch_size=128,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 4096 –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
        memory_size=90_000,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏: ~7.5GB VRAM (47% –æ—Ç 16GB)
        hidden_sizes=(256, 128, 64),  # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        train_repeats=3,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 4 –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
        use_amp=False,
        use_gpu_storage=False,  # –í–∫–ª—é—á–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        learning_rate=0.00015,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å –±–æ–ª—å—à–∏–º –±–∞—Ç—á–µ–º
        description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è Tesla P100 (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ CPU —è–¥—Ä–∞)",
        use_torch_compile=True,
        eps_decay_steps=3_000_000,
        dropout_rate=0.25
    ),
    
    # Tesla V100 - –µ—â–µ –±—ã—Å—Ç—Ä–µ–µ —Å Tensor Cores
    "tesla_v100": GPUConfig(
        name="Tesla V100",
        vram_gb=16.0, # 
        batch_size=512,
        memory_size=200_000,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ø–∏–∫–æ–≤ –ø–∞–º—è—Ç–∏ (—Ä–µ–ø–ª–µ–π-–±—É—Ñ–µ—Ä)
        hidden_sizes=(512, 256, 128),
        train_repeats=3,
        use_amp=True,
        use_gpu_storage=True,  # –í–∫–ª—é—á–∞–µ–º GPU storage –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
        learning_rate=0.00018,
        description="–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Tesla V100 —Å Tensor Cores",
        use_torch_compile=True,
        eps_decay_steps=3_000_000,
        dropout_rate=0.25
        #vram_gb=16.0,
        #batch_size=2048,
        #memory_size=300_000,
        #hidden_sizes=(1536, 768, 384),
        #train_repeats=2,
        #use_amp=True,
        #use_gpu_storage=True,
        #learning_rate=0.0001,
        #description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è Tesla V100 —Å Tensor Cores",
        #use_torch_compile=True
    ),
    
    # GTX 1660 Super - –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª—è 6GB VRAM
    "gtx_1660_super": GPUConfig(
        name="GTX 1660 Super",
        vram_gb=6.0,
        batch_size=256,
        memory_size=90_000,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ ~2.7GB VRAM (45% –æ—Ç 6GB)
        hidden_sizes=(512, 256, 128),
        train_repeats=1,
        use_amp=True,
        use_gpu_storage=True,  # –í–∫–ª—é—á–∞–µ–º GPU storage –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
        learning_rate=0.0001,
        description="–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è GTX 1660 Super",
        use_torch_compile=True,
        eps_decay_steps=3_000_000,
        dropout_rate=0.25
    ),
    
    # RTX 3080 - –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è –∫–∞—Ä—Ç–∞
    "rtx_3080": GPUConfig(
        name="RTX 3080",
        vram_gb=10.0,
        batch_size=2048,
        memory_size=1_000_000,
        hidden_sizes=(1024, 512, 256),
        train_repeats=4,
        use_amp=True,
        use_gpu_storage=True,
        learning_rate=0.0001,
        description="–í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è RTX 3080",
        use_torch_compile=True,
        eps_decay_steps=3_000_000,
        dropout_rate=0.25
    ),
    
    # RTX 4090 - —Ç–æ–ø–æ–≤–∞—è –∫–∞—Ä—Ç–∞
    "rtx_4090": GPUConfig(
        name="RTX 4090",
        vram_gb=24.0,
        batch_size=8192,
        memory_size=5_000_000,
        hidden_sizes=(4096, 2048, 1024),
        train_repeats=16,
        use_amp=True,
        use_gpu_storage=True,
        learning_rate=0.0001,
        description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è RTX 4090",
        use_torch_compile=True,
        eps_decay_steps=3_000_000,
        dropout_rate=0.25
    ),
    
    # CPU fallback
    "cpu": GPUConfig(
        name="CPU",
        vram_gb=0.0,
        batch_size=32,
        memory_size=50_000,
        hidden_sizes=(256, 128, 64),
        train_repeats=1,
        use_amp=False,
        use_gpu_storage=False,
        learning_rate=0.0001,
        description="Fallback –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è CPU",
        use_torch_compile=False,
        eps_decay_steps=3_000_000,
        dropout_rate=0.25
    )
}

# –§–ª–∞–≥–∏ –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ—Å—è –≤—ã–≤–æ–¥–∞
_gpu_detect_printed = False
_gpu_info_printed = False

def detect_gpu() -> str:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø GPU –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    if not torch.cuda.is_available():
        return "cpu"
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
        gpu_name = torch.cuda.get_device_name(0).lower()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        
        global _gpu_detect_printed
        if not _gpu_detect_printed:
            print(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ GPU: {torch.cuda.get_device_name(0)}")
            print(f"üíæ VRAM: {gpu_memory:.1f} GB")
            _gpu_detect_printed = True
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø GPU –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        if "tesla v100" in gpu_name:
            return "tesla_v100"
        elif "tesla p100" in gpu_name:
            return "tesla_p100"
        elif "gtx 1660" in gpu_name or "1660 super" in gpu_name:
            return "gtx_1660_super"
        elif "rtx 3080" in gpu_name:
            return "rtx_3080"
        elif "rtx 4090" in gpu_name:
            return "rtx_4090"
        else:
            # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö GPU –≤—ã–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ VRAM
            if gpu_memory >= 20:
                return "rtx_4090"  # 24GB+
            elif gpu_memory >= 15:
                # –î–ª—è 16GB –≤—ã–±–∏—Ä–∞–µ–º V100 –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã Tensor Cores, –∏–Ω–∞—á–µ P100
                if "volta" in gpu_name or "tensor" in gpu_name:
                    return "tesla_v100"  # 16GB —Å Tensor Cores
                else:
                    return "tesla_p100"  # 16GB –±–µ–∑ Tensor Cores
            elif gpu_memory >= 8:
                return "rtx_3080"  # 10GB
            elif gpu_memory >= 4:
                return "gtx_1660_super"  # 6GB
            else:
                return "cpu"  # <4GB
                
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è GPU: {e}")
        return "cpu"

def get_gpu_config(gpu_key: str = None) -> GPUConfig:
    global _gpu_info_printed
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π GPU –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç
    """
    if gpu_key is None:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ GPU
        forced_gpu = os.environ.get('FORCE_GPU_CONFIG', '').strip().lower()
        if forced_gpu and forced_gpu in GPU_CONFIGS:
            if not _gpu_info_printed:
                print(f"üîß –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–∞ GPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {forced_gpu}")
            gpu_key = forced_gpu
        else:
            gpu_key = detect_gpu()
    
    if gpu_key not in GPU_CONFIGS:
        print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è GPU: {gpu_key}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
        gpu_key = "cpu"
    
    config = GPU_CONFIGS[gpu_key]
    if not _gpu_info_printed:
        print(f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config.name}")
        print(f"üìä Batch size: {config.batch_size}")
        print(f"üíæ Memory size: {config.memory_size}")
        print(f"üß† Hidden sizes: {config.hidden_sizes}")
        print(f"üîÑ Train repeats: {config.train_repeats}")
        print(f"‚ö° AMP: {config.use_amp}")
        print(f"üíæ GPU storage: {config.use_gpu_storage}")
        print(f"üß© torch.compile: {config.use_torch_compile}")
        _gpu_info_printed = True
    
    return config

def apply_gpu_config_to_vconfig(gpu_config: GPUConfig) -> Dict[str, Any]:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç GPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫ vDqnConfig
    """
    return {
        'batch_size': gpu_config.batch_size,
        'memory_size': gpu_config.memory_size,
        'hidden_sizes': gpu_config.hidden_sizes,
        'train_repeats': gpu_config.train_repeats,
        'use_amp': gpu_config.use_amp,
        'use_gpu_storage': gpu_config.use_gpu_storage,
        'learning_rate': gpu_config.learning_rate,
        'use_torch_compile': gpu_config.use_torch_compile,
        'eps_decay_steps': gpu_config.eps_decay_steps,
        'dropout_rate': gpu_config.dropout_rate
    }

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
def get_optimal_config() -> GPUConfig:
    """–ü–æ–ª—É—á–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ç–µ–∫—É—â–µ–π GPU"""
    return get_gpu_config()
