from agents.vdqn.dqnn import DQNN
from envs.dqn_model.gym.gutils import check_nan, setup_wandb
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
import os
import pickle
from pickle import HIGHEST_PROTOCOL
import math
from collections import deque
import heapq

# –í–∞—à–∏ –∏–º–ø–æ—Ä—Ç—ã
from agents.vdqn.cfg.vconfig import vDqnConfig

cfg = vDqnConfig()

print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è PyTorch: {cfg.device}")

class PrioritizedReplayBuffer:
    """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –æ–ø—ã—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU"""
    
    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=0.001):
        self.capacity = capacity
        self.alpha = alpha
        self.beta = beta
        self.beta_increment = beta_increment
        self.buffer = []
        self.priorities = []
        self.position = 0
        
        # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.device = cfg.device
        
    def push(self, state, action, reward, next_state, done, gamma_n=1.0):
        max_priority = max(self.priorities) if self.priorities else 1.0
        
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done, gamma_n))
            self.priorities.append(max_priority)
        else:
            self.buffer[self.position] = (state, action, reward, next_state, done, gamma_n)
            self.priorities[self.position] = max_priority
        
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        if len(self.buffer) == 0:
            return [], [], [], [], [], [], []
        
        # –í—ã–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        priorities = np.array(self.priorities[:len(self.buffer)])
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è importance sampling
        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)
        weights /= weights.max()
        weights = torch.FloatTensor(weights).to(self.device)
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        batch = [self.buffer[idx] for idx in indices]
        states, actions, rewards, next_states, dones, gamma_ns = zip(*batch)
        
        return states, actions, rewards, next_states, dones, gamma_ns, weights, indices
    
    def update_priorities(self, indices, priorities):
        for idx, priority in zip(indices, priorities):
            if idx < len(self.priorities):
                self.priorities[idx] = priority
    
    def __len__(self):
        return len(self.buffer)

# --- DQNSolver –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥ PyTorch —Å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ ---
class DQNSolver:
    def __init__(self, observation_space, action_space, load=False):
        self.cfg      = cfg or vDqnConfig()
        self.n_step   = getattr(self.cfg, "n_step", 3)   # 3‚Äë5 —à–∞–≥–æ–≤
        self.n_queue  = deque(maxlen=self.n_step)                
        self.epsilon      = cfg.eps_start
        self.action_space = action_space
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Prioritized Replay Buffer –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        if self.cfg.prioritized:
            self.memory = PrioritizedReplayBuffer(
                self.cfg.memory_size, 
                self.cfg.alpha, 
                self.cfg.beta, 
                self.cfg.beta_increment
            )
        else:
            self.memory = deque(maxlen=self.cfg.memory_size)
        
        self._eps_decay_rate = math.exp(math.log(self.cfg.eps_final / self.cfg.eps_start) / self.cfg.eps_decay_steps)

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏
        self.model = DQNN(
            observation_space, 
            action_space, 
            self.cfg.hidden_sizes,
            dropout_rate=self.cfg.dropout_rate,
            layer_norm=self.cfg.layer_norm,
            dueling=self.cfg.dueling_dqn
        ).to(self.cfg.device)
        
        self.target_model = DQNN(
            observation_space, 
            action_space, 
            self.cfg.hidden_sizes,
            dropout_rate=self.cfg.dropout_rate,
            layer_norm=self.cfg.layer_norm,
            dueling=self.cfg.dueling_dqn
        ).to(self.cfg.device)
        
        # –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞
        self.target_model.load_state_dict(self.model.state_dict())
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º learning rate
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=self.cfg.lr,
            weight_decay=1e-4,
            eps=1e-7
        )
        
        # Scheduler –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ learning rate
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, 
            T_0=1000, 
            T_mult=2,
            eta_min=1e-6
        )
        
        # Loss function
        self.criterion = nn.HuberLoss(delta=1.0)
        
        # –°—á–µ—Ç—á–∏–∫–∏
        self.step_count = 0
        self.update_count = 0
        
        # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.cfg.device.type == 'cuda':
            # –í–∫–ª—é—á–∞–µ–º cudnn benchmark –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # Mixed precision training
            if self.cfg.use_amp:
                self.scaler = torch.cuda.amp.GradScaler()
            else:
                self.scaler = None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if load:
            self.load_model()
            
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        self.model.eval()
        self.target_model.eval()

    def update_target_model(self):
        """–ö–æ–ø–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å."""
        self.target_model.load_state_dict(self.model.state_dict())

    def remember(self, state, action, reward, next_state, done, gamma_n=1.0):
        if self.cfg.prioritized:
            self.memory.push(state, action, reward, next_state, done, gamma_n)
        else:
            self.memory.append((
                torch.tensor(state,      dtype=torch.float32),
                torch.tensor(action,     dtype=torch.long),     
                torch.tensor(reward,     dtype=torch.float32),  
                torch.tensor(next_state, dtype=torch.float32),  
                torch.tensor(done,       dtype=torch.bool),     
                torch.tensor(gamma_n,    dtype=torch.float32)   
            ))
           
                
    def store_transition(self, state, action, reward, next_state, done, gamma_n=1.0):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –≤ replay buffer —Å n-step returns"""
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –≤ n-step –æ—á–µ—Ä–µ–¥—å
        self.n_queue.append((state, action, reward, next_state, done))
        
        # –ï—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∞, –≤—ã—á–∏—Å–ª—è–µ–º n-step return
        if len(self.n_queue) == self.n_step:
            s0, a0, r0, _, _ = self.n_queue[0]
            
            # –í—ã—á–∏—Å–ª—è–µ–º n-step return
            R_n = r0
            gamma_pow = 1.0
            for i in range(1, self.n_step):
                _, _, r, _, d = self.n_queue[i]
                if d:  # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è, –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                    break
                gamma_pow *= self.cfg.gamma
                R_n += gamma_pow * r
            
            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            s_n = self.n_queue[-1][3]  # next_state –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
            d_n = self.n_queue[-1][4]  # done –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º "—É–∫—Ä—É–ø–Ω—ë–Ω–Ω—ã–π" –ø–µ—Ä–µ—Ö–æ–¥ –≤ –±—É—Ñ–µ—Ä
            if self.cfg.prioritized:
                self.memory.push(s0, a0, R_n, s_n, d_n, gamma_pow)
            else:
                self.memory.append((s0, a0, R_n, s_n, d_n, gamma_pow))
            
            # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è ‚Äî –æ—á–∏—â–∞–µ–º –æ—á–µ—Ä–µ–¥—å
            if done:
                self.n_queue.clear()
                
    def act(self, state):
        if np.random.rand() < self.epsilon:
            return random.randrange(self.action_space)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –∏ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –Ω—É–ª–∏
        if torch.isnan(torch.tensor(state)).any():
            state = np.nan_to_num(state, nan=0.0)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.cfg.device)
            q_values = self.model(state_tensor)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –≤ –≤—ã—Ö–æ–¥–µ
            if torch.isnan(q_values).any():
                print("Warning: NaN detected in Q-values, using random action")
                return random.randrange(self.action_space)
            
            return q_values.argmax().item()

    def experience_replay(self, need_metrics=False):
        if len(self.memory) < self.cfg.batch_size:
            return False, None, None, None
        
        # –ü–æ–ª—É—á–∞–µ–º batch
        if self.cfg.prioritized:
            states, actions, rewards, next_states, dones, gamma_ns, weights, indices = \
                self.memory.sample(self.cfg.batch_size)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU
            states = torch.stack([torch.FloatTensor(s) for s in states]).to(self.cfg.device, non_blocking=True)
            actions = torch.LongTensor(actions).to(self.cfg.device, non_blocking=True)
            rewards = torch.FloatTensor(rewards).to(self.cfg.device, non_blocking=True)
            next_states = torch.stack([torch.FloatTensor(s) for s in next_states]).to(self.cfg.device, non_blocking=True)
            dones = torch.BoolTensor(dones).to(self.cfg.device, non_blocking=True)
            gamma_ns = torch.FloatTensor(gamma_ns).to(self.cfg.device, non_blocking=True)
            weights = weights.to(self.cfg.device, non_blocking=True)
        else:
            batch = random.sample(self.memory, self.cfg.batch_size)
            states, actions, rewards, next_states, dones, gamma_ns = zip(*batch)
            
            states = torch.stack(states).to(self.cfg.device, non_blocking=True)
            actions = actions.to(self.cfg.device, non_blocking=True)
            rewards = rewards.to(self.cfg.device, non_blocking=True)
            next_states = torch.stack(next_states).to(self.cfg.device, non_blocking=True)
            dones = dones.to(self.cfg.device, non_blocking=True)
            gamma_ns = gamma_ns.to(self.cfg.device, non_blocking=True)
            weights = torch.ones(self.cfg.batch_size).to(self.cfg.device, non_blocking=True)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN
        if torch.isnan(states).any() or torch.isnan(next_states).any():
            print("Warning: NaN detected in states, skipping batch")
            return False, None, None, None
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        self.model.train()
        
        # Double DQN: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å–µ—Ç—å –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π, —Ü–µ–ª–µ–≤—É—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        if self.cfg.double_dqn:
            with torch.no_grad():
                next_actions = self.model(next_states).argmax(dim=1)
                next_q_values = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        else:
            with torch.no_grad():
                next_q_values = self.target_model(next_states).max(1)[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º target Q-values
        target_q_values = rewards + (gamma_ns * next_q_values * ~dones)
        
        # –¢–µ–∫—É—â–∏–µ Q-values
        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # –í—ã—á–∏—Å–ª—è–µ–º loss —Å importance sampling weights
        td_errors = target_q_values - current_q_values
        loss = (weights * self.criterion(current_q_values, target_q_values)).mean()
        
        # Mixed Precision Training –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if self.scaler is not None:
            self.optimizer.zero_grad()
            self.scaler.scale(loss).backward()
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            if self.cfg.grad_clip:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)
            
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            # –û–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            self.optimizer.zero_grad()
            loss.backward()
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            if self.cfg.grad_clip:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)
            
            self.optimizer.step()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PER
        if self.cfg.prioritized:
            priorities = (torch.abs(td_errors) + 1e-6).detach().cpu().numpy()
            self.memory.update_priorities(indices, priorities)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º scheduler
        self.scheduler.step()
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è inference
        self.model.eval()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        if need_metrics:
            abs_q = current_q_values.abs().mean().item()
            q_gap = (target_q_values - current_q_values).abs().mean().item()
            return True, loss.item(), abs_q, q_gap
        
        return True, None, None, None

    def print_trade_stats(self, trades):
        if not trades:
            return {"trades_count": 0, "winrate": 0.0, "avg_profit": 0.0, "avg_loss": 0.0}
        
        profits = [t["profit"] for t in trades if t["profit"] > 0]
        losses = [t["profit"] for t in trades if t["profit"] < 0]
        
        winrate = len(profits) / len(trades) if trades else 0.0
        avg_profit = np.mean(profits) if profits else 0.0
        avg_loss = np.mean(losses) if losses else 0.0
        avg_roi = np.mean([t["profit"] for t in trades]) if trades else 0.0
        
        pl_ratio = abs(avg_profit / avg_loss) if avg_loss != 0 else 0.0
        
        # –°—á–∏—Ç–∞–µ–º –ø–ª–æ—Ö–∏–µ —Å–¥–µ–ª–∫–∏
        bad_trades = [t for t in trades if abs(t["profit"]) < 0.001]  # <0.1%
        bad_trades_count = len(bad_trades)
        
        stats = {
            "trades_count": len(trades),
            "winrate": winrate,
            "avg_profit": avg_profit,
            "avg_loss": avg_loss,
            "avg_roi": avg_roi,
            "pl_ratio": pl_ratio,
            "bad_trades_count": bad_trades_count
        }
        
        print(f"üìä - Trades: {stats['trades_count']}, Winrate: {stats['winrate']*100:.2f}%, "
              f"Avg P: {stats['avg_profit']:.3f}, Avg L: {stats['avg_loss']:.3f}, "
              f"P/L ratio: {stats['pl_ratio']:.2f}")
        print(f"‚ùó Bad trades (<0.1% ROI): {stats['bad_trades_count']}")
        
        return stats

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ replay buffer"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'target_model_state_dict': self.target_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epsilon': self.epsilon,
            'cfg': self.cfg
        }, self.cfg.model_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º replay buffer
        with open(self.cfg.buffer_path, 'wb') as f:
            pickle.dump(self.memory, f, protocol=HIGHEST_PROTOCOL)
        
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.cfg.model_path}")
        print(f"Replay buffer —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {self.cfg.buffer_path}")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
        if os.path.exists(self.cfg.model_path):
            try:
                checkpoint = torch.load(self.cfg.model_path, map_location=self.cfg.device)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
                if 'model_state_dict' in checkpoint:
                    model_state = checkpoint['model_state_dict']
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è
                    if 'net.feature_layers.0.weight' in model_state:
                        saved_input_size = model_state['net.feature_layers.0.weight'].shape[1]
                        current_input_size = self.model.net.feature_layers[0].weight.shape[1]
                        
                        if saved_input_size != current_input_size:
                            print(f"‚ö†Ô∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è {saved_input_size}, —Ç–µ–∫—É—â–∞—è {current_input_size}")
                            print("üîÑ –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π")
                            return
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                    self.target_model.load_state_dict(checkpoint['model_state_dict'])
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                    if 'optimizer_state_dict' in checkpoint:
                        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    if 'scheduler_state_dict' in checkpoint:
                        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    if 'epsilon' in checkpoint:
                        self.epsilon = checkpoint['epsilon']
                        
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {self.cfg.model_path}")
                else:
                    print(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç checkpoint –≤ {self.cfg.model_path}")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
                print("üîÑ –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å")
        else:
            print(f"üìù –§–∞–π–ª –º–æ–¥–µ–ª–∏ {self.cfg.model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å")

    def load_state(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç replay buffer"""
        if os.path.exists(self.cfg.buffer_path):
            with open(self.cfg.buffer_path, 'rb') as f:
                self.memory = pickle.load(f)
            print(f"Replay buffer –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {self.cfg.buffer_path}")
        else:
            print(f"–§–∞–π–ª replay buffer {self.cfg.buffer_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")

    def soft_update(self, tau=0.01):
        """Soft update target network"""
        for target_param, local_param in zip(self.target_model.parameters(), self.model.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)        

    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å (–∞–Ω–∞–ª–æ–≥ save)"""
        self.save()
        
    def update_target_model(self):
        """Hard update target network –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è"""
        self.target_model.load_state_dict(self.model.state_dict())
        
    def store_transition(self, state, action, reward, next_state, done, gamma_n=1.0):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –≤ replay buffer —Å n-step returns"""
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –≤ n-step –æ—á–µ—Ä–µ–¥—å
        self.n_queue.append((state, action, reward, next_state, done))
        
        # –ï—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∞, –≤—ã—á–∏—Å–ª—è–µ–º n-step return
        if len(self.n_queue) == self.n_step:
            s0, a0, r0, _, _ = self.n_queue[0]
            
            # –í—ã—á–∏—Å–ª—è–µ–º n-step return
            R_n = r0
            gamma_pow = 1.0
            for i in range(1, self.n_step):
                _, _, r, _, d = self.n_queue[i]
                if d:  # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è, –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                    break
                gamma_pow *= self.cfg.gamma
                R_n += gamma_pow * r
            
            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            s_n = self.n_queue[-1][3]  # next_state –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
            d_n = self.n_queue[-1][4]  # done –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º "—É–∫—Ä—É–ø–Ω—ë–Ω–Ω—ã–π" –ø–µ—Ä–µ—Ö–æ–¥ –≤ –±—É—Ñ–µ—Ä
            if self.cfg.prioritized:
                self.memory.push(s0, a0, R_n, s_n, d_n, gamma_pow)
            else:
                self.memory.append((s0, a0, R_n, s_n, d_n, gamma_pow))
            
            # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è ‚Äî –æ—á–∏—â–∞–µ–º –æ—á–µ—Ä–µ–¥—å
            if done:
                self.n_queue.clear()
                
    def remember(self, state, action, reward, next_state, done, gamma_n=1.0):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è store_transition"""
        self.store_transition(state, action, reward, next_state, done, gamma_n)
        




