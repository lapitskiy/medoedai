from agents.vdqn.dqnn import DQNN
from envs.dqn_model.gym.gutils import check_nan, setup_wandb
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
import os
import pickle
from pickle import HIGHEST_PROTOCOL
import math
from collections import deque

# –í–∞—à–∏ –∏–º–ø–æ—Ä—Ç—ã
from agents.vdqn.cfg.vconfig import vDqnConfig

cfg = vDqnConfig()

print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è PyTorch: {cfg.device}")

# --- DQNSolver –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥ PyTorch ---
class DQNSolver:
    def __init__(self, observation_space, action_space, load=False):
        self.cfg      = cfg or vDqnConfig()
        self.n_step   = getattr(self.cfg, "n_step", 3)   # 3‚Äë5 —à–∞–≥–æ–≤
        self.n_queue  = deque(maxlen=self.n_step)                
        self.epsilon      = cfg.eps_start
        self.action_space = action_space
        self.memory = deque(maxlen=self.cfg.memory_size)       
        
        self._eps_decay_rate = math.exp(math.log(self.cfg.eps_final / self.cfg.eps_start) / self.cfg.eps_decay_steps)        # ‚âà 0.99986 –ø—Ä–∏ 10k —à–∞–≥–æ–≤

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ PyTorch
        self.model = DQNN(
            obs_dim=observation_space,
            act_dim=action_space,
            hidden_sizes=self.cfg.hidden_sizes   # –±–µ—Ä—ë–º –∏–∑ dataclass‚Äë–∫–æ–Ω—Ñ–∏–≥–∞
            ).to(self.cfg.device)
        
        # Target Network (—á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ DQN –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
        # –≠—Ç–æ –∫–æ–ø–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ—Ç–æ—Ä–æ–π –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Ä–µ–∂–µ
        self.target_model = DQNN(
            obs_dim=observation_space,
            act_dim=action_space,
            hidden_sizes=self.cfg.hidden_sizes
            ).to(self.cfg.device)
        
        print("[DQN] model device:", next(self.model.parameters()).device)  # <-- self.model        
        
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.cfg.lr)
        self.criterion = nn.SmoothL1Loss() # –î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π
        
        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(
                             self.optimizer, gamma=0.999)
        
        if load:
            self.load_model()
            self.load_state()
                
        self.update_target_model() # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é —Å–µ—Ç—å
        self.target_model.eval() # –¶–µ–ª–µ–≤–∞—è —Å–µ—Ç—å –≤—Å–µ–≥–¥–∞ –≤ —Ä–µ–∂–∏–º–µ –æ—Ü–µ–Ω–∫–∏

    def update_target_model(self):
        """–ö–æ–ø–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å."""
        self.target_model.load_state_dict(self.model.state_dict())

    def remember(self, state, action, reward, next_state, done, gamma_n=1.0):
        self.memory.append((
            torch.tensor(state,      dtype=torch.float32),  # s‚ÇÄ
            torch.tensor(action,     dtype=torch.long),     # a‚ÇÄ
            torch.tensor(reward,     dtype=torch.float32),  # R‚Çô
            torch.tensor(next_state, dtype=torch.float32),  # s‚Çô
            torch.tensor(done,       dtype=torch.bool),     # done‚Çô
            torch.tensor(gamma_n,    dtype=torch.float32)   # Œ≥‚Åø
        ))
           
                
    def store_transition(self, s, a, r, s_next, done):
        # 1. –∫–ª–∞–¥—ë–º –ø–µ—Ä–µ—Ö–æ–¥ –≤ –æ—á–µ—Ä–µ–¥—å
        self.n_queue.append((s, a, r, s_next, done))

        # 2. –µ—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–æ—Å—å n —à–∞–≥–æ–≤ –ò–õ–ò —ç–ø–∏–∑–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω
        if len(self.n_queue) == self.n_step or done:
            R_n, gamma_pow = 0.0, 1.0
            for (_, _, r_i, _, _) in self.n_queue:
                R_n      += gamma_pow * r_i
                gamma_pow *= self.cfg.gamma    # Œ≥, Œ≥¬≤, Œ≥¬≥ ‚Ä¶

            s0, a0, _, s_n, d_n = self.n_queue[0]

            # 3. —Å–æ—Ö—Ä–∞–Ω—è–µ–º ¬´—É–∫—Ä—É–ø–Ω—ë–Ω–Ω—ã–π¬ª –ø–µ—Ä–µ—Ö–æ–¥ –≤ –±—É—Ñ–µ—Ä
            self.remember(s0, a0, R_n, s_n, d_n, gamma_pow)

            # 4. –µ—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è ‚Äî –æ—á–∏—Å—Ç–∏—Ç—å –æ—á–µ—Ä–µ–¥—å
            if done:
                self.n_queue.clear()                
                
    def act(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.choice([0,1,2], p=[0.5,0.25,0.25])
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.cfg.device)
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        with torch.no_grad():
            q_values = self.model(state_tensor)
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ–º
        return torch.argmax(q_values[0]).item()
          
    def save(self):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        torch.save(self.model.state_dict(), self.cfg.model_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π –±—É—Ñ–µ—Ä –≤ .bak
        if os.path.exists(self.cfg.buffer_path):
            os.rename(self.cfg.buffer_path, self.cfg.buffer_path + ".bak")

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ temp-–ø–∞–ø–∫–∞ –µ—Å—Ç—å
        tmp_path = f"./temp/{self.cfg.buffer_path}.tmp"
        os.makedirs(os.path.dirname(tmp_path), exist_ok=True)

        agent_data = {
            "memory": self.memory,
            "epsilon": self.epsilon
        }

        with open(tmp_path, "wb") as f:
            pickle.dump(agent_data, f, protocol=HIGHEST_PROTOCOL)

        os.replace(tmp_path, self.cfg.buffer_path)
    
    def experience_replay(self, need_metrics: bool = False):
        """
        ‚Ü©Ô∏é did_step: bool
        td_loss : torch.Tensor|None (detached)
        abs_q   : torch.Tensor|None (detached)
        q_gap   : torch.Tensor|None (detached)
        """
        if len(self.memory) < self.cfg.batch_size:
            return False, None, None, None

        batch = random.sample(self.memory, self.cfg.batch_size)
        states, actions, rewards, next_states, dones, gammas = zip(*batch)

        device = self.cfg.device

        # –í–ê–ñ–ù–û: —è–≤–Ω—ã–µ —Ç–∏–ø—ã
        states      = torch.stack(states,      dim=0).to(device=device, dtype=torch.float32, non_blocking=True)
        actions     = torch.stack(actions,     dim=0).to(device=device, dtype=torch.long,     non_blocking=True)  # –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è gather
        rewards     = torch.stack(rewards,     dim=0).to(device=device, dtype=torch.float32,  non_blocking=True)
        next_states = torch.stack(next_states, dim=0).to(device=device, dtype=torch.float32,  non_blocking=True)
        dones       = torch.stack(dones,       dim=0).to(device=device, dtype=torch.bool,     non_blocking=True)
        gammas   = torch.tensor(gammas, device=device, dtype=torch.float32) 

        # ---- Q(s,a) ----
        q_all     = self.model(states)                     # (B, A)
        current_q = q_all.gather(1, actions.unsqueeze(1)).squeeze(1)

        # ---- r + Œ≥¬∑QÃÑ(s', argmax_a Q_online(s',a)) ---- (Double DQN)
        with torch.no_grad():
            next_actions = self.model(next_states).argmax(dim=1, keepdim=True)
            next_q       = self.target_model(next_states).gather(1, next_actions).squeeze(1)
            target_q  =    rewards + gammas * next_q * (~dones).float()   # !!! Œ≥‚Åø –∑–¥–µ—Å—å

        # ---- loss ----
        loss = self.criterion(current_q, target_q)

        # ---- back‚Äëprop ----
        self.optimizer.zero_grad(set_to_none=True)
        loss.backward()
        if not check_nan("grad", *(p.grad for p in self.model.parameters() if p.grad is not None)):
            self.optimizer.zero_grad()
            return False, torch.tensor(float('nan')), None, None
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10)
        self.optimizer.step()     
        
        for layer in self.model.modules():
            if isinstance(layer, nn.Linear):
                torch.nan_to_num_(layer.weight, nan=0.0, posinf=1e3, neginf=-1e3)   
                
        self.scheduler.step()
        

        abs_q = q_gap = None
        if need_metrics:
            with torch.no_grad():
                abs_q = q_all.abs().mean().detach()
                q_gap = (q_all - self.target_model(states)).abs().mean().detach()

        return True, loss.detach(), abs_q, q_gap

    def load_model(self):
        if os.path.exists(self.cfg.model_path):
            self.model.load_state_dict(torch.load(self.cfg.model_path, map_location=self.cfg.device))
            self.model.eval()
            print("‚úÖ - –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑", self.cfg.model_path)
        else:
            print("‚ö†Ô∏è - –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å.")

    def load_state(self):
        if os.path.exists(self.cfg.buffer_path):
            try:
                print("–ó–∞–≥—Ä—É–∑–∫–∞ replay buffer –∏ epsilon...")
                with open(self.cfg.buffer_path, "rb") as f:
                    agent_data = pickle.load(f)

                if isinstance(agent_data, dict):
                    self.memory = agent_data.get("memory", deque(maxlen=self.cfg.memory_size))
                    self.epsilon = agent_data.get("epsilon", 1.0)
                else:
                    self.memory = agent_data
                    self.epsilon = self.cfg.eps_start    

                print(f"‚úÖ - Replay buffer –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {self.cfg.buffer_path}, {len(self.memory)} –∑–∞–ø–∏—Å–µ–π.")
            except (EOFError, pickle.UnpicklingError):
                with open(self.cfg.buffer_path + ".bak", "rb") as f:
                    agent_data = pickle.load(f)       
                    if isinstance(agent_data, dict):
                        self.memory = agent_data.get("memory", deque(maxlen=self.cfg.memory_size))
                        self.epsilon = agent_data.get("epsilon", self.cfg.eps_start)
                    else:
                        self.memory = agent_data
                        self.epsilon = self.cfg.eps_start
                print(f"‚úÖ - Replay buffer –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ BAK {self.cfg.buffer_path}.bak, {len(self.memory)} –∑–∞–ø–∏—Å–µ–π.")

            except Exception as e:
                print("‚ö†Ô∏è - –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ replay buffer:", e)
        else:
            print(f"‚ö†Ô∏è - –§–∞–π–ª replay buffer –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ {self.cfg.buffer_path}. –ü–∞–º—è—Ç—å –Ω–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
            
    def print_trade_stats(self, trades=None):
        profits = [t['roi'] for t in trades if t['roi'] > 0]
        losses = [t['roi'] for t in trades if t['roi'] <= 0]
        winrate = len(profits) / len(trades) if trades else 0
        avg_profit = np.mean(profits) if profits else 0
        avg_loss = np.mean(losses) if losses else 0
        avg_roi = np.mean([t['roi'] for t in trades]) if trades else 0
        bad_trades = [t for t in trades if t['roi'] < 0.001]
        pl_ratio = abs(avg_profit / avg_loss) if avg_loss != 0 else float("inf")

        print(f"üìä - Trades: {len(trades)}, Winrate: {winrate:.2%}, Avg P: {avg_profit:.3f}, Avg L: {avg_loss:.3f}, P/L ratio: {pl_ratio:.2f}")
        print(f"‚ùó Bad trades (<0.1% ROI): {len(bad_trades)}")

        return {
            "trades_count": len(trades),
            "winrate": winrate,
            "avg_profit": avg_profit,
            "avg_loss": avg_loss,   
            "avg_roi": avg_roi,   
            "pl_ratio": pl_ratio,
            "bad_trades_count": len(bad_trades)
        }
        
    @torch.no_grad()
    def soft_update(self, tau: float = 1e-3):
        """
        Polyak‚Äë–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: target ‚Üê œÑ¬∑online + (1‚ÄëœÑ)¬∑target
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ self.soft_update(tau).
        """
        for tgt_param, src_param in zip(self.target_model.parameters(),
                                        self.model.parameters()):
            tgt_param.data.copy_(tau * src_param.data + (1.0 - tau) * tgt_param.data)        




