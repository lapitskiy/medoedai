from agents.vdqn.dqnn import DQNN
from envs.dqn_model.gym.gutils import setup_wandb
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
import os
import pickle
from pickle import HIGHEST_PROTOCOL
import math
from collections import deque

# –í–∞—à–∏ –∏–º–ø–æ—Ä—Ç—ã
from agents.vdqn.cfg.vconfig import vDqnConfig

cfg = vDqnConfig()

print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è PyTorch: {cfg.device}")

# --- DQNSolver –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥ PyTorch ---
class DQNSolver:
    def __init__(self, observation_space, action_space, load=False):
        self.cfg          = cfg
        self.epsilon      = cfg.eps_start
        self.action_space = action_space
        self.memory = deque(maxlen=self.cfg.memory_size)       
        
        self._eps_decay_rate = math.exp(math.log(self.cfg.eps_final / self.cfg.eps_start) / self.cfg.eps_decay_steps)        # ‚âà 0.99986 –ø—Ä–∏ 10k —à–∞–≥–æ–≤

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ PyTorch
        self.model = DQNN(
            obs_dim=observation_space,
            act_dim=action_space,
            hidden_sizes=self.cfg.hidden_sizes   # –±–µ—Ä—ë–º –∏–∑ dataclass‚Äë–∫–æ–Ω—Ñ–∏–≥–∞
            ).to(self.cfg.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.cfg.lr)
        self.criterion = nn.MSELoss() # –î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π
        
        if load:
            self.load_model()
            self.load_state()
        
        # Target Network (—á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ DQN –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
        # –≠—Ç–æ –∫–æ–ø–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ—Ç–æ—Ä–æ–π –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Ä–µ–∂–µ
        self.target_model = DQNN(
            obs_dim=observation_space,
            act_dim=action_space,
            hidden_sizes=self.cfg.hidden_sizes
            ).to(self.cfg.device)
        self.update_target_model() # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é —Å–µ—Ç—å
        self.target_model.eval() # –¶–µ–ª–µ–≤–∞—è —Å–µ—Ç—å –≤—Å–µ–≥–¥–∞ –≤ —Ä–µ–∂–∏–º–µ –æ—Ü–µ–Ω–∫–∏

    def update_target_model(self):
        """–ö–æ–ø–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å."""
        self.target_model.load_state_dict(self.model.state_dict())

    def remember(self, state, action, reward, next_state, done):
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy –º–∞—Å—Å–∏–≤—ã –≤ —Ç–µ–Ω–∑–æ—Ä—ã PyTorch
        state_t = torch.tensor(state, dtype=torch.float32)
        action_t = torch.tensor(action, dtype=torch.long) # –î–µ–π—Å—Ç–≤–∏–µ - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å
        reward_t = torch.tensor(reward, dtype=torch.float32)
        next_state_t = torch.tensor(next_state, dtype=torch.float32)
        done_t = torch.tensor(done, dtype=torch.bool)
        
        self.memory.append((state_t, action_t, reward_t, next_state_t, done_t))

    def act(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.choice([0,1,2], p=[0.5,0.25,0.25])
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.cfg.device)
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        with torch.no_grad():
            q_values = self.model(state_tensor)
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ–º
        return torch.argmax(q_values[0]).item()
          
    def save(self):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        torch.save(self.model.state_dict(), self.cfg.model_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π –±—É—Ñ–µ—Ä –≤ .bak
        if os.path.exists(self.cfg.buffer_path):
            os.rename(self.cfg.buffer_path, self.cfg.buffer_path + ".bak")

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ temp-–ø–∞–ø–∫–∞ –µ—Å—Ç—å
        tmp_path = f"./temp/{self.cfg.buffer_path}.tmp"
        os.makedirs(os.path.dirname(tmp_path), exist_ok=True)

        agent_data = {
            "memory": self.memory,
            "epsilon": self.epsilon
        }

        with open(tmp_path, "wb") as f:
            pickle.dump(agent_data, f, protocol=HIGHEST_PROTOCOL)

        os.replace(tmp_path, self.cfg.buffer_path)

    def experience_replay(self):
        
        """
        ‚Ü©Ô∏é  did_step: bool          ‚Äî –±—ã–ª –ª–∏ —Å–¥–µ–ª–∞–Ω grad —à–∞–≥
            td_loss : float | None ‚Äî Huber/MSE loss
            abs_q   : float | None ‚Äî ‚ü®|Q(s,a)|‚ü© –ø–æ –±–∞—Ç—á—É
            q_gap   : float | None ‚Äî ‚ü®|Q_online   Q_target|‚ü©
        """
        
        if len(self.memory) < self.cfg.batch_size:
            return False, None, None, None      # ‚Üê –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–ª–∏


        batch = random.sample(self.memory, self.cfg.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states      = torch.stack(states).to(self.cfg.device)
        actions     = torch.stack(actions).to(self.cfg.device)
        rewards     = torch.stack(rewards).to(self.cfg.device)
        next_states = torch.stack(next_states).to(self.cfg.device)
        dones       = torch.stack(dones).to(self.cfg.device)

        # ---- Q(s,a) ----
        q_all      = self.model(states)                 # (B, n_actions)
        current_q  = q_all.gather(1, actions.unsqueeze(1)).squeeze(1)

        # ---- r + Œ≥¬∑max‚ÄÜQ' ----
        with torch.no_grad():
            next_q  = self.target_model(next_states).max(1)[0]
        target_q = rewards + self.cfg.gamma * next_q * (~dones)

        # ---- loss ----
        loss = self.criterion(current_q, target_q)

        # ---- back‚Äëprop ----
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # ---- –¥–æ–ø.‚Äë–º–µ—Ç—Ä–∏–∫–∏ ----
        abs_q  = q_all.abs().mean().item()                             # ‚ü®|Q|‚ü©
        with torch.no_grad():
            q_gap = (self.model(states) - self.target_model(states)).abs().mean().item()

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è —Å–Ω–∞—Ä—É–∂–∏
        self.abs_q_mean = abs_q
        self.q_gap      = q_gap

        return True, loss.item(), abs_q, q_gap
          

    def load_model(self):
        if os.path.exists(self.cfg.model_path):
            self.model.load_state_dict(torch.load(self.cfg.model_path, map_location=self.cfg.device))
            self.model.eval()
            print("‚úÖ - –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑", self.cfg.model_path)
        else:
            print("‚ö†Ô∏è - –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å.")

    def load_state(self):
        if os.path.exists(self.cfg.buffer_path):
            try:
                print("–ó–∞–≥—Ä—É–∑–∫–∞ replay buffer –∏ epsilon...")
                with open(self.cfg.buffer_path, "rb") as f:
                    agent_data = pickle.load(f)

                if isinstance(agent_data, dict):
                    self.memory = agent_data.get("memory", deque(maxlen=self.cfg.memory_size))
                    self.epsilon = agent_data.get("epsilon", 1.0)
                else:
                    self.memory = agent_data
                    self.epsilon = self.cfg.eps_start    

                print(f"‚úÖ - Replay buffer –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {self.cfg.buffer_path}, {len(self.memory)} –∑–∞–ø–∏—Å–µ–π.")
            except (EOFError, pickle.UnpicklingError):
                with open(self.cfg.buffer_path + ".bak", "rb") as f:
                    agent_data = pickle.load(f)       
                    if isinstance(agent_data, dict):
                        self.memory = agent_data.get("memory", deque(maxlen=self.cfg.memory_size))
                        self.epsilon = agent_data.get("epsilon", self.cfg.eps_start)
                    else:
                        self.memory = agent_data
                        self.epsilon = self.cfg.eps_start
                print(f"‚úÖ - Replay buffer –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ BAK {self.cfg.buffer_path}.bak, {len(self.memory)} –∑–∞–ø–∏—Å–µ–π.")

            except Exception as e:
                print("‚ö†Ô∏è - –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ replay buffer:", e)
        else:
            print(f"‚ö†Ô∏è - –§–∞–π–ª replay buffer –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ {self.cfg.buffer_path}. –ü–∞–º—è—Ç—å –Ω–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
            
    def print_trade_stats(self, trades=None):
        profits = [t['roi'] for t in trades if t['roi'] > 0]
        losses = [t['roi'] for t in trades if t['roi'] <= 0]
        winrate = len(profits) / len(trades) if trades else 0
        avg_profit = np.mean(profits) if profits else 0
        avg_loss = np.mean(losses) if losses else 0
        avg_roi = np.mean([t['roi'] for t in trades]) if trades else 0
        bad_trades = [t for t in trades if t['roi'] < 0.001]
        pl_ratio = abs(avg_profit / avg_loss) if avg_loss != 0 else float("inf")

        print(f"üìä - Trades: {len(trades)}, Winrate: {winrate:.2%}, Avg P: {avg_profit:.3f}, Avg L: {avg_loss:.3f}, P/L ratio: {pl_ratio:.2f}")
        print(f"‚ùó Bad trades (<0.1% ROI): {len(bad_trades)}")

        return {
            "trades_count": len(trades),
            "winrate": winrate,
            "avg_profit": avg_profit,
            "avg_loss": avg_loss,   
            "avg_roi": avg_roi,   
            "pl_ratio": pl_ratio,
            "bad_trades_count": len(bad_trades)
        }
        
    @torch.no_grad()
    def soft_update(self, tau: float = 1e-3):
        """
        Polyak‚Äë–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: target ‚Üê œÑ¬∑online + (1‚ÄëœÑ)¬∑target
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ self.soft_update(tau).
        """
        for tgt_param, src_param in zip(self.target_model.parameters(),
                                        self.model.parameters()):
            tgt_param.data.copy_(tau * src_param.data + (1.0 - tau) * tgt_param.data)        




