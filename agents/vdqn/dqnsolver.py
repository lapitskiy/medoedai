from agents.vdqn.dqnn import DQNN
from envs.dqn_model.gym.gutils import check_nan, setup_wandb
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
import os
import pickle
from pickle import HIGHEST_PROTOCOL
import math
from collections import deque
import heapq

# –í–∞—à–∏ –∏–º–ø–æ—Ä—Ç—ã
from agents.vdqn.cfg.vconfig import vDqnConfig

cfg = vDqnConfig()

print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è PyTorch: {cfg.device}")

class PrioritizedReplayBuffer:
    """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –æ–ø—ã—Ç–∞ —Å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
    
    def __init__(self, capacity, state_size, alpha=0.6, beta=0.4, beta_increment=0.001, use_gpu_storage=True):
        self.capacity = capacity
        self.state_size = state_size
        self.alpha = alpha
        self.beta = beta
        self.beta_increment = beta_increment
        self.device = cfg.device
        self.use_gpu_storage = use_gpu_storage and torch.cuda.is_available()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±—É—Ñ–µ—Ä—ã –Ω–∞ GPU –∏–ª–∏ CPU —Å pinned memory
        if self.use_gpu_storage:
            # –ü–æ–ª–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ GPU
            self.states = torch.zeros((capacity, state_size), dtype=torch.float32, device=self.device)
            self.next_states = torch.zeros((capacity, state_size), dtype=torch.float32, device=self.device)
            self.actions = torch.zeros(capacity, dtype=torch.long, device=self.device)
            self.rewards = torch.zeros(capacity, dtype=torch.float32, device=self.device)
            self.dones = torch.zeros(capacity, dtype=torch.bool, device=self.device)
            self.gamma_ns = torch.ones(capacity, dtype=torch.float32, device=self.device)
            self.priorities = torch.ones(capacity, dtype=torch.float32, device=self.device)
        else:
            # Pinned memory –Ω–∞ CPU –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞ GPU
            self.states = torch.zeros((capacity, state_size), dtype=torch.float32, pin_memory=True)
            self.next_states = torch.zeros((capacity, state_size), dtype=torch.float32, pin_memory=True)
            self.actions = torch.zeros(capacity, dtype=torch.long, pin_memory=True)
            self.rewards = torch.zeros(capacity, dtype=torch.float32, pin_memory=True)
            self.dones = torch.zeros(capacity, dtype=torch.bool, pin_memory=True)
            self.gamma_ns = torch.ones(capacity, dtype=torch.float32, pin_memory=True)
            self.priorities = torch.ones(capacity, dtype=torch.float32, pin_memory=True)
        
        self.position = 0
        self.size = 0
        
        print(f"üöÄ Replay Buffer: {'GPU storage' if self.use_gpu_storage else 'Pinned memory'} –Ω–∞ {self.device}")
        
    def push(self, state, action, reward, next_state, done, gamma_n=1.0):
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not isinstance(state, torch.Tensor):
            state = torch.FloatTensor(state)
        if not isinstance(next_state, torch.Tensor):
            next_state = torch.FloatTensor(next_state)
        if not isinstance(action, torch.Tensor):
            action = torch.LongTensor([action])
        if not isinstance(reward, torch.Tensor):
            reward = torch.FloatTensor([reward])
        if not isinstance(done, torch.Tensor):
            done = torch.BoolTensor([done])
        if not isinstance(gamma_n, torch.Tensor):
            gamma_n = torch.FloatTensor([gamma_n])
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if not self.use_gpu_storage:
            state = state.to(self.device, non_blocking=True)
            next_state = next_state.to(self.device, non_blocking=True)
            action = action.to(self.device, non_blocking=True)
            reward = reward.to(self.device, non_blocking=True)
            done = done.to(self.device, non_blocking=True)
            gamma_n = gamma_n.to(self.device, non_blocking=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        self.states[self.position] = state
        self.next_states[self.position] = next_state
        self.actions[self.position] = action
        self.rewards[self.position] = reward
        self.dones[self.position] = done
        self.gamma_ns[self.position] = gamma_n
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –Ω–æ–≤–æ–≥–æ –æ–ø—ã—Ç–∞
        max_priority = self.priorities.max() if self.size > 0 else 1.0
        self.priorities[self.position] = max_priority
        
        self.position = (self.position + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def push_n_step(self, n_step_transitions):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç n-step transitions –≤ replay buffer
        
        Args:
            n_step_transitions: —Å–ø–∏—Å–æ–∫ n-step transitions –æ—Ç environment
        """
        for transition in n_step_transitions:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None
            if (transition['state'] is not None and 
                transition['action'] is not None and 
                transition['reward'] is not None and 
                transition['next_state'] is not None):
                
                self.push(
                    state=transition['state'],
                    action=transition['action'],
                    reward=transition['reward'],
                    next_state=transition['next_state'],
                    done=transition['done'],
                    gamma_n=1.0  # gamma —É–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω –≤ n-step return
                )
    
    def sample(self, batch_size):
        if self.size == 0:
            return None, None, None, None, None, None, None, None
        
        # –í—ã–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        priorities = self.priorities[:self.size]
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        indices = torch.multinomial(probs, batch_size, replacement=True)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è importance sampling
        weights = (self.size * probs[indices]) ** (-self.beta)
        weights /= weights.max()
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ weights –Ω–∞—Ö–æ–¥—è—Ç—Å—è –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        if self.use_gpu_storage:
            weights = weights.to(self.device, non_blocking=True)
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º beta
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        # –ü–æ–ª—É—á–∞–µ–º batch (—É–∂–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ)
        states = self.states[indices]
        next_states = self.next_states[indices]
        actions = self.actions[indices]
        rewards = self.rewards[indices]
        dones = self.dones[indices]
        gamma_ns = self.gamma_ns[indices]
        
        return states, actions, rewards, next_states, dones, gamma_ns, weights, indices
    
    def update_priorities(self, indices, priorities):
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ priorities - —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä
        if not isinstance(priorities, torch.Tensor):
            priorities = torch.FloatTensor(priorities)
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ indices - —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä
        if not isinstance(indices, torch.Tensor):
            indices = torch.LongTensor(indices)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–¥–Ω–æ–º—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É
        if self.use_gpu_storage:
            priorities = priorities.to(self.device, non_blocking=True)
            indices = indices.to(self.device, non_blocking=True)
        else:
            priorities = priorities.cpu()
            indices = indices.cpu()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã
        self.priorities[indices] = priorities
    
    def __len__(self):
        return self.size

# --- DQNSolver –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥ PyTorch —Å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ ---
class DQNSolver:
    def __init__(self, observation_space, action_space, load=False):
        self.cfg      = vDqnConfig()
        self.n_step   = getattr(self.cfg, "n_step", 3)   # 3‚Äë5 —à–∞–≥–æ–≤
        self.n_queue  = deque(maxlen=self.n_step)                
        self.epsilon      = self.cfg.eps_start
        self.action_space = action_space
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Prioritized Replay Buffer –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        if self.cfg.prioritized:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ observation_space
            state_size = observation_space if isinstance(observation_space, int) else observation_space
            use_gpu_storage = getattr(self.cfg, 'use_gpu_storage', True)
            
            self.memory = PrioritizedReplayBuffer(
                self.cfg.memory_size, 
                state_size, 
                self.cfg.alpha, 
                self.cfg.beta, 
                self.cfg.beta_increment,
                use_gpu_storage
            )
        else:
            self.memory = deque(maxlen=self.cfg.memory_size)
        
        self._eps_decay_rate = math.exp(math.log(self.cfg.eps_final / self.cfg.eps_start) / self.cfg.eps_decay_steps)

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Rainbow –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        if getattr(self.cfg, 'use_noisy_networks', True):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Noisy Dueling DQN
            from agents.vdqn.dqnn import NoisyDuelingDQN
            self.model = NoisyDuelingDQN(
                observation_space, 
                action_space, 
                self.cfg.hidden_sizes,
                dropout_rate=self.cfg.dropout_rate,
                layer_norm=self.cfg.layer_norm
            ).to(self.cfg.device)
            
            self.target_model = NoisyDuelingDQN(
                observation_space, 
                action_space, 
                self.cfg.hidden_sizes,
                dropout_rate=self.cfg.dropout_rate,
                layer_norm=self.cfg.layer_norm
            ).to(self.cfg.device)
            
            print("üîÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º Noisy Dueling DQN –¥–ª—è –ª—É—á—à–µ–≥–æ exploration")
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π DQN
            self.model = DQNN(
                observation_space, 
                action_space, 
                self.cfg.hidden_sizes,
                dropout_rate=self.cfg.dropout_rate,
                layer_norm=self.cfg.layer_norm,
                dueling=self.cfg.dueling_dqn
            ).to(self.cfg.device)
            
            self.target_model = DQNN(
                observation_space, 
                action_space, 
                self.cfg.hidden_sizes,
                dropout_rate=self.cfg.dropout_rate,
                layer_norm=self.cfg.layer_norm,
                dueling=self.cfg.dueling_dqn
            ).to(self.cfg.device)
        
        # üöÄ PyTorch 2.x Compile –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è!
        if (getattr(self.cfg, 'use_torch_compile', True) and 
            not getattr(self.cfg, 'torch_compile_force_disable', False) and 
            hasattr(torch, 'compile')):
            try:
                print("üöÄ –ö–æ–º–ø–∏–ª–∏—Ä—É—é –º–æ–¥–µ–ª—å —Å torch.compile –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è...")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA capability –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ —Ä–µ–∂–∏–º–∞
                if self.cfg.device.type == 'cuda':
                    device_capability = torch.cuda.get_device_capability()
                    device_name = torch.cuda.get_device_name()
                    print(f"üîç CUDA Capability: {device_capability[0]}.{device_capability[1]}")
                    print(f"üéØ GPU: {device_name}")
                    
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è Tesla P100
                    if "Tesla P100" in device_name:
                        print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω Tesla P100 - –æ—Ç–∫–ª—é—á–∞–µ–º torch.compile")
                        raise RuntimeError("Tesla P100 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç torch.compile")
                    
                    if device_capability[0] >= 7:  # Volta+ (V100, A100, H100, etc.)
                        compile_mode = 'max-autotune'
                        print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∂–∏–º 'max-autotune' –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ GPU")
                    elif device_capability[0] >= 6:  # Pascal (GTX 1080, etc., –Ω–æ –Ω–µ P100)
                        if getattr(self.cfg, 'torch_compile_fallback', True):
                            compile_mode = 'default'
                            print("‚ö†Ô∏è GPU Pascal (–Ω–µ P100), –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∂–∏–º 'default'")
                        else:
                            raise RuntimeError("GPU Pascal –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç torch.compile –≤ —Ä–µ–∂–∏–º–µ max-autotune")
                    else:  # Maxwell –∏ —Å—Ç–∞—Ä—à–µ
                        if getattr(self.cfg, 'torch_compile_fallback', True):
                            compile_mode = 'default'
                            print("‚ö†Ô∏è GPU Maxwell –∏–ª–∏ —Å—Ç–∞—Ä—à–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∂–∏–º 'default'")
                        else:
                            raise RuntimeError("GPU —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä—ã–π –¥–ª—è torch.compile")
                else:
                    compile_mode = 'default'
                    print("‚ÑπÔ∏è CPU —Ä–µ–∂–∏–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º 'default'")
                
                self.model = torch.compile(self.model, mode=compile_mode)
                self.target_model = torch.compile(self.target_model, mode=compile_mode)
                print(f"‚úÖ –ú–æ–¥–µ–ª–∏ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ —Å —Ä–µ–∂–∏–º–æ–º '{compile_mode}'!")
                
            except Exception as e:
                print(f"‚ö†Ô∏è torch.compile –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å: {e}")
                print("üìù –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–∞–µ–º torch.compile –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
                self.cfg.use_torch_compile = False
                self.cfg.torch_compile_force_disable = True
                print("üîÑ torch.compile –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞")
        else:
            if not hasattr(torch, 'compile'):
                print("üìù PyTorch < 2.0, torch.compile –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            else:
                print("üìù torch.compile –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        
        # –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞
        self.target_model.load_state_dict(self.model.state_dict())
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º learning rate
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=self.cfg.lr,
            weight_decay=1e-4,
            eps=1e-7
        )
        
        # Scheduler –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ learning rate
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, 
            T_0=1000, 
            T_mult=2,
            eta_min=1e-6
        )
        
        # Loss function
        self.criterion = nn.HuberLoss(delta=1.0)
        
        # –°—á–µ—Ç—á–∏–∫–∏
        self.step_count = 0
        self.update_count = 0
        
        # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.cfg.device.type == 'cuda':
            # –í–∫–ª—é—á–∞–µ–º cudnn benchmark –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # Mixed precision training
            if self.cfg.use_amp:
                self.scaler = torch.cuda.amp.GradScaler()
            else:
                self.scaler = None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if load:
            self.load_model()
            
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        self.model.eval()
        self.target_model.eval()

    def update_target_model(self):
        """–ö–æ–ø–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å."""
        self.target_model.load_state_dict(self.model.state_dict())

    def remember(self, state, action, reward, next_state, done, gamma_n=1.0):
        if self.cfg.prioritized:
            self.memory.push(state, action, reward, next_state, done, gamma_n)
        else:
            self.memory.append((
                torch.tensor(state,      dtype=torch.float32),
                torch.tensor(action,     dtype=torch.long),     
                torch.tensor(reward,     dtype=torch.float32),  
                torch.tensor(next_state, dtype=torch.float32),  
                torch.tensor(done,       dtype=torch.bool),     
                torch.tensor(gamma_n,    dtype=torch.float32)   
            ))
           
                
    def store_transition(self, state, action, reward, next_state, done, gamma_n=1.0):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –≤ replay buffer —Å n-step returns"""
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –≤ n-step –æ—á–µ—Ä–µ–¥—å
        self.n_queue.append((state, action, reward, next_state, done))
        
        # –ï—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∞, –≤—ã—á–∏—Å–ª—è–µ–º n-step return
        if len(self.n_queue) == self.n_step:
            s0, a0, r0, _, _ = self.n_queue[0]
            
            # –í—ã—á–∏—Å–ª—è–µ–º n-step return
            R_n = r0
            gamma_pow = 1.0
            for i in range(1, self.n_step):
                _, _, r, _, d = self.n_queue[i]
                if d:  # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è, –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                    break
                gamma_pow *= self.cfg.gamma
                R_n += gamma_pow * r
            
            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            s_n = self.n_queue[-1][3]  # next_state –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
            d_n = self.n_queue[-1][4]  # done –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º "—É–∫—Ä—É–ø–Ω—ë–Ω–Ω—ã–π" –ø–µ—Ä–µ—Ö–æ–¥ –≤ –±—É—Ñ–µ—Ä
            if self.cfg.prioritized:
                self.memory.push(s0, a0, R_n, s_n, d_n, gamma_pow)
            else:
                self.memory.append((s0, a0, R_n, s_n, d_n, gamma_pow))
            
            # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è ‚Äî –æ—á–∏—â–∞–µ–º –æ—á–µ—Ä–µ–¥—å
            if done:
                self.n_queue.clear()
                
    def act(self, state):
        if np.random.rand() < self.epsilon:
            return random.randrange(self.action_space)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –∏ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –Ω—É–ª–∏
        if torch.isnan(torch.tensor(state)).any():
            state = np.nan_to_num(state, nan=0.0)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.cfg.device)
            q_values = self.model(state_tensor)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –≤ –≤—ã—Ö–æ–¥–µ
            if torch.isnan(q_values).any():
                print("Warning: NaN detected in Q-values, using random action")
                return random.randrange(self.action_space)
            
            return q_values.argmax().item()

    def experience_replay(self, need_metrics=False):
        if len(self.memory) < self.cfg.batch_size:
            return False, None, None, None
        
        # –ü–æ–ª—É—á–∞–µ–º batch
        if self.cfg.prioritized:
            states, actions, rewards, next_states, dones, gamma_ns, weights, indices = \
                self.memory.sample(self.cfg.batch_size)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ None (–ø—É—Å—Ç–æ–π –±—É—Ñ–µ—Ä)
            if states is None:
                return False, None, None, None
            
            # –î–∞–Ω–Ω—ã–µ —É–∂–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –±–ª–∞–≥–æ–¥–∞—Ä—è GPU storage!
            # –ù–∏–∫–∞–∫–∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö .to() –Ω–µ –Ω—É–∂–Ω–æ
        else:
            batch = random.sample(self.memory, self.cfg.batch_size)
            states, actions, rewards, next_states, dones, gamma_ns = zip(*batch)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã –∏ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU
            states = torch.stack(states).to(self.cfg.device, non_blocking=True)
            actions = torch.LongTensor(actions).to(self.cfg.device, non_blocking=True)
            rewards = torch.FloatTensor(rewards).to(self.cfg.device, non_blocking=True)
            next_states = torch.stack(next_states).to(self.cfg.device, non_blocking=True)
            dones = torch.BoolTensor(dones).to(self.cfg.device, non_blocking=True)
            gamma_ns = torch.FloatTensor(gamma_ns).to(self.cfg.device, non_blocking=True)
            weights = torch.ones(self.cfg.batch_size).to(self.cfg.device, non_blocking=True)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN
        if torch.isnan(states).any() or torch.isnan(next_states).any():
            print("Warning: NaN detected in states, skipping batch")
            return False, None, None, None
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        self.model.train()
        
        # Double DQN: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å–µ—Ç—å –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π, —Ü–µ–ª–µ–≤—É—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        if self.cfg.double_dqn:
            with torch.no_grad():
                next_actions = self.model(next_states).argmax(dim=1)
                next_q_values = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        else:
            with torch.no_grad():
                next_q_values = self.target_model(next_states).max(1)[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º target Q-values
        target_q_values = rewards + (gamma_ns * next_q_values * ~dones)
        
        # –¢–µ–∫—É—â–∏–µ Q-values
        current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # –í—ã—á–∏—Å–ª—è–µ–º loss —Å importance sampling weights
        td_errors = target_q_values - current_q_values
        loss = (weights * self.criterion(current_q_values, target_q_values)).mean()
        
        # Mixed Precision Training –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if self.scaler is not None:
            self.optimizer.zero_grad()
            self.scaler.scale(loss).backward()
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            if self.cfg.grad_clip:
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)
            
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            # –û–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            self.optimizer.zero_grad()
            loss.backward()
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
            if self.cfg.grad_clip:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)
            
            self.optimizer.step()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è PER
        if self.cfg.prioritized:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ priorities –∏ indices –Ω–∞—Ö–æ–¥—è—Ç—Å—è –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
            priorities = (torch.abs(td_errors) + 1e-6).detach()
            if self.memory.use_gpu_storage:
                # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º GPU storage, –æ—Å—Ç–∞–≤–ª—è–µ–º priorities –Ω–∞ GPU
                priorities = priorities.to(self.memory.device)
                indices = indices.to(self.memory.device)
            else:
                # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU storage, –ø–µ—Ä–µ–≤–æ–¥–∏–º priorities –Ω–∞ CPU
                priorities = priorities.cpu()
                indices = indices.cpu()
            
            self.memory.update_priorities(indices, priorities)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º scheduler
        self.scheduler.step()
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —à—É–º –≤ Noisy Networks –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        if hasattr(self.model, 'reset_noise'):
            self.model.reset_noise()
        if hasattr(self.target_model, 'reset_noise'):
            self.target_model.reset_noise()
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è inference
        self.model.eval()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        if need_metrics:
            abs_q = current_q_values.abs().mean().item()
            q_gap = (target_q_values - current_q_values).abs().mean().item()
            return True, loss.item(), abs_q, q_gap
        
        return True, None, None, None

    def print_trade_stats(self, trades):
        if not trades:
            return {"trades_count": 0, "winrate": 0.0, "avg_profit": 0.0, "avg_loss": 0.0}
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –∫–ª—é—á–∞–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞: "profit" –∏ "roi"
        def get_profit(trade):
            return trade.get("profit", trade.get("roi", 0))
        
        profits = [get_profit(t) for t in trades if get_profit(t) > 0]
        losses = [get_profit(t) for t in trades if get_profit(t) < 0]
        
        winrate = len(profits) / len(trades) if trades else 0.0
        avg_profit = np.mean(profits) if profits else 0.0
        avg_loss = np.mean(losses) if losses else 0.0
        avg_roi = np.mean([get_profit(t) for t in trades]) if trades else 0.0
        
        pl_ratio = abs(avg_profit / avg_loss) if avg_loss != 0 else 0.0
        
        # –°—á–∏—Ç–∞–µ–º –ø–ª–æ—Ö–∏–µ —Å–¥–µ–ª–∫–∏
        bad_trades = [t for t in trades if abs(get_profit(t)) < 0.001]  # <0.1%
        bad_trades_count = len(bad_trades)
        
        stats = {
            "trades_count": len(trades),
            "winrate": winrate,
            "avg_profit": avg_profit,
            "avg_loss": avg_loss,
            "avg_roi": avg_roi,
            "pl_ratio": pl_ratio,
            "bad_trades_count": bad_trades_count
        }
        
        print(f"üìä Trades: {stats['trades_count']}, Winrate: {stats['winrate']*100:.2f}%, "
              f"Avg P: {stats['avg_profit']:.3f}, Avg L: {stats['avg_loss']:.3f}, "
              f"P/L ratio: {stats['pl_ratio']:.2f}, Bad trades: {stats['bad_trades_count']}")
        
        return stats

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ replay buffer (–ø–æ–ª–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)"""
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º torch.compile –ø—Ä–µ—Ñ–∏–∫—Å –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
        model_state_dict = self.model.state_dict()
        target_state_dict = self.target_model.state_dict()
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        cleaned_model_state = {}
        cleaned_target_state = {}
        
        for key, value in model_state_dict.items():
            if key.startswith('_orig_mod.'):
                new_key = key.replace('_orig_mod.', '')
                cleaned_model_state[new_key] = value
            else:
                cleaned_model_state[key] = value
                
        for key, value in target_state_dict.items():
            if key.startswith('_orig_mod.'):
                new_key = key.replace('_orig_mod.', '')
                cleaned_target_state[new_key] = value
            else:
                cleaned_target_state[key] = value
        
        torch.save({
            'model_state_dict': cleaned_model_state,
            'target_model_state_dict': cleaned_target_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epsilon': self.epsilon,
            'cfg': self.cfg
        }, self.cfg.model_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º replay buffer
        with open(self.cfg.buffer_path, 'wb') as f:
            pickle.dump(self.memory, f, protocol=HIGHEST_PROTOCOL)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.cfg.model_path}")
        print(f"‚úÖ Replay buffer —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {self.cfg.buffer_path}")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
        if os.path.exists(self.cfg.model_path):
            try:
                checkpoint = torch.load(self.cfg.model_path, map_location=self.cfg.device)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
                if 'model_state_dict' in checkpoint:
                    model_state = checkpoint['model_state_dict']
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è (—É—á–∏—Ç—ã–≤–∞–µ–º torch.compile –ø—Ä–µ—Ñ–∏–∫—Å)
                    first_layer_key = None
                    for key in model_state.keys():
                        if 'feature_layers.0.weight' in key:
                            first_layer_key = key
                            break
                    
                    if first_layer_key:
                        saved_input_size = model_state[first_layer_key].shape[1]
                        current_input_size = self.model.net.feature_layers[0].weight.shape[1]
                        
                        if saved_input_size != current_input_size:
                            print(f"‚ö†Ô∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è {saved_input_size}, —Ç–µ–∫—É—â–∞—è {current_input_size}")
                            print("üîÑ –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π")
                            return
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º torch.compile –ø—Ä–µ—Ñ–∏–∫—Å
                    try:
                        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å
                        self.model.load_state_dict(checkpoint['model_state_dict'])
                        self.target_model.load_state_dict(checkpoint['model_state_dict'])
                        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞")
                    except Exception as compile_error:
                        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod
                        if "_orig_mod" in str(compile_error):
                            print("üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º torch.compile –ø—Ä–µ—Ñ–∏–∫—Å...")
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod
                            has_orig_mod = any(key.startswith('_orig_mod.') for key in checkpoint['model_state_dict'].keys())
                            
                            print(f"üîç –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–π –º–æ–¥–µ–ª–∏:")
                            print(f"   ‚Ä¢ –ö–ª—é—á–∏ —Å _orig_mod: {has_orig_mod}")
                            print(f"   ‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã –∫–ª—é—á–µ–π: {list(checkpoint['model_state_dict'].keys())[:3]}")
                            
                            if has_orig_mod:
                                # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                                print("üìù –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
                                adjusted_state_dict = {}
                                for key, value in checkpoint['model_state_dict'].items():
                                    if key.startswith('_orig_mod.'):
                                        new_key = key.replace('_orig_mod.', '')
                                        adjusted_state_dict[new_key] = value
                                    else:
                                        adjusted_state_dict[key] = value
                                
                                self.model.load_state_dict(adjusted_state_dict)
                                self.target_model.load_state_dict(adjusted_state_dict)
                                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ø—Ä–µ—Ñ–∏–∫—Å–∞")
                            else:
                                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                                print("üìù –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
                                adjusted_state_dict = {}
                                for key, value in checkpoint['model_state_dict'].items():
                                    new_key = f"_orig_mod.{key}"
                                    adjusted_state_dict[new_key] = value
                            
                                self.model.load_state_dict(adjusted_state_dict)
                                self.target_model.load_state_dict(adjusted_state_dict)
                                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ø—Ä–µ—Ñ–∏–∫—Å–∞")
                        else:
                            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–µ —Å–≤—è–∑–∞–Ω–∞ —Å torch.compile –ø—Ä–µ—Ñ–∏–∫—Å–æ–º: {compile_error}")
                            raise compile_error
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                    if 'optimizer_state_dict' in checkpoint:
                        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    if 'scheduler_state_dict' in checkpoint:
                        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    if 'epsilon' in checkpoint:
                        self.epsilon = checkpoint['epsilon']
                        
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {self.cfg.model_path}")
                else:
                    print(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç checkpoint –≤ {self.cfg.model_path}")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
                print("üîÑ –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å")
        else:
            print(f"üìù –§–∞–π–ª –º–æ–¥–µ–ª–∏ {self.cfg.model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å")

    def load_state(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç replay buffer"""
        if os.path.exists(self.cfg.buffer_path):
            with open(self.cfg.buffer_path, 'rb') as f:
                self.memory = pickle.load(f)
            print(f"Replay buffer –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {self.cfg.buffer_path}")
        else:
            print(f"–§–∞–π–ª replay buffer {self.cfg.buffer_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")

    def soft_update(self, tau=0.01):
        """Soft update target network"""
        for target_param, local_param in zip(self.target_model.parameters(), self.model.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)        

    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å (–±–µ–∑ replay buffer)"""
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º torch.compile –ø—Ä–µ—Ñ–∏–∫—Å –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
        model_state_dict = self.model.state_dict()
        target_state_dict = self.target_model.state_dict()
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å _orig_mod –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        cleaned_model_state = {}
        cleaned_target_state = {}
        
        for key, value in model_state_dict.items():
            if key.startswith('_orig_mod.'):
                new_key = key.replace('_orig_mod.', '')
                cleaned_model_state[new_key] = value
            else:
                cleaned_model_state[key] = value
                
        for key, value in target_state_dict.items():
            if key.startswith('_orig_mod.'):
                new_key = key.replace('_orig_mod.', '')
                cleaned_target_state[new_key] = value
            else:
                cleaned_target_state[key] = value
        
        torch.save({
            'model_state_dict': cleaned_model_state,
            'target_model_state_dict': cleaned_target_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epsilon': self.epsilon,
            'cfg': self.cfg
        }, self.cfg.model_path)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.cfg.model_path}")
        
    def update_target_model(self):
        """Hard update target network –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è"""
        self.target_model.load_state_dict(self.model.state_dict())
        
    def store_transition(self, state, action, reward, next_state, done, gamma_n=1.0):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –≤ replay buffer —Å n-step returns"""
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –≤ n-step –æ—á–µ—Ä–µ–¥—å
        self.n_queue.append((state, action, reward, next_state, done))
        
        # –ï—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∞, –≤—ã—á–∏—Å–ª—è–µ–º n-step return
        if len(self.n_queue) == self.n_step:
            s0, a0, r0, _, _ = self.n_queue[0]
            
            # –í—ã—á–∏—Å–ª—è–µ–º n-step return
            R_n = r0
            gamma_pow = 1.0
            for i in range(1, self.n_step):
                _, _, r, _, d = self.n_queue[i]
                if d:  # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è, –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                    break
                gamma_pow *= self.cfg.gamma
                R_n += gamma_pow * r
            
            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            s_n = self.n_queue[-1][3]  # next_state –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
            d_n = self.n_queue[-1][4]  # done –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º "—É–∫—Ä—É–ø–Ω—ë–Ω–Ω—ã–π" –ø–µ—Ä–µ—Ö–æ–¥ –≤ –±—É—Ñ–µ—Ä
            if self.cfg.prioritized:
                self.memory.push(s0, a0, R_n, s_n, d_n, gamma_pow)
            else:
                self.memory.append((s0, a0, R_n, s_n, d_n, gamma_pow))
            
            # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–∏–ª—Å—è ‚Äî –æ—á–∏—â–∞–µ–º –æ—á–µ—Ä–µ–¥—å
            if done:
                self.n_queue.clear()
                
    def remember(self, state, action, reward, next_state, done, gamma_n=1.0):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è store_transition"""
        self.store_transition(state, action, reward, next_state, done, gamma_n)
        




