version: "3.9"

services:
  postgres:
    image: postgres:14
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: medoed_db
      POSTGRES_USER: medoed_user
      POSTGRES_PASSWORD: medoed
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  redis:
    image: redis:latest
    container_name: redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 5s

  medoedai:
    build: .
    container_name: medoedai
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    ports:
      - "5050:5050"
    environment:
      TORCHSERVE_URL: http://serving:8080/predictions/medoedai_model
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      PYTHONDONTWRITEBYTECODE: "1"
      FLASK_DEBUG: "True"  # Включаем debug для разработки
    volumes:
      - ./:/app
    working_dir: /app
    command: ["python", "main.py"]
    gpus: all

  celery-worker:
    build: .
    container_name: celery-worker
    depends_on:
      medoedai:
        condition: service_started
      redis:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      PYTHONDONTWRITEBYTECODE: "1"
    env_file:
      - .env
    volumes:
      - ./:/app
    working_dir: /app
    command: ["celery", "-A", "tasks.celery_tasks", "worker",
              "-Q", "train", "-P", "solo", "-c", "1", "--loglevel=info"]
    gpus: all

  pytorch-gpu:
    image: pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime
    container_name: pytorch-gpu
    depends_on:
      celery-worker:
        condition: service_started
    restart: unless-stopped
    ports:
      - "8090:8090"
    volumes:
      - ./workspace:/workspace
      - ./models:/models
    working_dir: /workspace
    command: ["sleep", "infinity"]
    gpus: all

volumes:
  pgdata:
